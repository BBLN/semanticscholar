interactions:
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - api.semanticscholar.org
      user-agent:
      - python-httpx/0.25.1
    method: GET
    uri: https://api.semanticscholar.org/graph/v1/paper/CorpusID:1033682/references?fields=contexts,intents,isInfluential,abstract,authors,citationCount,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=1000
  response:
    content: '{"offset": 0, "data": [{"intents": [], "isInfluential": false, "contexts":
      [], "citedPaper": {"paperId": "2df62c0d7cdc61ab2cf82f30e40b7761e2e33299", "externalIds":
      {"DBLP": "journals/corr/abs-2303-04143", "ArXiv": "2303.04143", "DOI": "10.48550/arXiv.2303.04143",
      "CorpusId": 257378481}, "corpusId": 257378481, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/2df62c0d7cdc61ab2cf82f30e40b7761e2e33299",
      "title": "Can We Scale Transformers to Predict Parameters of Diverse ImageNet
      Models?", "abstract": "Pretraining a neural network on a large dataset is becoming
      a cornerstone in machine learning that is within the reach of only a few communities
      with large-resources. We aim at an ambitious goal of democratizing pretraining.
      Towards that goal, we train and release a single neural network that can predict
      high quality ImageNet parameters of other neural networks. By using predicted
      parameters for initialization we are able to boost training of diverse ImageNet
      models available in PyTorch. When transferred to other datasets, models initialized
      with predicted parameters also converge faster and reach competitive final performance.",
      "venue": "International Conference on Machine Learning", "year": 2023, "referenceCount":
      50, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true,
      "openAccessPdf": {"url": "http://arxiv.org/pdf/2303.04143", "status": null},
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2023-03-07", "journal":
      {"volume": "abs/2303.04143", "name": "ArXiv"}, "authors": [{"authorId": "49934559",
      "name": "Boris Knyazev"}, {"authorId": "40868309", "name": "Doha Hwang"}, {"authorId":
      "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"intents": [], "isInfluential":
      false, "contexts": [], "citedPaper": {"paperId": "c15da5309b72a2c6966e2ddb65520a62458c98b3",
      "externalIds": {"DBLP": "conf/icml/ZhangZLBS23", "ArXiv": "2301.13197", "DOI":
      "10.48550/arXiv.2301.13197", "CorpusId": 253181041}, "corpusId": 253181041,
      "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International
      Conference on Machine Learning", "type": "conference", "alternate_names": ["ICML",
      "Int Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/c15da5309b72a2c6966e2ddb65520a62458c98b3",
      "title": "Unlocking Slot Attention by Changing Optimal Transport Costs", "abstract":
      "Slot attention is a powerful method for object-centric modeling in images and
      videos. However, its set-equivariance limits its ability to handle videos with
      a dynamic number of objects because it cannot break ties. To overcome this limitation,
      we first establish a connection between slot attention and optimal transport.
      Based on this new perspective we propose MESH (Minimize Entropy of Sinkhorn):
      a cross-attention module that combines the tiebreaking properties of unregularized
      optimal transport with the speed of regularized optimal transport. We evaluate
      slot attention using MESH on multiple object-centric learning benchmarks and
      find significant improvements over slot attention in every setting.", "venue":
      "International Conference on Machine Learning", "year": 2023, "referenceCount":
      51, "citationCount": 5, "influentialCitationCount": 2, "isOpenAccess": true,
      "openAccessPdf": {"url": "http://arxiv.org/pdf/2301.13197", "status": null},
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2023-01-30", "journal": {"volume": "abs/2301.13197", "name": "ArXiv"}, "authors":
      [{"authorId": "36124320", "name": "Yan Zhang"}, {"authorId": "2127383427", "name":
      "David W. Zhang"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"},
      {"authorId": "1909303", "name": "G. Burghouts"}, {"authorId": "145404204", "name":
      "Cees G. M. Snoek"}]}}, {"intents": [], "isInfluential": false, "contexts":
      [], "citedPaper": {"paperId": "c10a93e922f74fea2af9f2ed53082b4a4a7af818", "externalIds":
      {"DBLP": "journals/corr/abs-2212-01674", "ArXiv": "2212.01674", "DOI": "10.48550/arXiv.2212.01674",
      "CorpusId": 254246307}, "corpusId": 254246307, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/c10a93e922f74fea2af9f2ed53082b4a4a7af818",
      "title": "CrossSplit: Mitigating Label Noise Memorization through Data Splitting",
      "abstract": "We approach the problem of improving robustness of deep learning
      algorithms in the presence of label noise. Building upon existing label correction
      and co-teaching methods, we propose a novel training procedure to mitigate the
      memorization of noisy labels, called CrossSplit, which uses a pair of neural
      networks trained on two disjoint parts of the labelled dataset. CrossSplit combines
      two main ingredients: (i) Cross-split label correction. The idea is that, since
      the model trained on one part of the data cannot memorize example-label pairs
      from the other part, the training labels presented to each network can be smoothly
      adjusted by using the predictions of its peer network; (ii) Cross-split semi-supervised
      training. A network trained on one part of the data also uses the unlabeled
      inputs of the other part. Extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet
      and mini-WebVision datasets demonstrate that our method can outperform the current
      state-of-the-art in a wide range of noise ratios.", "venue": "International
      Conference on Machine Learning", "year": 2022, "referenceCount": 49, "citationCount":
      1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url":
      "http://arxiv.org/pdf/2212.01674", "status": null}, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2022-12-03", "journal":
      {"pages": "16377-16392"}, "authors": [{"authorId": "2127158894", "name": "Jihye
      Kim"}, {"authorId": "14398916", "name": "A. Baratin"}, {"authorId": "2152823316",
      "name": "Yan Zhang"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}},
      {"intents": [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId":
      "598b65b8f514304abd34b7e14296559388b20f25", "externalIds": {"DBLP": "journals/corr/abs-2208-04425",
      "ArXiv": "2208.04425", "DOI": "10.48550/arXiv.2208.04425", "CorpusId": 251442494},
      "corpusId": 251442494, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/598b65b8f514304abd34b7e14296559388b20f25",
      "title": "Controlled Sparsity via Constrained Optimization or: How I Learned
      to Stop Tuning Penalties and Love Constraints", "abstract": "The performance
      of trained neural networks is robust to harsh levels of pruning. Coupled with
      the ever-growing size of deep learning models, this observation has motivated
      extensive research on learning sparse models. In this work, we focus on the
      task of controlling the level of sparsity when performing sparse learning. Existing
      methods based on sparsity-inducing penalties involve expensive trial-and-error
      tuning of the penalty factor, thus lacking direct control of the resulting model
      sparsity. In response, we adopt a constrained formulation: using the gate mechanism
      proposed by Louizos et al. (2018), we formulate a constrained optimization problem
      where sparsification is guided by the training objective and the desired sparsity
      target in an end-to-end fashion. Experiments on CIFAR-{10, 100}, TinyImageNet,
      and ImageNet using WideResNet and ResNet{18, 50} models validate the effectiveness
      of our proposal and demonstrate that we can reliably achieve pre-determined
      sparsity targets without compromising on predictive performance.", "venue":
      "Neural Information Processing Systems", "year": 2022, "referenceCount": 51,
      "citationCount": 6, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf":
      {"url": "https://arxiv.org/pdf/2208.04425", "status": null}, "fieldsOfStudy":
      ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2022-08-08", "journal": {"volume": "abs/2208.04425",
      "name": "ArXiv"}, "authors": [{"authorId": "1410596066", "name": "Jose Gallego-Posada"},
      {"authorId": "2111835126", "name": "Juan Ramirez"}, {"authorId": "3429013",
      "name": "Akram Erraqabi"}, {"authorId": "1865800402", "name": "Y. Bengio"},
      {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"intents": [],
      "isInfluential": false, "contexts": [], "citedPaper": {"paperId": "6f9d72dd797a67d0a477502a509ab29d215761ae",
      "externalIds": {"DBLP": "conf/nips/OrvietoLL22", "ArXiv": "2205.04583", "CorpusId":
      248665756}, "corpusId": 248665756, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/6f9d72dd797a67d0a477502a509ab29d215761ae",
      "title": "Dynamics of SGD with Stochastic Polyak Stepsizes: Truly Adaptive Variants
      and Convergence to Exact Solution", "abstract": "Recently, Loizou et al. (2021),
      proposed and analyzed stochastic gradient descent (SGD) with stochastic Polyak
      stepsize (SPS). The proposed SPS comes with strong convergence guarantees and
      competitive performance; however, it has two main drawbacks when it is used
      in non-over-parameterized regimes: (i) It requires a priori knowledge of the
      optimal mini-batch losses, which are not available when the interpolation condition
      is not satisfied (e.g., regularized objectives), and (ii) it guarantees convergence
      only to a neighborhood of the solution. In this work, we study the dynamics
      and the convergence properties of SGD equipped with new variants of the stochastic
      Polyak stepsize and provide solutions to both drawbacks of the original SPS.
      We first show that a simple modification of the original SPS that uses lower
      bounds instead of the optimal function values can directly solve issue (i).
      On the other hand, solving issue (ii) turns out to be more challenging and leads
      us to valuable insights into the method''s behavior. We show that if interpolation
      is not satisfied, the correlation between SPS and stochastic gradients introduces
      a bias, which effectively distorts the expectation of the gradient signal near
      minimizers, leading to non-convergence - even if the stepsize is scaled down
      during training. To fix this issue, we propose DecSPS, a novel modification
      of SPS, which guarantees convergence to the exact minimizer - without a priori
      knowledge of the problem parameters. For strongly-convex optimization problems,
      DecSPS is the first stochastic adaptive optimization method that converges to
      the exact solution without restrictive assumptions like bounded iterates/gradients.",
      "venue": "Neural Information Processing Systems", "year": 2022, "referenceCount":
      40, "citationCount": 12, "influentialCitationCount": 2, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2022-05-09", "journal": null, "authors": [{"authorId": "51931942", "name":
      "Antonio Orvieto"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"},
      {"authorId": "1941365", "name": "Nicolas Loizou"}]}}, {"intents": [], "isInfluential":
      false, "contexts": [], "citedPaper": {"paperId": "56559a76e9804bfe907297299f8aee88a8a14d64",
      "externalIds": {"DBLP": "journals/corr/abs-2203-04940", "ArXiv": "2203.04940",
      "DOI": "10.48550/arXiv.2203.04940", "CorpusId": 247318780}, "corpusId": 247318780,
      "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural
      Information Processing Systems", "type": "conference", "alternate_names": ["Neural
      Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url":
      "https://www.semanticscholar.org/paper/56559a76e9804bfe907297299f8aee88a8a14d64",
      "title": "Data-Efficient Structured Pruning via Submodular Optimization", "abstract":
      "Structured pruning is an effective approach for compressing large pre-trained
      neural networks without significantly affecting their performance. However,
      most current structured pruning methods do not provide any performance guarantees,
      and often require fine-tuning, which makes them inapplicable in the limited-data
      regime. We propose a principled data-efficient structured pruning method based
      on submodular optimization. In particular, for a given layer, we select neurons/channels
      to prune and corresponding new weights for the next layer, that minimize the
      change in the next layer''s input induced by pruning. We show that this selection
      problem is a weakly submodular maximization problem, thus it can be provably
      approximated using an efficient greedy algorithm. Our method is guaranteed to
      have an exponentially decreasing error between the original model and the pruned
      model outputs w.r.t the pruned size, under reasonable assumptions. It is also
      one of the few methods in the literature that uses only a limited-number of
      training data and no labels. Our experimental results demonstrate that our method
      outperforms state-of-the-art methods in the limited-data regime.", "venue":
      "Neural Information Processing Systems", "year": 2022, "referenceCount": 58,
      "citationCount": 7, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf":
      {"url": "http://arxiv.org/pdf/2203.04940", "status": null}, "fieldsOfStudy":
      ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Mathematics", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2022-03-09", "journal": {"volume": "abs/2203.04940",
      "name": "ArXiv"}, "authors": [{"authorId": "2845064", "name": "Marwa El Halabi"},
      {"authorId": "2822290", "name": "Suraj Srinivas"}, {"authorId": "1388317459",
      "name": "Simon Lacoste-Julien"}]}}, {"intents": [], "isInfluential": false,
      "contexts": [], "citedPaper": {"paperId": "cdf4a982bf6dc373eb6463263ab5fd147c61c8ca",
      "externalIds": {"ArXiv": "2202.13903", "DBLP": "journals/corr/abs-2202-13903",
      "CorpusId": 247158659}, "corpusId": 247158659, "publicationVenue": {"id": "f9af8000-42f8-410d-a622-e8811e41660a",
      "name": "Conference on Uncertainty in Artificial Intelligence", "type": "conference",
      "alternate_names": ["Uncertainty in Artificial Intelligence", "UAI", "Conf Uncertain
      Artif Intell", "Uncertain Artif Intell"], "url": "http://www.auai.org/"}, "url":
      "https://www.semanticscholar.org/paper/cdf4a982bf6dc373eb6463263ab5fd147c61c8ca",
      "title": "Bayesian Structure Learning with Generative Flow Networks", "abstract":
      "In Bayesian structure learning, we are interested in inferring a distribution
      over the directed acyclic graph (DAG) structure of Bayesian networks, from data.
      Defining such a distribution is very challenging, due to the combinatorially
      large sample space, and approximations based on MCMC are often required. Recently,
      a novel class of probabilistic models, called Generative Flow Networks (GFlowNets),
      have been introduced as a general framework for generative modeling of discrete
      and composite objects, such as graphs. In this work, we propose to use a GFlowNet
      as an alternative to MCMC for approximating the posterior distribution over
      the structure of Bayesian networks, given a dataset of observations. Generating
      a sample DAG from this approximate distribution is viewed as a sequential decision
      problem, where the graph is constructed one edge at a time, based on learned
      transition probabilities. Through evaluation on both simulated and real data,
      we show that our approach, called DAG-GFlowNet, provides an accurate approximation
      of the posterior over DAGs, and it compares favorably against other methods
      based on MCMC or variational inference.", "venue": "Conference on Uncertainty
      in Artificial Intelligence", "year": 2022, "referenceCount": 61, "citationCount":
      70, "influentialCitationCount": 3, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2022-02-28", "journal":
      {"volume": "abs/2202.13903", "name": "ArXiv"}, "authors": [{"authorId": "7636193",
      "name": "T. Deleu"}, {"authorId": "2060583060", "name": "Ant''onio G''ois"},
      {"authorId": "1591176064", "name": "Chris C. Emezue"}, {"authorId": "9166330",
      "name": "M. Rankawat"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"},
      {"authorId": "153125952", "name": "Stefan Bauer"}, {"authorId": "1865800402",
      "name": "Y. Bengio"}]}}, {"intents": [], "isInfluential": false, "contexts":
      [], "citedPaper": {"paperId": "7a8493a6d744f621945457bd0f5428a399ba5333", "externalIds":
      {"DBLP": "conf/iclr/ZhangZLBS22", "ArXiv": "2111.12193", "CorpusId": 244527640},
      "corpusId": 244527640, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/7a8493a6d744f621945457bd0f5428a399ba5333",
      "title": "Multiset-Equivariant Set Prediction with Approximate Implicit Differentiation",
      "abstract": "Most set prediction models in deep learning use set-equivariant
      operations, but they actually operate on multisets. We show that set-equivariant
      functions cannot represent certain functions on multisets, so we introduce the
      more appropriate notion of multiset-equivariance. We identify that the existing
      Deep Set Prediction Network (DSPN) can be multiset-equivariant without being
      hindered by set-equivariance and improve it with approximate implicit differentiation,
      allowing for better optimization while being faster and saving memory. In a
      range of toy experiments, we show that the perspective of multiset-equivariance
      is beneficial and that our changes to DSPN achieve better results in most cases.
      On CLEVR object property prediction, we substantially improve over the state-of-the-art
      Slot Attention from 8% to 77% in one of the strictest evaluation metrics because
      of the benefits made possible by implicit differentiation.", "venue": "International
      Conference on Learning Representations", "year": 2021, "referenceCount": 63,
      "citationCount": 10, "influentialCitationCount": 1, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2021-11-23", "journal":
      {"volume": "abs/2111.12193", "name": "ArXiv"}, "authors": [{"authorId": "2152821125",
      "name": "Yan Zhang"}, {"authorId": "2127383427", "name": "David W. Zhang"},
      {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}, {"authorId": "1909303",
      "name": "G. Burghouts"}, {"authorId": "145404204", "name": "Cees G. M. Snoek"}]}},
      {"intents": [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId":
      "a8b91f8fdc6408a66f1aef4eb0ad28f4d28236a0", "externalIds": {"DBLP": "journals/pami/HuangLVLR23",
      "ArXiv": "2110.14711", "DOI": "10.1109/TPAMI.2022.3199617", "CorpusId": 240070802,
      "PubMed": "35976841"}, "corpusId": 240070802, "publicationVenue": {"id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
      "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "type":
      "journal", "alternate_names": ["IEEE Trans Pattern Anal Mach Intell"], "issn":
      "0162-8828", "url": "http://www.computer.org/tpami/", "alternate_urls": ["http://www.computer.org/portal/web/tpami",
      "http://ieeexplore.ieee.org/servlet/opac?punumber=34"]}, "url": "https://www.semanticscholar.org/paper/a8b91f8fdc6408a66f1aef4eb0ad28f4d28236a0",
      "title": "A Survey of Self-Supervised and Few-Shot Object Detection", "abstract":
      "Labeling data is often expensive and time-consuming, especially for tasks such
      as object detection and instance segmentation, which require dense labeling
      of the image. While few-shot object detection is about training a model on novel
      (unseen) object classes with little data, it still requires prior training on
      many labeled examples of base (seen) classes. On the other hand, self-supervised
      methods aim at learning representations from unlabeled data which transfer well
      to downstream tasks such as object detection. Combining few-shot and self-supervised
      object detection is a promising research direction. In this survey, we review
      and characterize the most recent approaches on few-shot and self-supervised
      object detection. Then, we give our main takeaways and discuss future research
      directions. Project page: https://gabrielhuang.github.io/fsod-survey/.", "venue":
      "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": 2021,
      "referenceCount": 152, "citationCount": 37, "influentialCitationCount": 5, "isOpenAccess":
      true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2110.14711", "status":
      null}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Medicine",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2021-10-27",
      "journal": {"volume": "45", "pages": "4071-4089", "name": "IEEE Transactions
      on Pattern Analysis and Machine Intelligence"}, "authors": [{"authorId": "24040986",
      "name": "Gabriel Huang"}, {"authorId": "3266173", "name": "I. Laradji"}, {"authorId":
      "146560965", "name": "David V\u00e1zquez"}, {"authorId": "1388317459", "name":
      "Simon Lacoste-Julien"}, {"authorId": "117849477", "name": "Pau Rodr\u00edguez
      L\u00f3pez"}]}}, {"intents": [], "isInfluential": false, "contexts": [], "citedPaper":
      {"paperId": "6ad3586b9b1364de329101cae89f09712f510efc", "externalIds": {"DBLP":
      "conf/clear2/LachapelleRSEPL22", "ArXiv": "2107.10098", "CorpusId": 244400763},
      "corpusId": 244400763, "publicationVenue": {"id": "3d07319c-4f2a-4f30-b619-c295ccd29367",
      "name": "CLEaR", "type": "conference", "alternate_names": ["Classification of
      Events, Activities and Relationships", "CLEAR", "CLeaR", "Conf Causal Learn
      Reason", "Classif Event Act Relatsh", "Conference on Causal Learning and Reasoning"],
      "issn": "2453-7128", "url": "http://www.jolace.com/publications/clear/", "alternate_urls":
      ["https://www.cclear.cc/"]}, "url": "https://www.semanticscholar.org/paper/6ad3586b9b1364de329101cae89f09712f510efc",
      "title": "Disentanglement via Mechanism Sparsity Regularization: A New Principle
      for Nonlinear ICA", "abstract": "This work introduces a novel principle we call
      disentanglement via mechanism sparsity regularization, which can be applied
      when the latent factors of interest depend sparsely on past latent factors and/or
      observed auxiliary variables. We propose a representation learning method that
      induces disentanglement by simultaneously learning the latent factors and the
      sparse causal graphical model that relates them. We develop a rigorous identifiability
      theory, building on recent nonlinear independent component analysis (ICA) results,
      that formalizes this principle and shows how the latent variables can be recovered
      up to permutation if one regularizes the latent mechanisms to be sparse and
      if some graph connectivity criterion is satisfied by the data generating process.
      As a special case of our framework, we show how one can leverage unknown-target
      interventions on the latent factors to disentangle them, thereby drawing further
      connections between ICA and causality. We propose a VAE-based method in which
      the latent mechanisms are learned and regularized via binary masks, and validate
      our theory by showing it learns disentangled representations in simulations.",
      "venue": "CLEaR", "year": 2021, "referenceCount": 52, "citationCount": 74, "influentialCitationCount":
      14, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2021-07-21", "journal": {"pages": "428-484"}, "authors":
      [{"authorId": "134730235", "name": "S\u00e9bastien Lachapelle"}, {"authorId":
      "2121428093", "name": "Pau Rodr''iguez L''opez"}, {"authorId": "49738125", "name":
      "Yash Sharma"}, {"authorId": "98029815", "name": "K. Everett"}, {"authorId":
      "10712297", "name": "R\u00e9mi Le Priol"}, {"authorId": "8651990", "name": "Alexandre
      Lacoste"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"intents":
      [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId": "f91ca1bfe2c3f857b018948bb152c1d9a74b4018",
      "externalIds": {"DBLP": "journals/corr/abs-2107-00052", "ArXiv": "2107.00052",
      "CorpusId": 235694268}, "corpusId": 235694268, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/f91ca1bfe2c3f857b018948bb152c1d9a74b4018",
      "title": "Stochastic Gradient Descent-Ascent and Consensus Optimization for
      Smooth Games: Convergence Analysis under Expected Co-coercivity", "abstract":
      "Two of the most prominent algorithms for solving unconstrained smooth games
      are the classical stochastic gradient descent-ascent (SGDA) and the recently
      introduced stochastic consensus optimization (SCO) [Mescheder et al., 2017].
      SGDA is known to converge to a stationary point for specific classes of games,
      but current convergence analyses require a bounded variance assumption. SCO
      is used successfully for solving large-scale adversarial problems, but its convergence
      guarantees are limited to its deterministic variant. In this work, we introduce
      the expected co-coercivity condition, explain its benefits, and provide the
      first last-iterate convergence guarantees of SGDA and SCO under this condition
      for solving a class of stochastic variational inequality problems that are potentially
      non-monotone. We prove linear convergence of both methods to a neighborhood
      of the solution when they use constant step-size, and we propose insightful
      stepsize-switching rules to guarantee convergence to the exact solution. In
      addition, our convergence guarantees hold under the arbitrary sampling paradigm,
      and as such, we give insights into the complexity of minibatching.", "venue":
      "Neural Information Processing Systems", "year": 2021, "referenceCount": 82,
      "citationCount": 34, "influentialCitationCount": 4, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"},
      {"category": "Mathematics", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2021-06-30", "journal": {"pages": "19095-19108"},
      "authors": [{"authorId": "1941365", "name": "Nicolas Loizou"}, {"authorId":
      "40201329", "name": "Hugo Berard"}, {"authorId": "8150760", "name": "Gauthier
      Gidel"}, {"authorId": "3168518", "name": "Ioannis Mitliagkas"}, {"authorId":
      "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"intents": ["background"],
      "isInfluential": false, "contexts": ["Recently a more advanced styleGAN architecture
      has been proposed [32, 18, 33]."], "citedPaper": {"paperId": "c1ff08b59f00c44f34dfdde55cd53370733a2c19",
      "externalIds": {"MAG": "3174807077", "DBLP": "conf/nips/KarrasALHHLA21", "ArXiv":
      "2106.12423", "CorpusId": 235606261}, "corpusId": 235606261, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/c1ff08b59f00c44f34dfdde55cd53370733a2c19",
      "title": "Alias-Free Generative Adversarial Networks", "abstract": "We observe
      that despite their hierarchical convolutional nature, the synthesis process
      of typical generative adversarial networks depends on absolute pixel coordinates
      in an unhealthy manner. This manifests itself as, e.g., detail appearing to
      be glued to image coordinates instead of the surfaces of depicted objects. We
      trace the root cause to careless signal processing that causes aliasing in the
      generator network. Interpreting all signals in the network as continuous, we
      derive generally applicable, small architectural changes that guarantee that
      unwanted information cannot leak into the hierarchical synthesis process. The
      resulting networks match the FID of StyleGAN2 but differ dramatically in their
      internal representations, and they are fully equivariant to translation and
      rotation even at subpixel scales. Our results pave the way for generative models
      better suited for video and animation.", "venue": "Neural Information Processing
      Systems", "year": 2021, "referenceCount": 74, "citationCount": 921, "influentialCitationCount":
      129, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2021-06-23", "journal": {"pages": "852-863"}, "authors":
      [{"authorId": "2976930", "name": "Tero Karras"}, {"authorId": "1907688", "name":
      "M. Aittala"}, {"authorId": "36436218", "name": "S. Laine"}, {"authorId": "103642338",
      "name": "Erik H\u00e4rk\u00f6nen"}, {"authorId": "1454226629", "name": "Janne
      Hellsten"}, {"authorId": "49244945", "name": "J. Lehtinen"}, {"authorId": "1761103",
      "name": "Timo Aila"}]}}, {"intents": [], "isInfluential": false, "contexts":
      [], "citedPaper": {"paperId": "90fb86330c9ad808c6b14b0fb05e82beefaa1701", "externalIds":
      {"ArXiv": "2105.11646", "DBLP": "journals/corr/abs-2105-11646", "CorpusId":
      235187166}, "corpusId": 235187166, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/90fb86330c9ad808c6b14b0fb05e82beefaa1701",
      "title": "Structured Convolutional Kernel Networks for Airline Crew Scheduling",
      "abstract": "Motivated by the needs from an airline crew scheduling application,
      we introduce structured convolutional kernel networks (Struct-CKN), which combine
      CKNs from Mairal et al. (2014) in a structured prediction framework that supports
      constraints on the outputs. CKNs are a particular kind of convolutional neural
      networks that approximate a kernel feature map on training data, thus combining
      properties of deep learning with the non-parametric flexibility of kernel methods.
      Extending CKNs to structured outputs allows us to obtain useful initial solutions
      on a flight-connection dataset that can be further refined by an airline crew
      scheduling solver. More specifically, we use a flight-based network modeled
      as a general conditional random field capable of incorporating local constraints
      in the learning process. Our experiments demonstrate that this approach yields
      significant improvements for the large-scale crew pairing problem (50,000 flights
      per month) over standard approaches, reducing the solution cost by 17% (a gain
      of millions of dollars) and the cost of global constraints by 97%.", "venue":
      "International Conference on Machine Learning", "year": 2021, "referenceCount":
      49, "citationCount": 7, "influentialCitationCount": 0, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2021-05-25", "journal": {"volume": "abs/2105.11646",
      "name": "ArXiv"}, "authors": [{"authorId": "2105163656", "name": "Yassine Yaakoubi"},
      {"authorId": "1447920098", "name": "F. Soumis"}, {"authorId": "1388317459",
      "name": "Simon Lacoste-Julien"}]}}, {"intents": ["methodology", "background"],
      "isInfluential": false, "contexts": ["One may also use StyleGAN for image editing
      by calculating the latent vector of a given input image in the styleGAN (this
      operation is known as styleGAN inversion) and then manipulating this vector
      for editing the image [75, 60, 1, 68, 2, 65, 64, 54].", "It was proved successful
      in many domains such as computer vision [14, 29, 25, 38], semantic segmentation
      [39, 27, 70, 24], time-series synthesis [9, 23], image editing [61, 36, 19,
      3, 75], natural language processing [15, 28, 22], text-to-image generation [59,
      58, 54], and many more."], "citedPaper": {"paperId": "aaa99de83292370a964fcaa51e6e866a96726bb2",
      "externalIds": {"ArXiv": "2103.17249", "DBLP": "conf/iccv/PatashnikWSCL21",
      "DOI": "10.1109/ICCV48922.2021.00209", "CorpusId": 232428282}, "corpusId": 232428282,
      "publicationVenue": {"id": "7654260e-79f9-45c5-9663-d72027cf88f3", "name": "IEEE
      International Conference on Computer Vision", "type": "conference", "alternate_names":
      ["ICCV", "IEEE Int Conf Comput Vis", "ICCV Workshops", "ICCV Work"], "url":
      "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"}, "url": "https://www.semanticscholar.org/paper/aaa99de83292370a964fcaa51e6e866a96726bb2",
      "title": "StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery", "abstract":
      "Inspired by the ability of StyleGAN to generate highly realistic images in
      a variety of domains, much recent work has focused on understanding how to use
      the latent spaces of StyleGAN to manipulate generated and real images. However,
      discovering semantically meaningful latent manipulations typically involves
      painstaking human examination of the many degrees of freedom, or an annotated
      collection of images for each desired manipulation. In this work, we explore
      leveraging the power of recently introduced Contrastive Language-Image Pre-training
      (CLIP) models in order to develop a text-based interface for StyleGAN image
      manipulation that does not require such manual effort. We first introduce an
      optimization scheme that utilizes a CLIP-based loss to modify an input latent
      vector in response to a user-provided text prompt. Next, we describe a latent
      mapper that infers a text-guided latent manipulation step for a given input
      image, allowing faster and more stable text-based manipulation. Finally, we
      present a method for mapping text prompts to input-agnostic directions in StyleGAN\u2019s
      style space, enabling interactive text-driven image manipulation. Extensive
      results and comparisons demonstrate the effectiveness of our approaches.", "venue":
      "IEEE International Conference on Computer Vision", "year": 2021, "referenceCount":
      62, "citationCount": 760, "influentialCitationCount": 138, "isOpenAccess": true,
      "openAccessPdf": {"url": "https://arxiv.org/pdf/2103.17249", "status": null},
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2021-03-31", "journal": {"pages": "2065-2074", "name": "2021 IEEE/CVF International
      Conference on Computer Vision (ICCV)"}, "authors": [{"authorId": "2819477",
      "name": "Or Patashnik"}, {"authorId": "34815981", "name": "Zongze Wu"}, {"authorId":
      "2177801", "name": "Eli Shechtman"}, {"authorId": "1388323541", "name": "D.
      Cohen-Or"}, {"authorId": "70018371", "name": "D. Lischinski"}]}}, {"intents":
      [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId": "b6c4a3f913ee819afa49e115f9739fe31ecf13f5",
      "externalIds": {"DBLP": "journals/corr/abs-2103-09027", "MAG": "3129012083",
      "ArXiv": "2103.09027", "CorpusId": 232240622}, "corpusId": 232240622, "publicationVenue":
      {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
      on Learning Representations", "type": "conference", "alternate_names": ["Int
      Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/b6c4a3f913ee819afa49e115f9739fe31ecf13f5",
      "title": "Repurposing Pretrained Models for Robust Out-of-domain Few-Shot Learning",
      "abstract": "Model-agnostic meta-learning (MAML) is a popular method for few-shot
      learning but assumes that we have access to the meta-training set. In practice,
      training on the meta-training set may not always be an option due to data privacy
      concerns, intellectual property issues, or merely lack of computing resources.
      In this paper, we consider the novel problem of repurposing pretrained MAML
      checkpoints to solve new few-shot classification tasks. Because of the potential
      distribution mismatch, the original MAML steps may no longer be optimal. Therefore
      we propose an alternative meta-testing procedure and combine MAML gradient steps
      with adversarial training and uncertainty-based stepsize adaptation. Our method
      outperforms \"vanilla\" MAML on same-domain and cross-domains benchmarks using
      both SGD and Adam optimizers and shows improved robustness to the choice of
      base stepsize.", "venue": "International Conference on Learning Representations",
      "year": 2021, "referenceCount": 38, "citationCount": 4, "influentialCitationCount":
      0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2021-03-16", "journal": {"volume": "abs/2103.09027",
      "name": "ArXiv"}, "authors": [{"authorId": "2060921177", "name": "Namyeong Kwon"},
      {"authorId": "2534108", "name": "Hwidong Na"}, {"authorId": "24040986", "name":
      "Gabriel Huang"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}},
      {"intents": [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId":
      "c02ade047261fb3a199219ae1274d70529d0a26c", "externalIds": {"DBLP": "conf/iclr/MladenovicBBHLV22",
      "ArXiv": "2103.02014", "CorpusId": 232105154}, "corpusId": 232105154, "publicationVenue":
      {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
      on Learning Representations", "type": "conference", "alternate_names": ["Int
      Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/c02ade047261fb3a199219ae1274d70529d0a26c",
      "title": "Online Adversarial Attacks", "abstract": "Adversarial attacks expose
      important vulnerabilities of deep learning models, yet little attention has
      been paid to settings where data arrives as a stream. In this paper, we formalize
      the online adversarial attack problem, emphasizing two key elements found in
      real-world use-cases: attackers must operate under partial knowledge of the
      target model, and the decisions made by the attacker are irrevocable since they
      operate on a transient data stream. We first rigorously analyze a deterministic
      variant of the online threat model by drawing parallels to the well-studied
      $k$-secretary problem in theoretical computer science and propose Virtual+,
      a simple yet practical online algorithm. Our main theoretical result shows Virtual+
      yields provably the best competitive ratio over all single-threshold algorithms
      for $k<5$ -- extending the previous analysis of the $k$-secretary problem. We
      also introduce the \\textit{stochastic $k$-secretary} -- effectively reducing
      online blackbox transfer attacks to a $k$-secretary problem under noise -- and
      prove theoretical bounds on the performance of Virtual+ adapted to this setting.
      Finally, we complement our theoretical results by conducting experiments on
      MNIST, CIFAR-10, and Imagenet classifiers, revealing the necessity of online
      algorithms in achieving near-optimal performance and also the rich interplay
      between attack strategies and online attack selection, enabling simple strategies
      like FGSM to outperform stronger adversaries.", "venue": "International Conference
      on Learning Representations", "year": 2021, "referenceCount": 47, "citationCount":
      9, "influentialCitationCount": 1, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2021-03-02", "journal": {"volume": "abs/2103.02014", "name": "ArXiv"}, "authors":
      [{"authorId": "1643966780", "name": "Andjela Mladenovic"}, {"authorId": "26418299",
      "name": "A. Bose"}, {"authorId": "40201329", "name": "Hugo Berard"}, {"authorId":
      "49437682", "name": "William L. Hamilton"}, {"authorId": "1388317459", "name":
      "Simon Lacoste-Julien"}, {"authorId": "120247189", "name": "Pascal Vincent"},
      {"authorId": "8150760", "name": "Gauthier Gidel"}]}}, {"intents": ["background"],
      "isInfluential": false, "contexts": ["It was proved successful in many domains
      such as computer vision [14, 29, 25, 38], semantic segmentation [39, 27, 70,
      24], time-series synthesis [9, 23], image editing [61, 36, 19, 3, 75], natural
      language processing [15, 28, 22], text-to-image generation [59, 58, 54], and
      many more."], "citedPaper": {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
      "externalIds": {"ArXiv": "2103.00020", "DBLP": "conf/icml/RadfordKHRGASAM21",
      "CorpusId": 231591445}, "corpusId": 231591445, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "abstract": "State-of-the-art computer vision systems are trained to predict
      a fixed set of predetermined object categories. This restricted form of supervision
      limits their generality and usability since additional labeled data is needed
      to specify any other visual concept. Learning directly from raw text about images
      is a promising alternative which leverages a much broader source of supervision.
      We demonstrate that the simple pre-training task of predicting which caption
      goes with which image is an efficient and scalable way to learn SOTA image representations
      from scratch on a dataset of 400 million (image, text) pairs collected from
      the internet. After pre-training, natural language is used to reference learned
      visual concepts (or describe new ones) enabling zero-shot transfer of the model
      to downstream tasks. We study the performance of this approach by benchmarking
      on over 30 different existing computer vision datasets, spanning tasks such
      as OCR, action recognition in videos, geo-localization, and many types of fine-grained
      object classification. The model transfers non-trivially to most tasks and is
      often competitive with a fully supervised baseline without the need for any
      dataset specific training. For instance, we match the accuracy of the original
      ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million
      training examples it was trained on. We release our code and pre-trained model
      weights at https://github.com/OpenAI/CLIP.", "venue": "International Conference
      on Machine Learning", "year": 2021, "referenceCount": 221, "citationCount":
      9253, "influentialCitationCount": 3170, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2021-02-26", "journal": {"pages": "8748-8763"}, "authors":
      [{"authorId": "38909097", "name": "Alec Radford"}, {"authorId": "2110935237",
      "name": "Jong Wook Kim"}, {"authorId": "2004021329", "name": "Chris Hallacy"},
      {"authorId": "1992922591", "name": "A. Ramesh"}, {"authorId": "40087786", "name":
      "Gabriel Goh"}, {"authorId": "144517868", "name": "Sandhini Agarwal"}, {"authorId":
      "144864359", "name": "Girish Sastry"}, {"authorId": "119609682", "name": "Amanda
      Askell"}, {"authorId": "2051714782", "name": "Pamela Mishkin"}, {"authorId":
      "2115193883", "name": "Jack Clark"}, {"authorId": "2064404342", "name": "Gretchen
      Krueger"}, {"authorId": "1701686", "name": "Ilya Sutskever"}]}}, {"intents":
      ["background"], "isInfluential": false, "contexts": ["It was proved successful
      in many domains such as computer vision [14, 29, 25, 38], semantic segmentation
      [39, 27, 70, 24], time-series synthesis [9, 23], image editing [61, 36, 19,
      3, 75], natural language processing [15, 28, 22], text-to-image generation [59,
      58, 54], and many more."], "citedPaper": {"paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
      "externalIds": {"DBLP": "journals/corr/abs-2102-12092", "MAG": "3170016573",
      "ArXiv": "2102.12092", "CorpusId": 232035663}, "corpusId": 232035663, "publicationVenue":
      {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
      on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
      Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
      "title": "Zero-Shot Text-to-Image Generation", "abstract": "Text-to-image generation
      has traditionally focused on finding better modeling assumptions for training
      on a fixed dataset. These assumptions might involve complex architectures, auxiliary
      losses, or side information such as object part labels or segmentation masks
      supplied during training. We describe a simple approach for this task based
      on a transformer that autoregressively models the text and image tokens as a
      single stream of data. With sufficient data and scale, our approach is competitive
      with previous domain-specific models when evaluated in a zero-shot fashion.",
      "venue": "International Conference on Machine Learning", "year": 2021, "referenceCount":
      64, "citationCount": 2367, "influentialCitationCount": 300, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2021-02-24", "journal": {"volume": "abs/2102.12092",
      "name": "ArXiv"}, "authors": [{"authorId": "1992922591", "name": "A. Ramesh"},
      {"authorId": "2068123790", "name": "Mikhail Pavlov"}, {"authorId": "40087786",
      "name": "Gabriel Goh"}, {"authorId": "145565184", "name": "S. Gray"}, {"authorId":
      "153387869", "name": "Chelsea Voss"}, {"authorId": "38909097", "name": "Alec
      Radford"}, {"authorId": "2108828435", "name": "Mark Chen"}, {"authorId": "1701686",
      "name": "Ilya Sutskever"}]}}, {"intents": ["background"], "isInfluential": false,
      "contexts": ["It is worth mentioning also normalizing flows [51] and score based
      generative models [67, 66, 48], which become very popular recently and show
      competitive performance with GANs."], "citedPaper": {"paperId": "de18baa4964804cf471d85a5a090498242d2e79f",
      "externalIds": {"DBLP": "conf/icml/NicholD21", "ArXiv": "2102.09672", "CorpusId":
      231979499}, "corpusId": 231979499, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/de18baa4964804cf471d85a5a090498242d2e79f",
      "title": "Improved Denoising Diffusion Probabilistic Models", "abstract": "Denoising
      diffusion probabilistic models (DDPM) are a class of generative models which
      have recently been shown to produce excellent samples. We show that with a few
      simple modifications, DDPMs can also achieve competitive log-likelihoods while
      maintaining high sample quality. Additionally, we find that learning variances
      of the reverse diffusion process allows sampling with an order of magnitude
      fewer forward passes with a negligible difference in sample quality, which is
      important for the practical deployment of these models. We additionally use
      precision and recall to compare how well DDPMs and GANs cover the target distribution.
      Finally, we show that the sample quality and likelihood of these models scale
      smoothly with model capacity and training compute, making them easily scalable.
      We release our code at https://github.com/openai/improved-diffusion", "venue":
      "International Conference on Machine Learning", "year": 2021, "referenceCount":
      47, "citationCount": 1361, "influentialCitationCount": 200, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-02-18",
      "journal": {"volume": "abs/2102.09672", "name": "ArXiv"}, "authors": [{"authorId":
      "38967461", "name": "Alex Nichol"}, {"authorId": "6515819", "name": "Prafulla
      Dhariwal"}]}}, {"intents": ["methodology"], "isInfluential": false, "contexts":
      ["One may also use StyleGAN for image editing by calculating the latent vector
      of a given input image in the styleGAN (this operation is known as styleGAN
      inversion) and then manipulating this vector for editing the image [75, 60,
      1, 68, 2, 65, 64, 54]."], "citedPaper": {"paperId": "1e7e1a7ed075edfb87440b6b98f2a94a48f74a57",
      "externalIds": {"DBLP": "journals/corr/abs-2102-02766", "ArXiv": "2102.02766",
      "DOI": "10.1145/3476576.3476706", "CorpusId": 231802331}, "corpusId": 231802331,
      "publicationVenue": {"id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d", "name": "ACM
      Transactions on Graphics", "type": "journal", "alternate_names": ["ACM Trans
      Graph"], "issn": "0730-0301", "url": "http://www.acm.org/tog/", "alternate_urls":
      ["http://portal.acm.org/tog", "https://tog.acm.org/"]}, "url": "https://www.semanticscholar.org/paper/1e7e1a7ed075edfb87440b6b98f2a94a48f74a57",
      "title": "Designing an encoder for StyleGAN image manipulation", "abstract":
      "Recently, there has been a surge of diverse methods for performing image editing
      by employing pre-trained unconditional generators. Applying these methods on
      real images, however, remains a challenge, as it necessarily requires the inversion
      of the images into their latent space. To successfully invert a real image,
      one needs to find a latent code that reconstructs the input image accurately,
      and more importantly, allows for its meaningful manipulation. In this paper,
      we carefully study the latent space of StyleGAN, the state-of-the-art unconditional
      generator. We identify and analyze the existence of a distortion-editability
      tradeoff and a distortion-perception tradeoff within the StyleGAN latent space.
      We then suggest two principles for designing encoders in a manner that allows
      one to control the proximity of the inversions to regions that StyleGAN was
      originally trained on. We present an encoder based on our two principles that
      is specifically designed for facilitating editing on real images by balancing
      these tradeoffs. By evaluating its performance qualitatively and quantitatively
      on numerous challenging domains, including cars and horses, we show that our
      inversion method, followed by common editing techniques, achieves superior real-image
      editing quality, with only a small reconstruction accuracy drop.", "venue":
      "ACM Transactions on Graphics", "year": 2021, "referenceCount": 55, "citationCount":
      490, "influentialCitationCount": 91, "isOpenAccess": true, "openAccessPdf":
      {"url": "https://arxiv.org/pdf/2102.02766", "status": null}, "fieldsOfStudy":
      ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2021-02-04", "journal": {"volume": "40",
      "pages": "1 - 14", "name": "ACM Transactions on Graphics (TOG)"}, "authors":
      [{"authorId": "2047833213", "name": "Omer Tov"}, {"authorId": "1850630812",
      "name": "Yuval Alaluf"}, {"authorId": "1702992975", "name": "Yotam Nitzan"},
      {"authorId": "2819477", "name": "Or Patashnik"}, {"authorId": "1388323541",
      "name": "D. Cohen-Or"}]}}, {"intents": ["methodology", "background"], "isInfluential":
      false, "contexts": ["One may also use StyleGAN for image editing by calculating
      the latent vector of a given input image in the styleGAN (this operation is
      known as styleGAN inversion) and then manipulating this vector for editing the
      image [75, 60, 1, 68, 2, 65, 64, 54].", "It was proved successful in many domains
      such as computer vision [14, 29, 25, 38], semantic segmentation [39, 27, 70,
      24], time-series synthesis [9, 23], image editing [61, 36, 19, 3, 75], natural
      language processing [15, 28, 22], text-to-image generation [59, 58, 54], and
      many more."], "citedPaper": {"paperId": "a3ef5a321876738a6b257de5e1eebc4a8aa5b907",
      "externalIds": {"DBLP": "journals/pami/XiaZYXZY23", "ArXiv": "2101.05278", "DOI":
      "10.1109/TPAMI.2022.3181070", "CorpusId": 231603119, "PubMed": "37022469"},
      "corpusId": 231603119, "publicationVenue": {"id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
      "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "type":
      "journal", "alternate_names": ["IEEE Trans Pattern Anal Mach Intell"], "issn":
      "0162-8828", "url": "http://www.computer.org/tpami/", "alternate_urls": ["http://www.computer.org/portal/web/tpami",
      "http://ieeexplore.ieee.org/servlet/opac?punumber=34"]}, "url": "https://www.semanticscholar.org/paper/a3ef5a321876738a6b257de5e1eebc4a8aa5b907",
      "title": "GAN Inversion: A Survey", "abstract": "GAN inversion aims to invert
      a given image back into the latent space of a pretrained GAN model so that the
      image can be faithfully reconstructed from the inverted code by the generator.
      As an emerging technique to bridge the real and fake image domains, GAN inversion
      plays an essential role in enabling pretrained GAN models, such as StyleGAN
      and BigGAN, for applications of real image editing. Moreover, GAN inversion
      interprets GAN''s latent space and examines how realistic images can be generated.
      In this paper, we provide a survey of GAN inversion with a focus on its representative
      algorithms and its applications in image restoration and image manipulation.
      We further discuss the trends and challenges for future research. A curated
      list of GAN inversion methods, datasets, and other related information can be
      found at https://github.com/weihaox/awesome-gan-inversion.", "venue": "IEEE
      Transactions on Pattern Analysis and Machine Intelligence", "year": 2021, "referenceCount":
      306, "citationCount": 310, "influentialCitationCount": 12, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Medicine", "Computer Science"], "s2FieldsOfStudy":
      [{"category": "Medicine", "source": "external"}, {"category": "Computer Science",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2021-01-14",
      "journal": {"volume": "45", "pages": "3121-3138", "name": "IEEE Transactions
      on Pattern Analysis and Machine Intelligence"}, "authors": [{"authorId": "50875615",
      "name": "Weihao Xia"}, {"authorId": "2129519081", "name": "Yulun Zhang"}, {"authorId":
      "3001727", "name": "Yujiu Yang"}, {"authorId": "1891766", "name": "Jing-Hao
      Xue"}, {"authorId": "145291669", "name": "Bolei Zhou"}, {"authorId": "1715634",
      "name": "Ming-Hsuan Yang"}]}}, {"intents": ["background"], "isInfluential":
      false, "contexts": ["It is worth mentioning also normalizing \ufb02ows [51]
      and score based generative models [67, 66, 48], which become very popular recently
      and show competitive performance with GANs."], "citedPaper": {"paperId": "633e2fbfc0b21e959a244100937c5853afca4853",
      "externalIds": {"DBLP": "journals/corr/abs-2011-13456", "ArXiv": "2011.13456",
      "MAG": "3110257065", "CorpusId": 227209335}, "corpusId": 227209335, "publicationVenue":
      {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
      on Learning Representations", "type": "conference", "alternate_names": ["Int
      Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/633e2fbfc0b21e959a244100937c5853afca4853",
      "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
      "abstract": "Creating noise from data is easy; creating data from noise is generative
      modeling. We present a stochastic differential equation (SDE) that smoothly
      transforms a complex data distribution to a known prior distribution by slowly
      injecting noise, and a corresponding reverse-time SDE that transforms the prior
      distribution back into the data distribution by slowly removing the noise. Crucially,
      the reverse-time SDE depends only on the time-dependent gradient field (\\aka,
      score) of the perturbed data distribution. By leveraging advances in score-based
      generative modeling, we can accurately estimate these scores with neural networks,
      and use numerical SDE solvers to generate samples. We show that this framework
      encapsulates previous approaches in score-based generative modeling and diffusion
      probabilistic modeling, allowing for new sampling procedures and new modeling
      capabilities. In particular, we introduce a predictor-corrector framework to
      correct errors in the evolution of the discretized reverse-time SDE. We also
      derive an equivalent neural ODE that samples from the same distribution as the
      SDE, but additionally enables exact likelihood computation, and improved sampling
      efficiency. In addition, we provide a new way to solve inverse problems with
      score-based models, as demonstrated with experiments on class-conditional generation,
      image inpainting, and colorization. Combined with multiple architectural improvements,
      we achieve record-breaking performance for unconditional image generation on
      CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood
      of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images
      for the first time from a score-based generative model.", "venue": "International
      Conference on Learning Representations", "year": 2020, "referenceCount": 66,
      "citationCount": 2077, "influentialCitationCount": 551, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2020-11-26", "journal":
      {"volume": "abs/2011.13456", "name": "ArXiv"}, "authors": [{"authorId": "115504645",
      "name": "Yang Song"}, {"authorId": "1407546424", "name": "Jascha Narain Sohl-Dickstein"},
      {"authorId": "1726807", "name": "Diederik P. Kingma"}, {"authorId": "2109224633",
      "name": "Abhishek Kumar"}, {"authorId": "2490652", "name": "Stefano Ermon"},
      {"authorId": "16443937", "name": "Ben Poole"}]}}, {"intents": [], "isInfluential":
      false, "contexts": [], "citedPaper": {"paperId": "96e2e3f54b76992e95a447d50c0ac082e6f690b7",
      "externalIds": {"DBLP": "conf/icml/KerdreuxLLS21", "MAG": "3102110737", "ArXiv":
      "2011.03351", "CorpusId": 226278300}, "corpusId": 226278300, "publicationVenue":
      {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
      on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
      Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/96e2e3f54b76992e95a447d50c0ac082e6f690b7",
      "title": "Affine Invariant Analysis of Frank-Wolfe on Strongly Convex Sets",
      "abstract": "It is known that the Frank-Wolfe (FW) algorithm, which is affine-covariant,
      enjoys accelerated convergence rates when the constraint set is strongly convex.
      However, these results rely on norm-dependent assumptions, usually incurring
      non-affine invariant bounds, in contradiction with FW''s affine-covariant property.
      In this work, we introduce new structural assumptions on the problem (such as
      the directional smoothness) and derive an affine invariant, norm-independent
      analysis of Frank-Wolfe. Based on our analysis, we propose an affine invariant
      backtracking line-search. Interestingly, we show that typical backtracking line-searches
      using smoothness of the objective function surprisingly converge to an affine
      invariant step size, despite using affine-dependent norms in the step size''s
      computation. This indicates that we do not necessarily need to know the set''s
      structure in advance to enjoy the affine-invariant accelerated rate.", "venue":
      "International Conference on Machine Learning", "year": 2020, "referenceCount":
      57, "citationCount": 14, "influentialCitationCount": 0, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-11-06",
      "journal": {"pages": "5398-5408"}, "authors": [{"authorId": "40900821", "name":
      "Thomas Kerdreux"}, {"authorId": "2145377749", "name": "Lewis Liu"}, {"authorId":
      "1388317459", "name": "Simon Lacoste-Julien"}, {"authorId": "7665349", "name":
      "Damien Scieur"}]}}, {"intents": ["background"], "isInfluential": false, "contexts":
      ["Text style may be perceived as a linguistic variation while preserving the
      conceptual content of the text [10], or more formally, we can describe text
      from the linguistic view."], "citedPaper": {"paperId": "429e489ffe8eb8c876a1d89d9a4f3a1f1621b7fa",
      "externalIds": {"DBLP": "journals/corr/abs-2010-12742", "MAG": "3093908736",
      "CorpusId": 260441157}, "corpusId": 260441157, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/429e489ffe8eb8c876a1d89d9a4f3a1f1621b7fa",
      "title": "Text Style Transfer: A Review and Experiment Evaluation", "abstract":
      "The stylistic properties of text have intrigued computational linguistics researchers
      in recent years. Specifically, researchers have investigated the Text Style
      Transfer (TST) task, which aims to change the stylistic properties of the text
      while retaining its style independent content. Over the last few years, many
      novel TST algorithms have been developed, while the industry has leveraged these
      algorithms to enable exciting TST applications. The field of TST research has
      burgeoned because of this symbiosis. This article aims to provide a comprehensive
      review of recent research efforts on text style transfer. More concretely, we
      create a taxonomy to organize the TST models and provide a comprehensive summary
      of the state of the art. We review the existing evaluation methodologies for
      TST tasks and conduct a large-scale reproducibility study where we experimentally
      benchmark 19 state-of-the-art TST algorithms on two publicly available datasets.
      Finally, we expand on current trends and provide new perspectives on the new
      and exciting developments in the TST field.", "venue": "arXiv.org", "year":
      2020, "referenceCount": 93, "citationCount": 18, "influentialCitationCount":
      1, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"],
      "publicationDate": "2020-10-24", "journal": {"volume": "abs/2010.12742", "name":
      "ArXiv"}, "authors": [{"authorId": "1557412457", "name": "Zhiqiang Hu"}, {"authorId":
      "38656724", "name": "R. Lee"}, {"authorId": "1682418", "name": "C. Aggarwal"}]}},
      {"intents": ["background"], "isInfluential": false, "contexts": ["It is worth
      mentioning also normalizing flows [51] and score based generative models [67,
      66, 48], which become very popular recently and show competitive performance
      with GANs."], "citedPaper": {"paperId": "014576b866078524286802b1d0e18628520aa886",
      "externalIds": {"ArXiv": "2010.02502", "DBLP": "journals/corr/abs-2010-02502",
      "MAG": "3092442149", "CorpusId": 222140788}, "corpusId": 222140788, "publicationVenue":
      {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
      on Learning Representations", "type": "conference", "alternate_names": ["Int
      Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/014576b866078524286802b1d0e18628520aa886",
      "title": "Denoising Diffusion Implicit Models", "abstract": "Denoising diffusion
      probabilistic models (DDPMs) have achieved high quality image generation without
      adversarial training, yet they require simulating a Markov chain for many steps
      to produce a sample. To accelerate sampling, we present denoising diffusion
      implicit models (DDIMs), a more efficient class of iterative implicit probabilistic
      models with the same training procedure as DDPMs. In DDPMs, the generative process
      is defined as the reverse of a Markovian diffusion process. We construct a class
      of non-Markovian diffusion processes that lead to the same training objective,
      but whose reverse process can be much faster to sample from. We empirically
      demonstrate that DDIMs can produce high quality samples $10 \\times$ to $50
      \\times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade
      off computation for sample quality, and can perform semantically meaningful
      image interpolation directly in the latent space.", "venue": "International
      Conference on Learning Representations", "year": 2020, "referenceCount": 48,
      "citationCount": 1866, "influentialCitationCount": 448, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2020-10-06", "journal":
      {"volume": "abs/2010.02502", "name": "ArXiv"}, "authors": [{"authorId": "51453887",
      "name": "Jiaming Song"}, {"authorId": "2057110631", "name": "Chenlin Meng"},
      {"authorId": "2066211828", "name": "Stefano Ermon"}]}}, {"intents": ["background"],
      "isInfluential": false, "contexts": ["It was proved successful in many domains
      such as computer vision [14, 29, 25, 38], semantic segmentation [39, 27, 70,
      24], time-series synthesis [9, 23], image editing [61, 36, 19, 3, 75], natural
      language processing [15, 28, 22], text-to-image generation [59, 58, 54], and
      many more."], "citedPaper": {"paperId": "8e9e5224bdb37bd454a5ff409824bc05d1e9a979",
      "externalIds": {"DBLP": "journals/corr/abs-2008-02401", "MAG": "3047371217",
      "ArXiv": "2008.02401", "DOI": "10.1145/3447648", "CorpusId": 221006041}, "corpusId":
      221006041, "publicationVenue": {"id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
      "name": "ACM Transactions on Graphics", "type": "journal", "alternate_names":
      ["ACM Trans Graph"], "issn": "0730-0301", "url": "http://www.acm.org/tog/",
      "alternate_urls": ["http://portal.acm.org/tog", "https://tog.acm.org/"]}, "url":
      "https://www.semanticscholar.org/paper/8e9e5224bdb37bd454a5ff409824bc05d1e9a979",
      "title": "StyleFlow: Attribute-conditioned Exploration of StyleGAN-Generated
      Images using Conditional Continuous Normalizing Flows", "abstract": "High-quality,
      diverse, and photorealistic images can now be generated by unconditional GANs
      (e.g., StyleGAN). However, limited options exist to control the generation process
      using (semantic) attributes while still preserving the quality of the output.
      Further, due to the entangled nature of the GAN latent space, performing edits
      along one attribute can easily result in unwanted changes along other attributes.
      In this article, in the context of conditional exploration of entangled latent
      spaces, we investigate the two sub-problems of attribute-conditioned sampling
      and attribute-controlled editing. We present StyleFlow as a simple, effective,
      and robust solution to both the sub-problems by formulating conditional exploration
      as an instance of conditional continuous normalizing flows in the GAN latent
      space conditioned by attribute features. We evaluate our method using the face
      and the car latent space of StyleGAN, and demonstrate fine-grained disentangled
      edits along various attributes on both real photographs and StyleGAN generated
      images. For example, for faces, we vary camera pose, illumination variation,
      expression, facial hair, gender, and age. Finally, via extensive qualitative
      and quantitative comparisons, we demonstrate the superiority of StyleFlow over
      prior and several concurrent works. Project Page and Video: https://rameenabdal.github.io/StyleFlow.",
      "venue": "ACM Transactions on Graphics", "year": 2020, "referenceCount": 77,
      "citationCount": 391, "influentialCitationCount": 51, "isOpenAccess": true,
      "openAccessPdf": {"url": "https://repository.kaust.edu.sa/bitstream/10754/666192/3/3447648.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2020-08-06", "journal": {"volume": "40", "pages": "1 - 21", "name": "ACM Transactions
      on Graphics (TOG)"}, "authors": [{"authorId": "94395014", "name": "Rameen Abdal"},
      {"authorId": "100516590", "name": "Peihao Zhu"}, {"authorId": "1710455", "name":
      "N. Mitra"}, {"authorId": "1798011", "name": "Peter Wonka"}]}}, {"intents":
      [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId": "de41c249161092e5f556ccf977611159f085670e",
      "externalIds": {"DBLP": "conf/aistats/BaratinGLHLVL21", "MAG": "3095344139",
      "CorpusId": 225197372}, "corpusId": 225197372, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
      "name": "International Conference on Artificial Intelligence and Statistics",
      "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
      Stat"]}, "url": "https://www.semanticscholar.org/paper/de41c249161092e5f556ccf977611159f085670e",
      "title": "Implicit Regularization via Neural Feature Alignment", "abstract":
      "We approach the problem of implicit regularization in deep learning from a
      geometrical viewpoint. We highlight a regularization effect induced by a dynamical
      alignment of the neural tangent features introduced by Jacot et al, along a
      small number of task-relevant directions. This can be interpreted as a combined
      mechanism of feature selection and model compression. By extrapolating a new
      analysis of Rademacher complexity bounds for linear models, we motivate and
      study a heuristic complexity measure that captures this phenomenon, in terms
      of sequences of tangent kernel classes along the optimization paths.", "venue":
      "International Conference on Artificial Intelligence and Statistics", "year":
      2020, "referenceCount": 68, "citationCount": 39, "influentialCitationCount":
      8, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2020-08-03", "journal": {"pages": "2269-2277"},
      "authors": [{"authorId": "14398916", "name": "A. Baratin"}, {"authorId": "49917441",
      "name": "Thomas George"}, {"authorId": "40201308", "name": "C\u00e9sar Laurent"},
      {"authorId": "40482726", "name": "R. Devon Hjelm"}, {"authorId": "49921594",
      "name": "Guillaume Lajoie"}, {"authorId": "120247189", "name": "Pascal Vincent"},
      {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"intents": ["methodology",
      "background"], "isInfluential": false, "contexts": ["One may also use StyleGAN
      for image editing by calculating the latent vector of a given input image in
      the styleGAN (this operation is known as styleGAN inversion) and then manipulating
      this vector for editing the image [75, 60, 1, 68, 2, 65, 64, 54].", "Many state-of-the-art
      GAN architectures utilize this type of progressive training scheme, and it has
      resulted in very credible images [29, 30, 8, 32, 60], and more stable learning
      for both D and G."], "citedPaper": {"paperId": "4cc32db67ff82cf1aa160631c35bb315c5add749",
      "externalIds": {"DBLP": "conf/cvpr/RichardsonAPNAS21", "MAG": "3046411017",
      "ArXiv": "2008.00951", "DOI": "10.1109/CVPR46437.2021.00232", "CorpusId": 220936362},
      "corpusId": 220936362, "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
      "name": "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
      ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
      "url": "https://www.semanticscholar.org/paper/4cc32db67ff82cf1aa160631c35bb315c5add749",
      "title": "Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation",
      "abstract": "We present a generic image-to-image translation framework, pixel2style2pixel
      (pSp). Our pSp framework is based on a novel encoder network that directly generates
      a series of style vectors which are fed into a pretrained StyleGAN generator,
      forming the extended $\\mathcal{W} + $ latent space. We first show that our
      encoder can directly embed real images into $\\mathcal{W} + $, with no additional
      optimization. Next, we propose utilizing our encoder to directly solve image-to-image
      translation tasks, defining them as encoding problems from some input domain
      into the latent domain. By deviating from the standard \"invert first, edit
      later\" methodology used with previous StyleGAN encoders, our approach can handle
      a variety of tasks even when the input image is not represented in the StyleGAN
      domain. We show that solving translation tasks through StyleGAN significantly
      simplifies the training process, as no adversary is required, has better support
      for solving tasks without pixel-to-pixel correspondence, and inherently supports
      multi-modal synthesis via the resampling of styles. Finally, we demonstrate
      the potential of our framework on a variety of facial image-to-image translation
      tasks, even when compared to state-of-the-art solutions designed specifically
      for a single task, and further show that it can be extended beyond the human
      facial domain. Code is available at https://github.com/eladrich/pixel2style2pixel.",
      "venue": "Computer Vision and Pattern Recognition", "year": 2020, "referenceCount":
      55, "citationCount": 765, "influentialCitationCount": 149, "isOpenAccess": true,
      "openAccessPdf": {"url": "https://arxiv.org/pdf/2008.00951", "status": null},
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2020-08-03", "journal": {"pages": "2287-2296", "name": "2021 IEEE/CVF Conference
      on Computer Vision and Pattern Recognition (CVPR)"}, "authors": [{"authorId":
      "2511847", "name": "Elad Richardson"}, {"authorId": "1850630812", "name": "Yuval
      Alaluf"}, {"authorId": "2819477", "name": "Or Patashnik"}, {"authorId": "1702992975",
      "name": "Yotam Nitzan"}, {"authorId": "1920175", "name": "Yaniv Azar"}, {"authorId":
      "151221072", "name": "Stav Shapiro"}, {"authorId": "1388323541", "name": "D.
      Cohen-Or"}]}}, {"intents": ["methodology"], "isInfluential": false, "contexts":
      ["One may also use StyleGAN for image editing by calculating the latent vector
      of a given input image in the styleGAN (this operation is known as styleGAN
      inversion) and then manipulating this vector for editing the image [75, 60,
      1, 68, 2, 65, 64, 54]."], "citedPaper": {"paperId": "dc0092d06ab76465431edfd51b08d823b7d1ff3f",
      "externalIds": {"DBLP": "conf/cvpr/ShenZ21", "MAG": "3043243633", "ArXiv": "2007.06600",
      "DOI": "10.1109/CVPR46437.2021.00158", "CorpusId": 220514827}, "corpusId": 220514827,
      "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf", "name": "Computer
      Vision and Pattern Recognition", "type": "conference", "alternate_names": ["CVPR",
      "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
      "url": "https://www.semanticscholar.org/paper/dc0092d06ab76465431edfd51b08d823b7d1ff3f",
      "title": "Closed-Form Factorization of Latent Semantics in GANs", "abstract":
      "A rich set of interpretable dimensions has been shown to emerge in the latent
      space of the Generative Adversarial Networks (GANs) trained for synthesizing
      images. In order to identify such latent dimensions for image editing, previous
      methods typically annotate a collection of synthesized samples and train linear
      classifiers in the latent space. However, they require a clear definition of
      the target attribute as well as the corresponding manual annotations, limiting
      their applications in practice. In this work, we examine the internal representation
      learned by GANs to reveal the underlying variation factors in an unsupervised
      manner. In particular, we take a closer look into the generation mechanism of
      GANs and further propose a closedform factorization algorithm for latent semantic
      discovery by directly decomposing the pre-trained weights. With a lightning-fast
      implementation, our approach is capable of not only finding semantically meaningful
      dimensions comparably to the state-of-the-art supervised methods, but also resulting
      in far more versatile concepts across multiple GAN models trained on a wide
      range of datasets.1", "venue": "Computer Vision and Pattern Recognition", "year":
      2020, "referenceCount": 30, "citationCount": 442, "influentialCitationCount":
      70, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2007.06600",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2020-07-13", "journal": {"pages": "1532-1540", "name": "2021
      IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"}, "authors":
      [{"authorId": "2117687899", "name": "Yujun Shen"}, {"authorId": "145291669",
      "name": "Bolei Zhou"}]}}, {"intents": [], "isInfluential": false, "contexts":
      [], "citedPaper": {"paperId": "97faf5e185a7c28e48573212ecfa06f8f83497f3", "externalIds":
      {"MAG": "3034975784", "DBLP": "conf/icml/LoizouBJVLM20", "ArXiv": "2007.04202",
      "CorpusId": 220403670}, "corpusId": 220403670, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/97faf5e185a7c28e48573212ecfa06f8f83497f3",
      "title": "Stochastic Hamiltonian Gradient Methods for Smooth Games", "abstract":
      "The success of adversarial formulations in machine learning has brought renewed
      motivation for smooth games. In this work, we focus on the class of stochastic
      Hamiltonian methods and provide the first convergence guarantees for certain
      classes of stochastic smooth games. We propose a novel unbiased estimator for
      the stochastic Hamiltonian gradient descent (SHGD) and highlight its benefits.
      Using tools from the optimization literature we show that SHGD converges linearly
      to the neighbourhood of a stationary point. To guarantee convergence to the
      exact solution, we analyze SHGD with a decreasing step-size and we also present
      the first stochastic variance reduced Hamiltonian method. Our results provide
      the first global non-asymptotic last-iterate convergence guarantees for the
      class of stochastic unconstrained bilinear games and for the more general class
      of stochastic games that satisfy a \"sufficiently bilinear\" condition, notably
      including some non-convex non-concave problems. We supplement our analysis with
      experiments on stochastic bilinear and sufficiently bilinear games, where our
      theory is shown to be tight, and on simple adversarial machine learning formulations.",
      "venue": "International Conference on Machine Learning", "year": 2020, "referenceCount":
      77, "citationCount": 41, "influentialCitationCount": 3, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-08",
      "journal": {"volume": "abs/2007.04202", "name": "ArXiv"}, "authors": [{"authorId":
      "1941365", "name": "Nicolas Loizou"}, {"authorId": "40201329", "name": "Hugo
      Berard"}, {"authorId": "1401723615", "name": "Alexia Jolicoeur-Martineau"},
      {"authorId": "120247189", "name": "Pascal Vincent"}, {"authorId": "1388317459",
      "name": "Simon Lacoste-Julien"}, {"authorId": "3168518", "name": "Ioannis Mitliagkas"}]}},
      {"intents": [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId":
      "0da0af5ed59661fe4f901cf330a851e95eac7ea4", "externalIds": {"MAG": "3103069071",
      "DBLP": "conf/nips/BrouillardLLLD20", "ArXiv": "2007.01754", "CorpusId": 220347136},
      "corpusId": 220347136, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/0da0af5ed59661fe4f901cf330a851e95eac7ea4",
      "title": "Differentiable Causal Discovery from Interventional Data", "abstract":
      "Discovering causal relationships in data is a challenging task that involves
      solving a combinatorial problem for which the solution is not always identifiable.
      A new line of work reformulates the combinatorial problem as a continuous constrained
      optimization one, enabling the use of different powerful optimization techniques.
      However, methods based on this idea do not yet make use of interventional data,
      which can significantly alleviate identifiability issues. In this work, we propose
      a neural network-based method for this task that can leverage interventional
      data. We illustrate the flexibility of the continuous-constrained framework
      by taking advantage of expressive neural architectures such as normalizing flows.
      We show that our approach compares favorably to the state of the art in a variety
      of settings, including perfect and imperfect interventions for which the targeted
      nodes may even be unknown.", "venue": "Neural Information Processing Systems",
      "year": 2020, "referenceCount": 58, "citationCount": 111, "influentialCitationCount":
      32, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2020-07-03", "journal": {"volume": "abs/2007.01754", "name": "ArXiv"}, "authors":
      [{"authorId": "23138044", "name": "P. Brouillard"}, {"authorId": "134730235",
      "name": "S\u00e9bastien Lachapelle"}, {"authorId": "8651990", "name": "Alexandre
      Lacoste"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}, {"authorId":
      "14757354", "name": "Alexandre Drouin"}]}}, {"intents": [], "isInfluential":
      false, "contexts": [], "citedPaper": {"paperId": "2a799582775ff5527de1c24167aef0a9a6bf614b",
      "externalIds": {"MAG": "3039130562", "DBLP": "conf/nips/BoseGBCVLH20", "ArXiv":
      "2007.00720", "CorpusId": 220301567}, "corpusId": 220301567, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/2a799582775ff5527de1c24167aef0a9a6bf614b",
      "title": "Adversarial Example Games", "abstract": "The existence of adversarial
      examples capable of fooling trained neural network classifiers calls for a much
      better understanding of possible attacks to guide the development of safeguards
      against them. This includes attack methods in the challenging non-interactive
      blackbox setting, where adversarial attacks are generated without any access,
      including queries, to the target model. Prior attacks in this setting have relied
      mainly on algorithmic innovations derived from empirical observations (e.g.,
      that momentum helps), lacking principled transferability guarantees. In this
      work, we provide a theoretical foundation for crafting transferable adversarial
      examples to entire hypothesis classes. We introduce Adversarial Example Games
      (AEG), a framework that models the crafting of adversarial examples as a min-max
      game between a generator of attacks and a classifier. AEG provides a new way
      to design adversarial examples by adversarially training a generator and a classifier
      from a given hypothesis class (e.g., architecture). We prove that this game
      has an equilibrium, and that the optimal generator is able to craft adversarial
      examples that can attack any classifier from the corresponding hypothesis class.
      We demonstrate the efficacy of AEG on the MNIST and CIFAR-10 datasets, outperforming
      prior state-of-the-art approaches with an average relative improvement of $29.9\\%$
      and $47.2\\%$ against undefended and robust models (Table 2 & 3) respectively.",
      "venue": "Neural Information Processing Systems", "year": 2020, "referenceCount":
      81, "citationCount": 37, "influentialCitationCount": 8, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2020-07-01", "journal": {"volume": "abs/2007.00720", "name": "ArXiv"}, "authors":
      [{"authorId": "26418299", "name": "A. Bose"}, {"authorId": "8150760", "name":
      "Gauthier Gidel"}, {"authorId": "1784615280", "name": "Hugo Berrard"}, {"authorId":
      "35090018", "name": "Andre Cianflone"}, {"authorId": "120247189", "name": "Pascal
      Vincent"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}, {"authorId":
      "49437682", "name": "William L. Hamilton"}]}}, {"intents": ["background"], "isInfluential":
      false, "contexts": ["Data augmentation can be also helpful in the training of
      the GANs themselves [31, 78], which allows to train them with only few examples
      and still get high quality generated data (e."], "citedPaper": {"paperId": "670f9d0d8cafaeaeea564c88645b9816b1146cef",
      "externalIds": {"MAG": "3099088591", "ArXiv": "2006.10738", "DBLP": "journals/corr/abs-2006-10738",
      "CorpusId": 219793068}, "corpusId": 219793068, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/670f9d0d8cafaeaeea564c88645b9816b1146cef",
      "title": "Differentiable Augmentation for Data-Efficient GAN Training", "abstract":
      "The performance of generative adversarial networks (GANs) heavily deteriorates
      given a limited amount of training data. This is mainly because the discriminator
      is memorizing the exact training set. To combat it, we propose Differentiable
      Augmentation (DiffAugment), a simple method that improves the data efficiency
      of GANs by imposing various types of differentiable augmentations on both real
      and fake samples. Previous attempts to directly augment the training data manipulate
      the distribution of real images, yielding little benefit; DiffAugment enables
      us to adopt the differentiable augmentation for the generated samples, effectively
      stabilizes training, and leads to better convergence. Experiments demonstrate
      consistent gains of our method over a variety of GAN architectures and loss
      functions for both unconditional and class-conditional generation. With DiffAugment,
      we achieve a state-of-the-art FID of 6.80 with an IS of 100.8 on ImageNet 128x128.
      Furthermore, with only 20% training data, we can match the top performance on
      CIFAR-10 and CIFAR-100. Finally, our method can generate high-fidelity images
      using only 100 images without pre-training, while being on par with existing
      transfer learning algorithms. Code is available at this https URL.", "venue":
      "Neural Information Processing Systems", "year": 2020, "referenceCount": 57,
      "citationCount": 436, "influentialCitationCount": 68, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2020-06-18", "journal": {"volume": "abs/2006.10738", "name": "ArXiv"}, "authors":
      [{"authorId": "51234052", "name": "Shengyu Zhao"}, {"authorId": "1741412659",
      "name": "Zhijian Liu"}, {"authorId": "2110385919", "name": "Ji Lin"}, {"authorId":
      "2436356", "name": "Jun-Yan Zhu"}, {"authorId": "143840275", "name": "Song Han"}]}},
      {"intents": ["background"], "isInfluential": false, "contexts": ["Data augmentation
      can be also helpful in the training of the GANs themselves [31, 78], which allows
      to train them with only few examples and still get high quality generated data
      (e."], "citedPaper": {"paperId": "29858b40a15704398aecdca6bd2820f2fcc99891",
      "externalIds": {"DBLP": "conf/nips/KarrasAHLLA20", "MAG": "3034720584", "ArXiv":
      "2006.06676", "CorpusId": 219636053}, "corpusId": 219636053, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/29858b40a15704398aecdca6bd2820f2fcc99891",
      "title": "Training Generative Adversarial Networks with Limited Data", "abstract":
      "Training generative adversarial networks (GAN) using too little data typically
      leads to discriminator overfitting, causing training to diverge. We propose
      an adaptive discriminator augmentation mechanism that significantly stabilizes
      training in limited data regimes. The approach does not require changes to loss
      functions or network architectures, and is applicable both when training from
      scratch and when fine-tuning an existing GAN on another dataset. We demonstrate,
      on several datasets, that good results are now possible using only a few thousand
      training images, often matching StyleGAN2 results with an order of magnitude
      fewer images. We expect this to open up new application domains for GANs. We
      also find that the widely used CIFAR-10 is, in fact, a limited data benchmark,
      and improve the record FID from 5.59 to 2.42.", "venue": "Neural Information
      Processing Systems", "year": 2020, "referenceCount": 56, "citationCount": 1258,
      "influentialCitationCount": 329, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2020-06-01", "journal": {"volume": "abs/2006.06676",
      "name": "ArXiv"}, "authors": [{"authorId": "2976930", "name": "Tero Karras"},
      {"authorId": "1907688", "name": "M. Aittala"}, {"authorId": "1454226629", "name":
      "Janne Hellsten"}, {"authorId": "36436218", "name": "S. Laine"}, {"authorId":
      "49244945", "name": "J. Lehtinen"}, {"authorId": "1761103", "name": "Timo Aila"}]}},
      {"intents": ["methodology"], "isInfluential": false, "contexts": ["One may also
      use StyleGAN for image editing by calculating the latent vector of a given input
      image in the styleGAN (this operation is known as styleGAN inversion) and then
      manipulating this vector for editing the image [75, 60, 1, 68, 2, 65, 64, 54]."],
      "citedPaper": {"paperId": "dc0668754e95da573cd64dbe5e2fed07ac9ddf97", "externalIds":
      {"MAG": "3026885507", "ArXiv": "2005.09635", "DBLP": "journals/corr/abs-2005-09635",
      "DOI": "10.1109/tpami.2020.3034267", "CorpusId": 218719151, "PubMed": "33108282"},
      "corpusId": 218719151, "publicationVenue": {"id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
      "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "type":
      "journal", "alternate_names": ["IEEE Trans Pattern Anal Mach Intell"], "issn":
      "0162-8828", "url": "http://www.computer.org/tpami/", "alternate_urls": ["http://www.computer.org/portal/web/tpami",
      "http://ieeexplore.ieee.org/servlet/opac?punumber=34"]}, "url": "https://www.semanticscholar.org/paper/dc0668754e95da573cd64dbe5e2fed07ac9ddf97",
      "title": "InterFaceGAN: Interpreting the Disentangled Face Representation Learned
      by GANs", "abstract": "Although generative adversarial networks (GANs) have
      made significant progress in face synthesis, there lacks enough understanding
      of what GANs have learned in the latent representation to map a random code
      to a photo-realistic image. In this work, we propose a framework called InterFaceGAN
      to interpret the disentangled face representation learned by the state-of-the-art
      GAN models and study the properties of the facial semantics encoded in the latent
      space. We first find that GANs learn various semantics in some linear subspaces
      of the latent space. After identifying these subspaces, we can realistically
      manipulate the corresponding facial attributes without retraining the model.
      We then conduct a detailed study on the correlation between different semantics
      and manage to better disentangle them via subspace projection, resulting in
      more precise control of the attribute manipulation. Besides manipulating the
      gender, age, expression, and presence of eyeglasses, we can even alter the face
      pose and fix the artifacts accidentally made by GANs. Furthermore, we perform
      an in-depth face identity analysis and a layer-wise analysis to evaluate the
      editing results quantitatively. Finally, we apply our approach to real face
      editing by employing GAN inversion approaches and explicitly training feed-forward
      models based on the synthetic data established by InterFaceGAN. Extensive experimental
      results suggest that learning to synthesize faces spontaneously brings a disentangled
      and controllable face representation.", "venue": "IEEE Transactions on Pattern
      Analysis and Machine Intelligence", "year": 2020, "referenceCount": 71, "citationCount":
      415, "influentialCitationCount": 73, "isOpenAccess": true, "openAccessPdf":
      {"url": "https://arxiv.org/pdf/2005.09635", "status": null}, "fieldsOfStudy":
      ["Computer Science", "Engineering", "Medicine"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Engineering", "source":
      "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2020-05-18", "journal": {"volume": "44", "pages": "2004-2018",
      "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence"}, "authors":
      [{"authorId": "2117687899", "name": "Yujun Shen"}, {"authorId": "49984891",
      "name": "Ceyuan Yang"}, {"authorId": "50295995", "name": "Xiaoou Tang"}, {"authorId":
      "145291669", "name": "Bolei Zhou"}]}}, {"intents": [], "isInfluential": false,
      "contexts": [], "citedPaper": {"paperId": "0c604c8a025ccc29f1a8cadbd58e3dacc041ed1c",
      "externalIds": {"DBLP": "conf/aistats/PriolBBL21", "ArXiv": "2005.09136", "MAG":
      "3026172438", "CorpusId": 218684433}, "corpusId": 218684433, "publicationVenue":
      {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e", "name": "International Conference
      on Artificial Intelligence and Statistics", "type": "conference", "alternate_names":
      ["AISTATS", "Int Conf Artif Intell Stat"]}, "url": "https://www.semanticscholar.org/paper/0c604c8a025ccc29f1a8cadbd58e3dacc041ed1c",
      "title": "An Analysis of the Adaptation Speed of Causal Models", "abstract":
      "We consider the problem of discovering the causal process that generated a
      collection of datasets. We assume that all these datasets were generated by
      unknown sparse interventions on a structural causal model (SCM) $G$, that we
      want to identify. Recently, Bengio et al. (2020) argued that among all SCMs,
      $G$ is the fastest to adapt from one dataset to another, and proposed a meta-learning
      criterion to identify the causal direction in a two-variable SCM. While the
      experiments were promising, the theoretical justification was incomplete. Our
      contribution is a theoretical investigation of the adaptation speed of simple
      two-variable SCMs. We use convergence rates from stochastic optimization to
      justify that a relevant proxy for adaptation speed is distance in parameter
      space after intervention. Using this proxy, we show that the SCM with the correct
      causal direction is advantaged for categorical and normal cause-effect datasets
      when the intervention is on the cause variable. When the intervention is on
      the effect variable, we provide a more nuanced picture which highlights that
      the fastest-to-adapt heuristic is not always valid. Code to reproduce experiments
      is available at this https URL", "venue": "International Conference on Artificial
      Intelligence and Statistics", "year": 2020, "referenceCount": 40, "citationCount":
      12, "influentialCitationCount": 1, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2020-05-18", "journal": {"pages": "775-783"},
      "authors": [{"authorId": "10712297", "name": "R\u00e9mi Le Priol"}, {"authorId":
      "101340781", "name": "Reza Babanezhad Harikandeh"}, {"authorId": "1751762",
      "name": "Yoshua Bengio"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}},
      {"intents": [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId":
      "bc2fc5f394e709c78226ebac91f6e956781d9ef9", "externalIds": {"DBLP": "conf/aistats/LoizouVLL21",
      "ArXiv": "2002.10542", "MAG": "3007118755", "CorpusId": 211296607}, "corpusId":
      211296607, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
      "name": "International Conference on Artificial Intelligence and Statistics",
      "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
      Stat"]}, "url": "https://www.semanticscholar.org/paper/bc2fc5f394e709c78226ebac91f6e956781d9ef9",
      "title": "Stochastic Polyak Step-size for SGD: An Adaptive Learning Rate for
      Fast Convergence", "abstract": "We propose a stochastic variant of the classical
      Polyak step-size (Polyak, 1987) commonly used in the subgradient method. Although
      computing the Polyak step-size requires knowledge of the optimal function values,
      this information is readily available for typical modern machine learning applications.
      Consequently, the proposed stochastic Polyak step-size (SPS) is an attractive
      choice for setting the learning rate for stochastic gradient descent (SGD).
      We provide theoretical convergence guarantees for SGD equipped with SPS in different
      settings, including strongly convex, convex and non-convex functions. Furthermore,
      our analysis results in novel convergence guarantees for SGD with a constant
      step-size. We show that SPS is particularly effective when training over-parameterized
      models capable of interpolating the training data. In this setting, we prove
      that SPS enables SGD to converge to the true solution at a fast rate without
      requiring the knowledge of any problem-dependent constants or additional computational
      overhead. We experimentally validate our theoretical results via extensive experiments
      on synthetic and real datasets. We demonstrate the strong performance of SGD
      with SPS compared to state-of-the-art optimization methods when training over-parameterized
      models.", "venue": "International Conference on Artificial Intelligence and
      Statistics", "year": 2020, "referenceCount": 71, "citationCount": 113, "influentialCitationCount":
      21, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2020-02-24", "journal": {"pages": "1306-1314"}, "authors":
      [{"authorId": "1941365", "name": "Nicolas Loizou"}, {"authorId": "1711940",
      "name": "Sharan Vaswani"}, {"authorId": "3266173", "name": "I. Laradji"}, {"authorId":
      "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"intents": [], "isInfluential":
      false, "contexts": [], "citedPaper": {"paperId": "f8865cd38bb8a35f9c9cd965753518f609b2a8ff",
      "externalIds": {"DBLP": "journals/ejtl/YaakoubiSL20", "MAG": "3103502522", "ArXiv":
      "2010.00134", "DOI": "10.1016/j.ejtl.2020.100020", "CorpusId": 214631090}, "corpusId":
      214631090, "publicationVenue": {"id": "05d542af-bda7-4c57-ab4c-66a64dae8618",
      "name": "EURO Journal on Transportation and Logistics", "type": "journal", "alternate_names":
      ["EURO J Transp Logist"], "issn": "2192-4376", "url": "https://www.springer.com/business+&+management/operations+research/journal/13676",
      "alternate_urls": ["https://link.springer.com/journal/13676"]}, "url": "https://www.semanticscholar.org/paper/f8865cd38bb8a35f9c9cd965753518f609b2a8ff",
      "title": "Machine Learning in Airline Crew Pairing to Construct Initial Clusters
      for Dynamic Constraint Aggregation", "abstract": null, "venue": "EURO Journal
      on Transportation and Logistics", "year": 2020, "referenceCount": 57, "citationCount":
      16, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2020-02-01", "journal": {"volume": "abs/2010.00134",
      "name": "ArXiv"}, "authors": [{"authorId": "1781104", "name": "F. Soumis"},
      {"authorId": null, "name": "Yassine Yaakoubi"}, {"authorId": "1388317459", "name":
      "Simon Lacoste-Julien"}]}}, {"intents": [], "isInfluential": false, "contexts":
      [], "citedPaper": {"paperId": "1d80e1f63b9dd30cb59b44acfdc13ab368dcc53e", "externalIds":
      {"MAG": "2997244791", "ArXiv": "2001.00602", "DBLP": "journals/corr/abs-2001-00602",
      "CorpusId": 209832324}, "corpusId": 209832324, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
      "name": "International Conference on Artificial Intelligence and Statistics",
      "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
      Stat"]}, "url": "https://www.semanticscholar.org/paper/1d80e1f63b9dd30cb59b44acfdc13ab368dcc53e",
      "title": "Accelerating Smooth Games by Manipulating Spectral Shapes", "abstract":
      "We use matrix iteration theory to characterize acceleration in smooth games.
      We define the spectral shape of a family of games as the set containing all
      eigenvalues of the Jacobians of standard gradient dynamics in the family. Shapes
      restricted to the real line represent well-understood classes of problems, like
      minimization. Shapes spanning the complex plane capture the added numerical
      challenges in solving smooth games. In this framework, we describe gradient-based
      methods, such as extragradient, as transformations on the spectral shape. Using
      this perspective, we propose an optimal algorithm for bilinear games. For smooth
      and strongly monotone operators, we identify a continuum between convex minimization,
      where acceleration is possible using Polyak''s momentum, and the worst case
      where gradient descent is optimal. Finally, going beyond first-order methods,
      we propose an accelerated version of consensus optimization.", "venue": "International
      Conference on Artificial Intelligence and Statistics", "year": 2020, "referenceCount":
      61, "citationCount": 43, "influentialCitationCount": 11, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2020-01-02", "journal": {"pages": "1705-1715"}, "authors": [{"authorId": "1469119618",
      "name": "Wa\u00efss Azizian"}, {"authorId": "7665349", "name": "Damien Scieur"},
      {"authorId": "3168518", "name": "Ioannis Mitliagkas"}, {"authorId": "1388317459",
      "name": "Simon Lacoste-Julien"}, {"authorId": "8150760", "name": "Gauthier Gidel"}]}},
      {"intents": ["background"], "isInfluential": false, "contexts": ["It is worth
      mentioning also normalizing \ufb02ows [51] and score based generative models
      [67, 66, 48], which become very popular recently and show competitive performance
      with GANs."], "citedPaper": {"paperId": "501c02c7caa7fc2c7077405299b4cbe7d294b170",
      "externalIds": {"DBLP": "journals/jmlr/PapamakariosNRM21", "ArXiv": "1912.02762",
      "MAG": "2992035660", "CorpusId": 208637478}, "corpusId": 208637478, "publicationVenue":
      {"id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780", "name": "Journal of machine learning
      research", "type": "journal", "alternate_names": ["Journal of Machine Learning
      Research", "J mach learn res", "J Mach Learn Res"], "issn": "1532-4435", "alternate_issns":
      ["1533-7928"], "url": "http://www.ai.mit.edu/projects/jmlr/", "alternate_urls":
      ["http://jmlr.csail.mit.edu/", "http://www.jmlr.org/", "http://portal.acm.org/affiliated/jmlr"]},
      "url": "https://www.semanticscholar.org/paper/501c02c7caa7fc2c7077405299b4cbe7d294b170",
      "title": "Normalizing Flows for Probabilistic Modeling and Inference", "abstract":
      "Normalizing flows provide a general mechanism for defining expressive probability
      distributions, only requiring the specification of a (usually simple) base distribution
      and a series of bijective transformations. There has been much recent work on
      normalizing flows, ranging from improving their expressive power to expanding
      their application. We believe the field has now matured and is in need of a
      unified perspective. In this review, we attempt to provide such a perspective
      by describing flows through the lens of probabilistic modeling and inference.
      We place special emphasis on the fundamental principles of flow design, and
      discuss foundational topics such as expressive power and computational trade-offs.
      We also broaden the conceptual framing of flows by relating them to more general
      probability transformations. Lastly, we summarize the use of flows for tasks
      such as generative modeling, approximate inference, and supervised learning.",
      "venue": "Journal of machine learning research", "year": 2019, "referenceCount":
      150, "citationCount": 1039, "influentialCitationCount": 92, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2019-12-05",
      "journal": {"volume": "22", "pages": "57:1-57:64", "name": "J. Mach. Learn.
      Res."}, "authors": [{"authorId": "3065681", "name": "G. Papamakarios"}, {"authorId":
      "2478908", "name": "Eric T. Nalisnick"}, {"authorId": "1748523", "name": "Danilo
      Jimenez Rezende"}, {"authorId": "14594344", "name": "S. Mohamed"}, {"authorId":
      "40627523", "name": "Balaji Lakshminarayanan"}]}}, {"intents": ["background"],
      "isInfluential": false, "contexts": ["Recently a more advanced styleGAN architecture
      has been proposed [32, 18, 33].", "Many state-of-the-art GAN architectures utilize
      this type of progressive training scheme, and it has resulted in very credible
      images [29, 30, 8, 32, 60], and more stable learning for both \ud835\udc37 and
      \ud835\udc3a ."], "citedPaper": {"paperId": "14fdc18d9c164e5b0d6d946b3238c04e81921358",
      "externalIds": {"MAG": "3035574324", "DBLP": "conf/cvpr/KarrasLAHLA20", "ArXiv":
      "1912.04958", "DOI": "10.1109/cvpr42600.2020.00813", "CorpusId": 209202273},
      "corpusId": 209202273, "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
      "name": "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
      ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
      "url": "https://www.semanticscholar.org/paper/14fdc18d9c164e5b0d6d946b3238c04e81921358",
      "title": "Analyzing and Improving the Image Quality of StyleGAN", "abstract":
      "The style-based GAN architecture (StyleGAN) yields state-of-the-art results
      in data-driven unconditional generative image modeling. We expose and analyze
      several of its characteristic artifacts, and propose changes in both model architecture
      and training methods to address them. In particular, we redesign the generator
      normalization, revisit progressive growing, and regularize the generator to
      encourage good conditioning in the mapping from latent codes to images. In addition
      to improving image quality, this path length regularizer yields the additional
      benefit that the generator becomes significantly easier to invert. This makes
      it possible to reliably attribute a generated image to a particular network.
      We furthermore visualize how well the generator utilizes its output resolution,
      and identify a capacity problem, motivating us to train larger models for additional
      quality improvements. Overall, our improved model redefines the state of the
      art in unconditional image modeling, both in terms of existing distribution
      quality metrics as well as perceived image quality.", "venue": "Computer Vision
      and Pattern Recognition", "year": 2019, "referenceCount": 53, "citationCount":
      3879, "influentialCitationCount": 1074, "isOpenAccess": true, "openAccessPdf":
      {"url": "https://arxiv.org/pdf/1912.04958", "status": null}, "fieldsOfStudy":
      ["Computer Science", "Engineering", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Engineering", "source":
      "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2019-12-03", "journal": {"pages": "8107-8116",
      "name": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition
      (CVPR)"}, "authors": [{"authorId": "2976930", "name": "Tero Karras"}, {"authorId":
      "36436218", "name": "S. Laine"}, {"authorId": "1907688", "name": "M. Aittala"},
      {"authorId": "1454226629", "name": "Janne Hellsten"}, {"authorId": "49244945",
      "name": "J. Lehtinen"}, {"authorId": "1761103", "name": "Timo Aila"}]}}, {"intents":
      ["methodology"], "isInfluential": false, "contexts": ["One may also use StyleGAN
      for image editing by calculating the latent vector of a given input image in
      the styleGAN (this operation is known as styleGAN inversion) and then manipulating
      this vector for editing the image [75, 60, 1, 68, 2, 65, 64, 54]."], "citedPaper":
      {"paperId": "56e3b48e72f9452cb862de1b76c51ade2b681c43", "externalIds": {"DBLP":
      "journals/corr/abs-1911-11544", "MAG": "2989788785", "ArXiv": "1911.11544",
      "DOI": "10.1109/cvpr42600.2020.00832", "CorpusId": 208290975}, "corpusId": 208290975,
      "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf", "name": "Computer
      Vision and Pattern Recognition", "type": "conference", "alternate_names": ["CVPR",
      "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
      "url": "https://www.semanticscholar.org/paper/56e3b48e72f9452cb862de1b76c51ade2b681c43",
      "title": "Image2StyleGAN++: How to Edit the Embedded Images?", "abstract": "We
      propose Image2StyleGAN++, a flexible image editing framework with many applications.
      Our framework extends the recent Image2StyleGAN in three ways. First, we introduce
      noise optimization as a complement to the W+ latent space embedding. Our noise
      optimization can restore high frequency features in images and thus significantly
      improves the quality of reconstructed images, e.g. a big increase of PSNR from
      20 dB to 45 dB. Second, we extend the global W+ latent space embedding to enable
      local embeddings. Third, we combine embedding with activation tensor manipulation
      to perform high quality local edits along with global semantic edits on images.
      Such edits motivate various high quality image editing applications, e.g. image
      reconstruction, image inpainting, image crossover, local style transfer, image
      editing using scribbles, and attribute level feature transfer. Examples of the
      edited images are shown across the paper for visual inspection.", "venue": "Computer
      Vision and Pattern Recognition", "year": 2019, "referenceCount": 42, "citationCount":
      434, "influentialCitationCount": 43, "isOpenAccess": true, "openAccessPdf":
      {"url": "https://arxiv.org/pdf/1911.11544", "status": null}, "fieldsOfStudy":
      ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2019-11-26", "journal":
      {"pages": "8293-8302", "name": "2020 IEEE/CVF Conference on Computer Vision
      and Pattern Recognition (CVPR)"}, "authors": [{"authorId": "94395014", "name":
      "Rameen Abdal"}, {"authorId": "2408885", "name": "Yipeng Qin"}, {"authorId":
      "1798011", "name": "Peter Wonka"}]}}, {"intents": [], "isInfluential": false,
      "contexts": [], "citedPaper": {"paperId": "11396aae4adff67fcb073627f60d0f6049471d7c",
      "externalIds": {"MAG": "2980003999", "DBLP": "conf/aistats/MengVL0L20", "ArXiv":
      "1910.04920", "CorpusId": 204402754}, "corpusId": 204402754, "publicationVenue":
      {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e", "name": "International Conference
      on Artificial Intelligence and Statistics", "type": "conference", "alternate_names":
      ["AISTATS", "Int Conf Artif Intell Stat"]}, "url": "https://www.semanticscholar.org/paper/11396aae4adff67fcb073627f60d0f6049471d7c",
      "title": "Fast and Furious Convergence: Stochastic Second Order Methods under
      Interpolation", "abstract": "We consider stochastic second-order methods for
      minimizing smooth and strongly-convex functions under an interpolation condition
      satisfied by over-parameterized models. Under this condition, we show that the
      regularized subsampled Newton method (R-SSN) achieves global linear convergence
      with an adaptive step-size and a constant batch-size. By growing the batch size
      for both the subsampled gradient and Hessian, we show that R-SSN can converge
      at a quadratic rate in a local neighbourhood of the solution. We also show that
      R-SSN attains local linear convergence for the family of self-concordant functions.
      Furthermore, we analyze stochastic BFGS algorithms in the interpolation setting
      and prove their global linear convergence. We empirically evaluate stochastic
      L-BFGS and a \"Hessian-free\" implementation of R-SSN for binary classification
      on synthetic, linearly-separable datasets and real datasets under a kernel mapping.
      Our experimental results demonstrate the fast convergence of these methods,
      both in terms of the number of iterations and wall-clock time.", "venue": "International
      Conference on Artificial Intelligence and Statistics", "year": 2019, "referenceCount":
      83, "citationCount": 28, "influentialCitationCount": 1, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2019-10-11", "journal":
      {"volume": "abs/1910.04920", "name": "ArXiv"}, "authors": [{"authorId": "118126099",
      "name": "S. Meng"}, {"authorId": "1711940", "name": "Sharan Vaswani"}, {"authorId":
      "3266173", "name": "I. Laradji"}, {"authorId": "145610994", "name": "Mark W.
      Schmidt"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"intents":
      ["methodology"], "isInfluential": false, "contexts": ["For example, Delaney
      et al. employed GANs to generate synthetic ECG signals that resemble real ECG
      data [12]."], "citedPaper": {"paperId": "b37f3acec92b1d8fa221160817d71e908f1159ed",
      "externalIds": {"DBLP": "journals/corr/abs-1909-09150", "MAG": "2973342229",
      "ArXiv": "1909.09150", "CorpusId": 202712887}, "corpusId": 202712887, "publicationVenue":
      {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
      ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/b37f3acec92b1d8fa221160817d71e908f1159ed",
      "title": "Synthesis of Realistic ECG using Generative Adversarial Networks",
      "abstract": "Access to medical data is highly restricted due to its sensitive
      nature, preventing communities from using this data for research or clinical
      training. Common methods of de-identification implemented to enable the sharing
      of data are sometimes inadequate to protect the individuals contained in the
      data. For our research, we investigate the ability of generative adversarial
      networks (GANs) to produce realistic medical time series data which can be used
      without concerns over privacy. The aim is to generate synthetic ECG signals
      representative of normal ECG waveforms. GANs have been used successfully to
      generate good quality synthetic time series and have been shown to prevent re-identification
      of individual records. In this work, a range of GAN architectures are developed
      to generate synthetic sine waves and synthetic ECG. Two evaluation metrics are
      then used to quantitatively assess how suitable the synthetic data is for real
      world applications such as clinical training and data analysis. Finally, we
      discuss the privacy concerns associated with sharing synthetic data produced
      by GANs and test their ability to withstand a simple membership inference attack.
      For the first time we both quantitatively and qualitatively demonstrate that
      GAN architecture can successfully generate time series signals that are not
      only structurally similar to the training sets but also diverse in nature across
      generated samples. We also report on their ability to withstand a simple membership
      inference attack, protecting the privacy of the training set.", "venue": "arXiv.org",
      "year": 2019, "referenceCount": 45, "citationCount": 67, "influentialCitationCount":
      8, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Engineering", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Engineering", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Medicine",
      "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2019-09-19", "journal":
      {"volume": "abs/1909.09150", "name": "ArXiv"}, "authors": [{"authorId": "2076220218",
      "name": "Anne Marie Delaney"}, {"authorId": "52133219", "name": "Eoin Brophy"},
      {"authorId": "145950787", "name": "T. Ward"}]}}, {"intents": [], "isInfluential":
      false, "contexts": [], "citedPaper": {"paperId": "56364a9dc5d51a619ee589172b19dfddaa77ad68",
      "externalIds": {"MAG": "3037501197", "DBLP": "conf/aistats/Gallego-PosadaV20",
      "ArXiv": "1906.08325", "CorpusId": 208138262}, "corpusId": 208138262, "publicationVenue":
      {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e", "name": "International Conference
      on Artificial Intelligence and Statistics", "type": "conference", "alternate_names":
      ["AISTATS", "Int Conf Artif Intell Stat"]}, "url": "https://www.semanticscholar.org/paper/56364a9dc5d51a619ee589172b19dfddaa77ad68",
      "title": "GAIT: A Geometric Approach to Information Theory", "abstract": "We
      advocate the use of a notion of entropy that reflects the relative abundances
      of the symbols in an alphabet, as well as the similarities between them. This
      concept was originally introduced in theoretical ecology to study the diversity
      of ecosystems. Based on this notion of entropy, we introduce geometry-aware
      counterparts for several concepts and theorems in information theory. Notably,
      our proposed divergence exhibits performance on par with state-of-the-art methods
      based on the Wasserstein distance, but enjoys a closed-form expression that
      can be computed efficiently. We demonstrate the versatility of our method via
      experiments on a broad range of domains: training generative models, computing
      image barycenters, approximating empirical measures and counting modes.", "venue":
      "International Conference on Artificial Intelligence and Statistics", "year":
      2019, "referenceCount": 44, "citationCount": 8, "influentialCitationCount":
      1, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Mathematics", "source": "s2-fos-model"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2019-06-19", "journal": {"pages": "2601-2611"}, "authors": [{"authorId": "1410596066",
      "name": "Jose Gallego-Posada"}, {"authorId": "34360821", "name": "A. Vani"},
      {"authorId": "51881243", "name": "Max Schwarzer"}, {"authorId": "1388317459",
      "name": "Simon Lacoste-Julien"}]}}, {"intents": [], "isInfluential": false,
      "contexts": [], "citedPaper": {"paperId": "512ca25646d03f58ddce0231c5e36babdc0816f5",
      "externalIds": {"MAG": "2950041755", "DBLP": "journals/corr/abs-1906-05945",
      "ArXiv": "1906.05945", "CorpusId": 189897674}, "corpusId": 189897674, "publicationVenue":
      {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
      ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/512ca25646d03f58ddce0231c5e36babdc0816f5",
      "title": "A Tight and Unified Analysis of Extragradient for a Whole Spectrum
      of Differentiable Games", "abstract": "We consider differentiable games: multi-objective
      minimization problems, where the goal is to find a Nash equilibrium. The machine
      learning community has recently started using extrapolation-based variants of
      the gradient method. A prime example is the extragradient, which yields linear
      convergence in cases like bilinear games, where the standard gradient method
      fails. The full benefits of extrapolation-based methods are not known: i) there
      is no unified analysis for a large class of games that includes both strongly
      monotone and bilinear games; ii) it is not known whether the rate achieved by
      extragradient can be improved, e.g. by considering multiple extrapolation steps.
      We answer these questions through new analysis of the extragradient''s local
      and global convergence properties. Our analysis covers the whole range of settings
      between purely bilinear and strongly monotone games. It reveals that extragradient
      converges via different mechanisms at these extremes; in between, it exploits
      the most favorable mechanism for the given problem. We then present lower bounds
      on the rate of convergence for a wide class of algorithms with any number of
      extrapolations. Our bounds prove that the extragradient achieves the optimal
      rate in this class, and that our upper bounds are tight. Our precise characterization
      of the extragradient''s convergence behavior in games shows that, unlike in
      convex optimization, the extragradient method may be much faster than the gradient
      method.", "venue": "arXiv.org", "year": 2019, "referenceCount": 49, "citationCount":
      24, "influentialCitationCount": 3, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category":
      "Mathematics", "source": "external"}, {"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2019-06-13", "journal": {"volume": "abs/1906.05945",
      "name": "ArXiv"}, "authors": [{"authorId": "1469119618", "name": "Wa\u00efss
      Azizian"}, {"authorId": "3168518", "name": "Ioannis Mitliagkas"}, {"authorId":
      "1388317459", "name": "Simon Lacoste-Julien"}, {"authorId": "8150760", "name":
      "Gauthier Gidel"}]}}, {"intents": ["background"], "isInfluential": false, "contexts":
      ["It was proved successful in many domains such as computer vision [14, 29,
      25, 38], semantic segmentation [39, 27, 70, 24], time-series synthesis [9, 23],
      image editing [61, 36, 19, 3, 75], natural language processing [15, 28, 22],
      text-to-image generation [59, 58, 54], and many more."], "citedPaper": {"paperId":
      "944d54502895811d3b2c72d1a1d49a395588f67e", "externalIds": {"DBLP": "conf/aaai/HusseinTG20",
      "ArXiv": "1906.05284", "MAG": "2951684903", "DOI": "10.1609/AAAI.V34I04.5708",
      "CorpusId": 189762039}, "corpusId": 189762039, "publicationVenue": {"id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence", "type": "conference",
      "alternate_names": ["National Conference on Artificial Intelligence", "National
      Conf Artif Intell", "AAAI Conf Artif Intell", "AAAI"], "url": "http://www.aaai.org/"},
      "url": "https://www.semanticscholar.org/paper/944d54502895811d3b2c72d1a1d49a395588f67e",
      "title": "Image-Adaptive GAN based Reconstruction", "abstract": "In the recent
      years, there has been a significant improvement in the quality of samples produced
      by (deep) generative models such as variational auto-encoders and generative
      adversarial networks. However, the representation capabilities of these methods
      still do not capture the full distribution for complex classes of images, such
      as human faces. This deficiency has been clearly observed in previous works
      that use pre-trained generative models to solve imaging inverse problems. In
      this paper, we suggest to mitigate the limited representation capabilities of
      generators by making them image-adaptive and enforcing compliance of the restoration
      with the observations via back-projections. We empirically demonstrate the advantages
      of our proposed approach for image super-resolution and compressed sensing.",
      "venue": "AAAI Conference on Artificial Intelligence", "year": 2019, "referenceCount":
      43, "citationCount": 68, "influentialCitationCount": 10, "isOpenAccess": true,
      "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/5708/5564",
      "status": null}, "fieldsOfStudy": ["Engineering", "Computer Science"], "s2FieldsOfStudy":
      [{"category": "Engineering", "source": "external"}, {"category": "Computer Science",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"},
      {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2019-06-12", "journal":
      {"pages": "3121-3129"}, "authors": [{"authorId": "2066689074", "name": "Shady
      Abu Hussein"}, {"authorId": "3166526", "name": "Tom Tirer"}, {"authorId": "2711839",
      "name": "R. Giryes"}]}}, {"intents": [], "isInfluential": false, "contexts":
      [], "citedPaper": {"paperId": "ec09db517aa4399ebbb33fb3acc342178f2aca90", "externalIds":
      {"DBLP": "journals/corr/abs-1906-04848", "MAG": "2995128076", "ArXiv": "1906.04848",
      "CorpusId": 186206649}, "corpusId": 186206649, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/ec09db517aa4399ebbb33fb3acc342178f2aca90",
      "title": "A Closer Look at the Optimization Landscapes of Generative Adversarial
      Networks", "abstract": "Generative adversarial networks have been very successful
      in generative modeling, however they remain relatively challenging to train
      compared to standard deep neural networks. In this paper, we propose new visualization
      techniques for the optimization landscapes of GANs that enable us to study the
      game vector field resulting from the concatenation of the gradient of both players.
      Using these visualization techniques we try to bridge the gap between theory
      and practice by showing empirically that the training of GANs exhibits significant
      rotations around Local Stable Stationary Points (LSSP), similar to the one predicted
      by theory on toy examples. Moreover, we provide empirical evidence that GAN
      training converge to a stable stationary point which is a saddle point for the
      generator loss, not a minimum, while still achieving excellent performance.",
      "venue": "International Conference on Learning Representations", "year": 2019,
      "referenceCount": 42, "citationCount": 57, "influentialCitationCount": 5, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2019-06-11", "journal": {"volume": "abs/1906.04848", "name": "ArXiv"}, "authors":
      [{"authorId": "40201329", "name": "Hugo Berard"}, {"authorId": "8150760", "name":
      "Gauthier Gidel"}, {"authorId": "2634674", "name": "Amjad Almahairi"}, {"authorId":
      "145467703", "name": "Pascal Vincent"}, {"authorId": "1388317459", "name": "Simon
      Lacoste-Julien"}]}}, {"intents": [], "isInfluential": false, "contexts": [],
      "citedPaper": {"paperId": "ae39e54c451897999032cde7d3e1c33139d7fdc3", "externalIds":
      {"MAG": "2948607799", "DBLP": "conf/iclr/LachapelleBDL20", "ArXiv": "1906.02226",
      "CorpusId": 174802415}, "corpusId": 174802415, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/ae39e54c451897999032cde7d3e1c33139d7fdc3",
      "title": "Gradient-Based Neural DAG Learning", "abstract": "We propose a novel
      score-based approach to learning a directed acyclic graph (DAG) from observational
      data. We adapt a recently proposed continuous constrained optimization formulation
      to allow for nonlinear relationships between variables using neural networks.
      This extension allows to model complex interactions while avoiding the combinatorial
      nature of the problem. In addition to comparing our method to existing continuous
      optimization methods, we provide missing empirical comparisons to nonlinear
      greedy search methods. On both synthetic and real-world data sets, this new
      method outperforms current continuous methods on most tasks, while being competitive
      with existing greedy search methods on important metrics for causal inference.",
      "venue": "International Conference on Learning Representations", "year": 2019,
      "referenceCount": 47, "citationCount": 185, "influentialCitationCount": 38,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science",
      "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2019-06-05", "journal": {"volume": "abs/1906.02226", "name": "ArXiv"}, "authors":
      [{"authorId": "134730235", "name": "S\u00e9bastien Lachapelle"}, {"authorId":
      "23138044", "name": "P. Brouillard"}, {"authorId": "7636193", "name": "T. Deleu"},
      {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"intents": [],
      "isInfluential": false, "contexts": [], "citedPaper": {"paperId": "37f675bf4329cd9e0811e232e81eef2b814c703d",
      "externalIds": {"MAG": "2948065500", "DBLP": "journals/corr/abs-1906-01529",
      "ArXiv": "1906.01529", "CorpusId": 174797842}, "corpusId": 174797842, "publicationVenue":
      {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
      ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/37f675bf4329cd9e0811e232e81eef2b814c703d",
      "title": "Generative Adversarial Networks: A Survey and Taxonomy", "abstract":
      "Generative adversarial networks (GANs) have been extensively studied in the
      past few years. Arguably the revolutionary techniques are in the area of computer
      vision such as plausible image generation, image to image translation, facial
      attribute manipulation and similar domains. Despite the significant success
      achieved in the computer vision field, applying GANs to real-world problems
      still poses significant challenges, three of which we focus on here: (1) High
      quality image generation; (2) Diverse image generation; and (3) Stable training.
      Through an in-depth review of GAN-related research in the literature, we provide
      an account of the architecture-variants and loss-variants, which have been proposed
      to handle these three challenges from two perspectives. We propose loss-variants
      and architecture-variants for classifying the most popular GANs, and discuss
      the potential improvements with focusing on these two aspects. While several
      reviews for GANs have been presented to date, none have focused on the review
      of GAN-variants based on their handling the challenges mentioned above. In this
      paper, we review and critically discuss 7 architecture-variant GANs and 9 loss-variant
      GANs for remedying those three challenges. The objective of this review is to
      provide an insight on the footprint that current GANs research focuses on the
      performance improvement. Code related to GAN-variants studied in this work is
      summarized on https://github.com/sheqi/GAN_Review.", "venue": "arXiv.org", "year":
      2019, "referenceCount": 104, "citationCount": 83, "influentialCitationCount":
      8, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Review"], "publicationDate": "2019-06-04", "journal": {"volume":
      "abs/1906.01529", "name": "ArXiv"}, "authors": [{"authorId": "2145911462", "name":
      "Zhengwei Wang"}, {"authorId": "1891221", "name": "Qi She"}, {"authorId": "145950787",
      "name": "T. Ward"}]}}, {"intents": ["methodology"], "isInfluential": true, "contexts":
      ["Figure was taken from [52].", "SphereGAN [52] is a novel integral probability
      metric (IPM)-based GAN, which uses the hypersphere to bound IPMs in the objective
      function, thus enhancing the stability of the training.", "Unlike conventional
      approaches that use the Wasserstein distance and add additional constraint terms
      (see Table 1 in [52]), SphereGAN does not need any additional constraints to
      force \ud835\udc37 in a desired function space, due to the usage of geometric
      transformation in \ud835\udc37 ."], "citedPaper": {"paperId": "6a552ef06b5cec584d7b75f8c17a9420db78b246",
      "externalIds": {"DBLP": "conf/cvpr/ParkK19", "MAG": "2948697666", "DOI": "10.1109/CVPR.2019.00442",
      "CorpusId": 163157169}, "corpusId": 163157169, "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
      "name": "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
      ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
      "url": "https://www.semanticscholar.org/paper/6a552ef06b5cec584d7b75f8c17a9420db78b246",
      "title": "Sphere Generative Adversarial Network Based on Geometric Moment Matching",
      "abstract": "We propose sphere generative adversarial network (GAN), a novel
      integral probability metric (IPM)-based GAN. Sphere GAN uses the hypersphere
      to bound IPMs in the objective function. Thus, it can be trained stably. On
      the hypersphere, sphere GAN exploits the information of higher-order statistics
      of data using geometric moment matching, thereby providing more accurate results.
      In the paper, we mathematically prove the good properties of sphere GAN. In
      experiments, sphere GAN quantitatively and qualitatively surpasses recent state-of-the-art
      GANs for unsupervised image generation problems with the CIFAR-10, STL-10, and
      LSUN bedroom datasets. Source code is available at https://github.com/pswkiki/SphereGAN.",
      "venue": "Computer Vision and Pattern Recognition", "year": 2019, "referenceCount":
      41, "citationCount": 30, "influentialCitationCount": 4, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-06-01",
      "journal": {"pages": "4287-4296", "name": "2019 IEEE/CVF Conference on Computer
      Vision and Pattern Recognition (CVPR)"}, "authors": [{"authorId": "2115276875",
      "name": "Sung Woo Park"}, {"authorId": "2110140", "name": "Junseok Kwon"}]}},
      {"intents": [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId":
      "d32aa34a807560c94d5018e875a021813890447d", "externalIds": {"ArXiv": "1905.09997",
      "MAG": "2970028551", "DBLP": "conf/nips/VaswaniMLSGL19", "CorpusId": 165163603},
      "corpusId": 165163603, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/d32aa34a807560c94d5018e875a021813890447d",
      "title": "Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence
      Rates", "abstract": "Recent works have shown that stochastic gradient descent
      (SGD) achieves the fast convergence rates of full-batch gradient descent for
      over-parameterized models satisfying certain interpolation conditions. However,
      the step-size used in these works depends on unknown quantities and SGD''s practical
      performance heavily relies on the choice of this step-size. We propose to use
      line-search techniques to automatically set the step-size when training models
      that can interpolate the data. In the interpolation setting, we prove that SGD
      with a stochastic variant of the classic Armijo line-search attains the deterministic
      convergence rates for both convex and strongly-convex functions. Under additional
      assumptions, SGD with Armijo line-search is shown to achieve fast convergence
      for non-convex functions. Furthermore, we show that stochastic extra-gradient
      with a Lipschitz line-search attains linear convergence for an important class
      of non-convex functions and saddle-point problems satisfying interpolation.
      To improve the proposed methods'' practical performance, we give heuristics
      to use larger step-sizes and acceleration. We compare the proposed algorithms
      against numerous optimization methods on standard classification tasks using
      both kernel methods and deep networks. The proposed methods result in competitive
      performance across all models and datasets, while being robust to the precise
      choices of hyper-parameters. For multi-class classification using deep networks,
      SGD with Armijo line-search results in both faster convergence and better generalization.",
      "venue": "Neural Information Processing Systems", "year": 2019, "referenceCount":
      97, "citationCount": 158, "influentialCitationCount": 24, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2019-05-24", "journal":
      {"pages": "3727-3740"}, "authors": [{"authorId": "1711940", "name": "Sharan
      Vaswani"}, {"authorId": "21529944", "name": "Aaron Mishkin"}, {"authorId": "3266173",
      "name": "I. Laradji"}, {"authorId": "145610994", "name": "Mark W. Schmidt"},
      {"authorId": "8150760", "name": "Gauthier Gidel"}, {"authorId": "1388317459",
      "name": "Simon Lacoste-Julien"}]}}, {"intents": ["background"], "isInfluential":
      false, "contexts": ["More important, these images can be controlled to exhibit
      speci\ufb01c attributes (e.g. level of happiness or facial textures), and thus
      enhancing the stimuli variation in the experiment [73]."], "citedPaper": {"paperId":
      "2d395f7715692aefc8c5d3f264e2fb3bd405585f", "externalIds": {"MAG": "3025202934",
      "ArXiv": "1905.04243", "DBLP": "journals/ijon/WangSSWH20", "DOI": "10.1016/j.neucom.2020.04.069",
      "CorpusId": 211011212}, "corpusId": 211011212, "publicationVenue": {"id": "df12d289-f447-47d3-8846-75e39de3ab57",
      "name": "Neurocomputing", "type": "journal", "issn": "0925-2312", "url": "http://www.elsevier.com/locate/neucom",
      "alternate_urls": ["http://www.sciencedirect.com/science/journal/09252312"]},
      "url": "https://www.semanticscholar.org/paper/2d395f7715692aefc8c5d3f264e2fb3bd405585f",
      "title": "Synthetic-Neuroscore: Using a neuro-AI interface for evaluating generative
      adversarial networks", "abstract": null, "venue": "Neurocomputing", "year":
      2019, "referenceCount": 66, "citationCount": 9, "influentialCitationCount":
      0, "isOpenAccess": true, "openAccessPdf": {"url": "https://doras.dcu.ie/24650/1/Neurocomputing_latest.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Engineering",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2019-05-10", "journal":
      {"volume": "405", "pages": "26-36", "name": "Neurocomputing"}, "authors": [{"authorId":
      "2145911462", "name": "Zhengwei Wang"}, {"authorId": "1486411393", "name": "Qi
      She"}, {"authorId": "1680223", "name": "A. Smeaton"}, {"authorId": "145950787",
      "name": "T. Ward"}, {"authorId": "30978009", "name": "G. Healy"}]}}, {"intents":
      ["background"], "isInfluential": false, "contexts": ["It was proved successful
      in many domains such as computer vision [14, 29, 25, 38], semantic segmentation
      [39, 27, 70, 24], time-series synthesis [9, 23], image editing [61, 36, 19,
      3, 75], natural language processing [15, 28, 22], text-to-image generation [59,
      58, 54], and many more."], "citedPaper": {"paperId": "ccaf15d4ad006171061508ca0a99c73814671501",
      "externalIds": {"MAG": "2942890911", "DBLP": "conf/iccv/ShahamDM19", "ArXiv":
      "1905.01164", "DOI": "10.1109/ICCV.2019.00467", "CorpusId": 145052179}, "corpusId":
      145052179, "publicationVenue": {"id": "7654260e-79f9-45c5-9663-d72027cf88f3",
      "name": "IEEE International Conference on Computer Vision", "type": "conference",
      "alternate_names": ["ICCV", "IEEE Int Conf Comput Vis", "ICCV Workshops", "ICCV
      Work"], "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"},
      "url": "https://www.semanticscholar.org/paper/ccaf15d4ad006171061508ca0a99c73814671501",
      "title": "SinGAN: Learning a Generative Model From a Single Natural Image",
      "abstract": "We introduce SinGAN, an unconditional generative model that can
      be learned from a single natural image. Our model is trained to capture the
      internal distribution of patches within the image, and is then able to generate
      high quality, diverse samples that carry the same visual content as the image.
      SinGAN contains a pyramid of fully convolutional GANs, each responsible for
      learning the patch distribution at a different scale of the image. This allows
      generating new samples of arbitrary size and aspect ratio, that have significant
      variability, yet maintain both the global structure and the fine textures of
      the training image. In contrast to previous single image GAN schemes, our approach
      is not limited to texture images, and is not conditional (i.e. it generates
      samples from noise). User studies confirm that the generated samples are commonly
      confused to be real images. We illustrate the utility of SinGAN in a wide range
      of image manipulation tasks.", "venue": "IEEE International Conference on Computer
      Vision", "year": 2019, "referenceCount": 70, "citationCount": 658, "influentialCitationCount":
      115, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1905.01164",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2019-05-02", "journal": {"pages": "4569-4579", "name": "2019
      IEEE/CVF International Conference on Computer Vision (ICCV)"}, "authors": [{"authorId":
      "3459255", "name": "Tamar Rott Shaham"}, {"authorId": "2112779", "name": "Tali
      Dekel"}, {"authorId": "1880407", "name": "T. Michaeli"}]}}, {"intents": [],
      "isInfluential": false, "contexts": [], "citedPaper": {"paperId": "c232afff29037ca4058f2b45e987cd12081bae0a",
      "externalIds": {"DBLP": "journals/corr/abs-1904-13262", "ArXiv": "1904.13262",
      "MAG": "2970166047", "CorpusId": 140225885}, "corpusId": 140225885, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/c232afff29037ca4058f2b45e987cd12081bae0a",
      "title": "Implicit Regularization of Discrete Gradient Dynamics in Deep Linear
      Neural Networks", "abstract": "When optimizing over-parameterized models, such
      as deep neural networks, a large set of parameters can achieve zero training
      error. In such cases, the choice of the optimization algorithm and its respective
      hyper-parameters introduces biases that will lead to convergence to specific
      minimizers of the objective. Consequently, this choice can be considered as
      an implicit regularization for the training of over-parametrized models. In
      this work, we push this idea further by studying the discrete gradient dynamics
      of the training of a two-layer linear network with the least-squares loss. Using
      a time rescaling, we show that, with a vanishing initialization and a small
      enough step size, this dynamics sequentially learns the solutions of a reduced-rank
      regression with a gradually increasing rank.", "venue": "Neural Information
      Processing Systems", "year": 2019, "referenceCount": 29, "citationCount": 118,
      "influentialCitationCount": 16, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2019-04-30", "journal": {"pages": "3196-3206"},
      "authors": [{"authorId": "8150760", "name": "Gauthier Gidel"}, {"authorId":
      "144570279", "name": "F. Bach"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}},
      {"intents": [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId":
      "db56581b2f21b78bf063d47a0f708aef915200e6", "externalIds": {"ArXiv": "1904.08598",
      "DBLP": "conf/nips/ChavdarovaGFL19", "MAG": "2939856750", "CorpusId": 120411570},
      "corpusId": 120411570, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/db56581b2f21b78bf063d47a0f708aef915200e6",
      "title": "Reducing Noise in GAN Training with Variance Reduced Extragradient",
      "abstract": "We study the effect of the stochastic gradient noise on the training
      of generative adversarial networks (GANs) and show that it can prevent the convergence
      of standard game optimization methods, while the batch version converges. We
      address this issue with a novel stochastic variance-reduced extragradient (SVRE)
      optimization algorithm, which for a large class of games improves upon the previous
      convergence rates proposed in the literature. We observe empirically that SVRE
      performs similarly to a batch method on MNIST while being computationally cheaper,
      and that SVRE yields more stable GAN training on standard datasets.", "venue":
      "Neural Information Processing Systems", "year": 2019, "referenceCount": 51,
      "citationCount": 116, "influentialCitationCount": 16, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2019-04-18", "journal": {"volume": "abs/1904.08598", "name": "ArXiv"}, "authors":
      [{"authorId": "2458542", "name": "Tatjana Chavdarova"}, {"authorId": "8150760",
      "name": "Gauthier Gidel"}, {"authorId": "2721983", "name": "F. Fleuret"}, {"authorId":
      "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"intents": ["methodology"],
      "isInfluential": false, "contexts": ["One may also use StyleGAN for image editing
      by calculating the latent vector of a given input image in the styleGAN (this
      operation is known as styleGAN inversion) and then manipulating this vector
      for editing the image [75, 60, 1, 68, 2, 65, 64, 54]."], "citedPaper": {"paperId":
      "62931b3e0dce8748364e19c87ef318e22ec59c7f", "externalIds": {"ArXiv": "1904.03189",
      "MAG": "2934375473", "DBLP": "conf/iccv/AbdalQW19", "DOI": "10.1109/ICCV.2019.00453",
      "CorpusId": 102350964}, "corpusId": 102350964, "publicationVenue": {"id": "7654260e-79f9-45c5-9663-d72027cf88f3",
      "name": "IEEE International Conference on Computer Vision", "type": "conference",
      "alternate_names": ["ICCV", "IEEE Int Conf Comput Vis", "ICCV Workshops", "ICCV
      Work"], "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"},
      "url": "https://www.semanticscholar.org/paper/62931b3e0dce8748364e19c87ef318e22ec59c7f",
      "title": "Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?",
      "abstract": "We propose an efficient algorithm to embed a given image into the
      latent space of StyleGAN. This embedding enables semantic image editing operations
      that can be applied to existing photographs. Taking the StyleGAN trained on
      the FFHD dataset as an example, we show results for image morphing, style transfer,
      and expression transfer. Studying the results of the embedding algorithm provides
      valuable insights into the structure of the StyleGAN latent space. We propose
      a set of experiments to test what class of images can be embedded, how they
      are embedded, what latent space is suitable for embedding, and if the embedding
      is semantically meaningful.", "venue": "IEEE International Conference on Computer
      Vision", "year": 2019, "referenceCount": 40, "citationCount": 850, "influentialCitationCount":
      127, "isOpenAccess": true, "openAccessPdf": {"url": "https://orca.cardiff.ac.uk/id/eprint/125332/1/ICCV_StyleGAN_HQ.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2019-04-05", "journal": {"pages": "4431-4440", "name": "2019
      IEEE/CVF International Conference on Computer Vision (ICCV)"}, "authors": [{"authorId":
      "94395014", "name": "Rameen Abdal"}, {"authorId": "2408885", "name": "Yipeng
      Qin"}, {"authorId": "1798011", "name": "Peter Wonka"}]}}, {"intents": ["background"],
      "isInfluential": false, "contexts": ["Di\ufb00erent GAN architectures were proposed
      for di\ufb00erent tasks, such as image super-resolution [38] and image-to-image
      transfer [79, 53]."], "citedPaper": {"paperId": "a1a19aaddf57c0546357d890d9269092ba0afb26",
      "externalIds": {"MAG": "2920879895", "DBLP": "conf/cvpr/Park0WZ19", "ArXiv":
      "1903.07291", "DOI": "10.1109/CVPR.2019.00244", "CorpusId": 81981856}, "corpusId":
      81981856, "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
      "name": "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
      ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
      "url": "https://www.semanticscholar.org/paper/a1a19aaddf57c0546357d890d9269092ba0afb26",
      "title": "Semantic Image Synthesis With Spatially-Adaptive Normalization", "abstract":
      "We propose spatially-adaptive normalization, a simple but effective layer for
      synthesizing photorealistic images given an input semantic layout. Previous
      methods directly feed the semantic layout as input to the network, forcing the
      network to memorize the information throughout all the layers. Instead, we propose
      using the input layout for modulating the activations in normalization layers
      through a spatially-adaptive, learned affine transformation. Experiments on
      several challenging datasets demonstrate the superiority of our method compared
      to existing approaches, regarding both visual fidelity and alignment with input
      layouts. Finally, our model allows users to easily control the style and content
      of image synthesis results as well as create multi-modal results. Code is available
      upon publication.", "venue": "Computer Vision and Pattern Recognition", "year":
      2019, "referenceCount": 65, "citationCount": 2052, "influentialCitationCount":
      466, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1903.07291",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2019-03-18", "journal": {"pages": "2332-2341", "name": "2019
      IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"}, "authors":
      [{"authorId": "2071929129", "name": "Taesung Park"}, {"authorId": "39793900",
      "name": "Ming-Yu Liu"}, {"authorId": "2195314", "name": "Ting-Chun Wang"}, {"authorId":
      "2436356", "name": "Jun-Yan Zhu"}]}}, {"intents": ["background"], "isInfluential":
      false, "contexts": ["It was proved successful in many domains such as computer
      vision [14, 29, 25, 38], semantic segmentation [39, 27, 70, 24], time-series
      synthesis [9, 23], image editing [61, 36, 19, 3, 75], natural language processing
      [15, 28, 22], text-to-image generation [59, 58, 54], and many more."], "citedPaper":
      {"paperId": "8abf64356c65727225ad052b01ef6b2f6b31c502", "externalIds": {"DBLP":
      "journals/corr/abs-1902-05624", "MAG": "2911436395", "ArXiv": "1902.05624",
      "CorpusId": 67751496}, "corpusId": 67751496, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/8abf64356c65727225ad052b01ef6b2f6b31c502",
      "title": "Quick and Easy Time Series Generation with Established Image-based
      GANs", "abstract": "In the recent years Generative Adversarial Networks (GANs)
      have demonstrated significant progress in generating authentic looking data.
      In this work we introduce our simple method to exploit the advancements in well
      established image-based GANs to synthesise single channel time series data.
      We implement Wasserstein GANs (WGANs) with gradient penalty due to their stability
      in training to synthesise three different types of data; sinusoidal data, photoplethysmograph
      (PPG) data and electrocardiograph (ECG) data. The length of the returned time
      series data is limited only by the image resolution, we use an image size of
      64x64 pixels which yields 4096 data points. We present both visual and quantitative
      evidence that our novel method can successfully generate time series data using
      image-based GANs.", "venue": "arXiv.org", "year": 2019, "referenceCount": 6,
      "citationCount": 31, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2019-02-14", "journal":
      {"volume": "abs/1902.05624", "name": "ArXiv"}, "authors": [{"authorId": "52133219",
      "name": "Eoin Brophy"}, {"authorId": "2145911462", "name": "Zhengwei Wang"},
      {"authorId": "145950787", "name": "T. Ward"}]}}, {"intents": ["background"],
      "isInfluential": true, "contexts": ["Figure is taken from [30].", "Many state-of-the-art
      GAN architectures utilize this type of progressive training scheme, and it has
      resulted in very credible images [29, 30, 8, 32, 60], and more stable learning
      for both \ud835\udc37 and \ud835\udc3a .", "Architectures like StyleGAN [30]
      (see Section 4.6) are specialized to generate broad types of face images as
      a stimulus.", "StyleGAN [30] proposed an alternative generator architecture
      for GANs.", "Many types of new GAN architectures have been proposed since 2014
      ([6, 8, 13, 29, 57, 77, 30, 16])."], "citedPaper": {"paperId": "ceb2ebef0b41e31c1a21b28c2734123900c005e2",
      "externalIds": {"DBLP": "journals/corr/abs-1812-04948", "MAG": "2904367110",
      "ArXiv": "1812.04948", "DOI": "10.1109/CVPR.2019.00453", "CorpusId": 54482423},
      "corpusId": 54482423, "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
      "name": "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
      ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
      "url": "https://www.semanticscholar.org/paper/ceb2ebef0b41e31c1a21b28c2734123900c005e2",
      "title": "A Style-Based Generator Architecture for Generative Adversarial Networks",
      "abstract": "We propose an alternative generator architecture for generative
      adversarial networks, borrowing from style transfer literature. The new architecture
      leads to an automatically learned, unsupervised separation of high-level attributes
      (e.g., pose and identity when trained on human faces) and stochastic variation
      in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific
      control of the synthesis. The new generator improves the state-of-the-art in
      terms of traditional distribution quality metrics, leads to demonstrably better
      interpolation properties, and also better disentangles the latent factors of
      variation. To quantify interpolation quality and disentanglement, we propose
      two new, automated methods that are applicable to any generator architecture.
      Finally, we introduce a new, highly varied and high-quality dataset of human
      faces.", "venue": "Computer Vision and Pattern Recognition", "year": 2018, "referenceCount":
      65, "citationCount": 7030, "influentialCitationCount": 1688, "isOpenAccess":
      true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1812.04948", "status":
      null}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-12-12",
      "journal": {"pages": "4396-4405", "name": "2019 IEEE/CVF Conference on Computer
      Vision and Pattern Recognition (CVPR)"}, "authors": [{"authorId": "2976930",
      "name": "Tero Karras"}, {"authorId": "36436218", "name": "S. Laine"}, {"authorId":
      "1761103", "name": "Timo Aila"}]}}, {"intents": ["background"], "isInfluential":
      false, "contexts": ["SSGANs [10] exploit two popular unsupervised learning techniques,
      adversarial training, and self-supervision, bridging the gap between conditional
      and unconditional GANs."], "citedPaper": {"paperId": "0e96b0a586e3acfcf1602c0e246c04c6080e2315",
      "externalIds": {"MAG": "2953327099", "DBLP": "conf/cvpr/ChenZRLH19", "ArXiv":
      "1811.11212", "DOI": "10.1109/CVPR.2019.01243", "CorpusId": 104292237}, "corpusId":
      104292237, "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
      "name": "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
      ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
      "url": "https://www.semanticscholar.org/paper/0e96b0a586e3acfcf1602c0e246c04c6080e2315",
      "title": "Self-Supervised GANs via Auxiliary Rotation Loss", "abstract": "Conditional
      GANs are at the forefront of natural image synthesis. The main drawback of such
      models is the necessity for labeled data. In this work we exploit two popular
      unsupervised learning techniques, adversarial training and self-supervision,
      and take a step towards bridging the gap between conditional and unconditional
      GANs. In particular, we allow the networks to collaborate on the task of representation
      learning, while being adversarial with respect to the classic GAN game. The
      role of self-supervision is to encourage the discriminator to learn meaningful
      feature representations which are not forgotten during training. We test empirically
      both the quality of the learned image representations, and the quality of the
      synthesized images. Under the same conditions, the self-supervised GAN attains
      a similar performance to state-of-the-art conditional counterparts. Finally,
      we show that this approach to fully unsupervised learning can be scaled to attain
      an FID of 23.4 on unconditional ImageNet generation.", "venue": "Computer Vision
      and Pattern Recognition", "year": 2018, "referenceCount": 48, "citationCount":
      268, "influentialCitationCount": 37, "isOpenAccess": true, "openAccessPdf":
      {"url": "https://arxiv.org/pdf/1811.11212", "status": null}, "fieldsOfStudy":
      ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Mathematics", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2018-11-27", "journal":
      {"pages": "12146-12155", "name": "2019 IEEE/CVF Conference on Computer Vision
      and Pattern Recognition (CVPR)"}, "authors": [{"authorId": "2117180415", "name":
      "Ting Chen"}, {"authorId": "2743563", "name": "Xiaohua Zhai"}, {"authorId":
      "39687627", "name": "Marvin Ritter"}, {"authorId": "34302129", "name": "Mario
      Lucic"}, {"authorId": "2815290", "name": "N. Houlsby"}]}}, {"intents": [], "isInfluential":
      false, "contexts": [], "citedPaper": {"paperId": "52f982d781a5bd5675bd9d31d17ce64c5caba9d3",
      "externalIds": {"ArXiv": "1810.11544", "DBLP": "conf/nips/StruminskyLO18", "MAG":
      "2892001400", "CorpusId": 53105897}, "corpusId": 53105897, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/52f982d781a5bd5675bd9d31d17ce64c5caba9d3",
      "title": "Quantifying Learning Guarantees for Convex but Inconsistent Surrogates",
      "abstract": "We study consistency properties of machine learning methods based
      on minimizing convex surrogates. We extend the recent framework of Osokin et
      al. (2017) for the quantitative analysis of consistency properties to the case
      of inconsistent surrogates. Our key technical contribution consists in a new
      lower bound on the calibration function for the quadratic surrogate, which is
      non-trivial (not always zero) for inconsistent cases. The new bound allows to
      quantify the level of inconsistency of the setting and shows how learning with
      inconsistent surrogates can have guarantees on sample complexity and optimization
      difficulty. We apply our theory to two concrete cases: multi-class classification
      with the tree-structured loss and ranking with the mean average precision loss.
      The results show the approximation-computation trade-offs caused by inconsistent
      surrogates and their potential benefits.", "venue": "Neural Information Processing
      Systems", "year": 2018, "referenceCount": 25, "citationCount": 5, "influentialCitationCount":
      0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2018-10-01", "journal": {"pages": "667-675"}, "authors": [{"authorId": "8546356",
      "name": "Kirill Struminsky"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"},
      {"authorId": "145319877", "name": "A. Osokin"}]}}, {"intents": ["background"],
      "isInfluential": false, "contexts": ["BigGAN [8] has achieved state-of-the-art
      generation on the ImageNet datasets.", "Many types of new GAN architectures
      have been proposed since 2014 ([6, 8, 13, 29, 57, 77, 30, 16]).", "Many state-of-the-art
      GAN architectures utilize this type of progressive training scheme, and it has
      resulted in very credible images [29, 30, 8, 32, 60], and more stable learning
      for both \ud835\udc37 and \ud835\udc3a ."], "citedPaper": {"paperId": "22aab110058ebbd198edb1f1e7b4f69fb13c0613",
      "externalIds": {"ArXiv": "1809.11096", "DBLP": "journals/corr/abs-1809-11096",
      "MAG": "2893749619", "CorpusId": 52889459}, "corpusId": 52889459, "publicationVenue":
      {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
      on Learning Representations", "type": "conference", "alternate_names": ["Int
      Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/22aab110058ebbd198edb1f1e7b4f69fb13c0613",
      "title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis",
      "abstract": "Despite recent progress in generative image modeling, successfully
      generating high-resolution, diverse samples from complex datasets such as ImageNet
      remains an elusive goal. To this end, we train Generative Adversarial Networks
      at the largest scale yet attempted, and study the instabilities specific to
      such scale. We find that applying orthogonal regularization to the generator
      renders it amenable to a simple \"truncation trick,\" allowing fine control
      over the trade-off between sample fidelity and variety by reducing the variance
      of the Generator''s input. Our modifications lead to models which set the new
      state of the art in class-conditional image synthesis. When trained on ImageNet
      at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS)
      of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous
      best IS of 52.52 and FID of 18.6.", "venue": "International Conference on Learning
      Representations", "year": 2018, "referenceCount": 59, "citationCount": 4152,
      "influentialCitationCount": 627, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2018-09-27", "journal": {"volume": "abs/1809.11096",
      "name": "ArXiv"}, "authors": [{"authorId": "2065040247", "name": "Andrew Brock"},
      {"authorId": "7408951", "name": "Jeff Donahue"}, {"authorId": "34838386", "name":
      "K. Simonyan"}]}}, {"intents": ["background"], "isInfluential": false, "contexts":
      ["Teaching 2017-2023 (6) University of Montreal Advanced Structured Prediction
      and Optimization (IFT 6132) - grad class 2016-2022 (6) University of Montreal
      Probabilistic Graphical Model (IFT 6269) - grad class 2014, 2015 \u00c9cole
      Normale Sup\u00e9rieure Statistical Machine Learning - 1st year master 2014,
      2015 \u00c9cole Normale Sup\u00e9rieure de Cachan Programming Projects for Machine
      Learning - 2nd year master 2015 \u00c9cole Normale Sup\u00e9rieure de Cachan
      Probabilistic Graphical Models - 2nd year master 2006, 2008 University of California,
      Berkeley Practical Machine Learning - grad class"], "citedPaper": {"paperId":
      "099c704bdbd6344bba4c1aefa4dea17f1d00c61c", "externalIds": {"DBLP": "journals/corr/abs-1809-06367",
      "MAG": "2884013187", "ArXiv": "1809.06367", "DOI": "10.1109/TPAMI.2018.2855738",
      "CorpusId": 206769416, "PubMed": "30028690"}, "corpusId": 206769416, "publicationVenue":
      {"id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b", "name": "IEEE Transactions on
      Pattern Analysis and Machine Intelligence", "type": "journal", "alternate_names":
      ["IEEE Trans Pattern Anal Mach Intell"], "issn": "0162-8828", "url": "http://www.computer.org/tpami/",
      "alternate_urls": ["http://www.computer.org/portal/web/tpami", "http://ieeexplore.ieee.org/servlet/opac?punumber=34"]},
      "url": "https://www.semanticscholar.org/paper/099c704bdbd6344bba4c1aefa4dea17f1d00c61c",
      "title": "Scattering Networks for Hybrid Representation Learning", "abstract":
      "Scattering networks are a class of designed Convolutional Neural Networks (CNNs)
      with fixed weights. We argue they can serve as generic representations for modelling
      images. In particular, by working in scattering space, we achieve competitive
      results both for supervised and unsupervised learning tasks, while making progress
      towards constructing more interpretable CNNs. For supervised learning, we demonstrate
      that the early layers of CNNs do not necessarily need to be learned, and can
      be replaced with a scattering network instead. Indeed, using hybrid architectures,
      we achieve the best results with predefined representations to-date, while being
      competitive with end-to-end learned CNNs. Specifically, even applying a shallow
      cascade of small-windowed scattering coefficients followed by $1\\times 1$1\u00d71-convolutions
      results in AlexNet accuracy on the ILSVRC2012 classification task. Moreover,
      by combining scattering networks with deep residual networks, we achieve a single-crop
      top-5 error of 11.4 percent on ILSVRC2012. Also, we show they can yield excellent
      performance in the small sample regime on CIFAR-10 and STL-10 datasets, exceeding
      their end-to-end counterparts, through their ability to incorporate geometrical
      priors. For unsupervised learning, scattering coefficients can be a competitive
      representation that permits image recovery. We use this fact to train hybrid
      GANs to generate images. Finally, we empirically analyze several properties
      related to stability and reconstruction of images from scattering coefficients.",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year":
      2018, "referenceCount": 62, "citationCount": 67, "influentialCitationCount":
      10, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1809.06367",
      "status": null}, "fieldsOfStudy": ["Computer Science", "Medicine", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Medicine", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2018-09-17", "journal":
      {"volume": "41", "pages": "2208-2221", "name": "IEEE Transactions on Pattern
      Analysis and Machine Intelligence"}, "authors": [{"authorId": "3306593", "name":
      "Edouard Oyallon"}, {"authorId": "2134433", "name": "Sergey Zagoruyko"}, {"authorId":
      "24040986", "name": "Gabriel Huang"}, {"authorId": "2505902", "name": "N. Komodakis"},
      {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}, {"authorId": "1758219",
      "name": "Matthew B. Blaschko"}, {"authorId": "1829344", "name": "Eugene Belilovsky"}]}},
      {"intents": [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId":
      "461afff92a4b51c6b64a05dde407270029fd19ec", "externalIds": {"ArXiv": "1807.11876",
      "DBLP": "journals/informs/LarsenLBFLL22", "MAG": "2914375891", "DOI": "10.1287/ijoc.2021.1091",
      "CorpusId": 59158767}, "corpusId": 59158767, "publicationVenue": {"id": "b76a26bb-18f2-4155-bb35-7e7de85d01bb",
      "name": "INFORMS journal on computing", "type": "journal", "alternate_names":
      ["INFORMS j comput", "Informs J Comput", "Informs Journal on Computing"], "issn":
      "1091-9856", "url": "https://www.informs.org/", "alternate_urls": ["http://joc.pubs.informs.org/BackIssues.html"]},
      "url": "https://www.semanticscholar.org/paper/461afff92a4b51c6b64a05dde407270029fd19ec",
      "title": "Predicting Tactical Solutions to Operational Planning Problems under
      Imperfect Information", "abstract": "This paper offers a methodological contribution
      at the intersection of machine learning and operations research. Namely, we
      propose a methodology to quickly predict expected tactical descriptions of operational
      solutions (TDOSs). The problem we address occurs in the context of two-stage
      stochastic programming, where the second stage is demanding computationally.
      We aim to predict at a high speed the expected TDOS associated with the second-stage
      problem, conditionally on the first-stage variables. This may be used in support
      of the solution to the overall two-stage problem by avoiding the online generation
      of multiple second-stage scenarios and solutions. We formulate the tactical
      prediction problem as a stochastic optimal prediction program, whose solution
      we approximate with supervised machine learning. The training data set consists
      of a large number of deterministic operational problems generated by controlled
      probabilistic sampling. The labels are computed based on solutions to these
      problems (solved independently and offline), employing appropriate aggregation
      and subselection methods to address uncertainty. Results on our motivating application
      on load planning for rail transportation show that deep learning models produce
      accurate predictions in very short computing time (milliseconds or less). The
      predictive accuracy is close to the lower bounds calculated based on sample
      average approximation of the stochastic prediction programs.", "venue": "INFORMS
      journal on computing", "year": 2018, "referenceCount": 49, "citationCount":
      34, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url":
      "https://arxiv.org/pdf/1807.11876", "status": null}, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2018-07-31", "journal": {"volume": "34", "pages": "227-242",
      "name": "INFORMS J. Comput."}, "authors": [{"authorId": "2065712703", "name":
      "Eric Larsen"}, {"authorId": "134730235", "name": "S\u00e9bastien Lachapelle"},
      {"authorId": "1751762", "name": "Yoshua Bengio"}, {"authorId": "35394395", "name":
      "Emma Frejinger"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"},
      {"authorId": "144390922", "name": "Andrea Lodi"}]}}, {"intents": [], "isInfluential":
      false, "contexts": [], "citedPaper": {"paperId": "3f7c5bec7c86712602866d0e07241fe8cd4a64f1",
      "externalIds": {"DBLP": "journals/corr/abs-1807-04740", "MAG": "2962981216",
      "ArXiv": "1807.04740", "CorpusId": 49671556}, "corpusId": 49671556, "publicationVenue":
      {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e", "name": "International Conference
      on Artificial Intelligence and Statistics", "type": "conference", "alternate_names":
      ["AISTATS", "Int Conf Artif Intell Stat"]}, "url": "https://www.semanticscholar.org/paper/3f7c5bec7c86712602866d0e07241fe8cd4a64f1",
      "title": "Negative Momentum for Improved Game Dynamics", "abstract": "Games
      generalize the single-objective optimization paradigm by introducing different
      objective functions for different players. Differentiable games often proceed
      by simultaneous or alternating gradient updates. In machine learning, games
      are gaining new importance through formulations like generative adversarial
      networks (GANs) and actor-critic systems. However, compared to single-objective
      optimization, game dynamics are more complex and less understood. In this paper,
      we analyze gradient-based methods with momentum on simple games. We prove that
      alternating updates are more stable than simultaneous updates. Next, we show
      both theoretically and empirically that alternating gradient updates with a
      negative momentum term achieves convergence in a difficult toy adversarial problem,
      but also on the notoriously difficult to train saturating GANs.", "venue": "International
      Conference on Artificial Intelligence and Statistics", "year": 2018, "referenceCount":
      41, "citationCount": 167, "influentialCitationCount": 28, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
      "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2018-07-12", "journal": {"volume": "abs/1807.04740", "name": "ArXiv"}, "authors":
      [{"authorId": "8150760", "name": "Gauthier Gidel"}, {"authorId": "7872299",
      "name": "Reyhane Askari Hemmat"}, {"authorId": "145507036", "name": "M. Pezeshki"},
      {"authorId": "24040986", "name": "Gabriel Huang"}, {"authorId": "10712297",
      "name": "R\u00e9mi Le Priol"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"},
      {"authorId": "3168518", "name": "Ioannis Mitliagkas"}]}}, {"intents": ["background"],
      "isInfluential": false, "contexts": ["It was proved successful in many domains
      such as computer vision [14, 29, 25, 38], semantic segmentation [39, 27, 70,
      24], time-series synthesis [9, 23], image editing [61, 36, 19, 3, 75], natural
      language processing [15, 28, 22], text-to-image generation [59, 58, 54], and
      many more."], "citedPaper": {"paperId": "27e13389203b2f8f6138afed867965a3a38cbd8e",
      "externalIds": {"DBLP": "journals/corr/abs-1806-01875", "MAG": "2806591294",
      "ArXiv": "1806.01875", "CorpusId": 46945788}, "corpusId": 46945788, "publicationVenue":
      {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
      ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/27e13389203b2f8f6138afed867965a3a38cbd8e",
      "title": "EEG-GAN: Generative adversarial networks for electroencephalograhic
      (EEG) brain signals", "abstract": "Generative adversarial networks (GANs) are
      recently highly successful in generative applications involving images and start
      being applied to time series data. Here we describe EEG-GAN as a framework to
      generate electroencephalographic (EEG) brain signals. We introduce a modification
      to the improved training of Wasserstein GANs to stabilize training and investigate
      a range of architectural choices critical for time series generation (most notably
      up- and down-sampling). For evaluation we consider and compare different metrics
      such as Inception score, Frechet inception distance and sliced Wasserstein distance,
      together showing that our EEG-GAN framework generated naturalistic EEG examples.
      It thus opens up a range of new generative application scenarios in the neuroscientific
      and neurological context, such as data augmentation in brain-computer interfacing
      tasks, EEG super-sampling, or restoration of corrupted data segments. The possibility
      to generate signals of a certain class and/or with specific properties may also
      open a new avenue for research into the underlying structure of brain signals.",
      "venue": "arXiv.org", "year": 2018, "referenceCount": 17, "citationCount": 174,
      "influentialCitationCount": 18, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Engineering", "Biology", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Engineering", "source": "external"}, {"category": "Biology", "source":
      "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2018-06-05", "journal": {"volume": "abs/1806.01875", "name": "ArXiv"}, "authors":
      [{"authorId": "31899329", "name": "K. Hartmann"}, {"authorId": "9948572", "name":
      "R. Schirrmeister"}, {"authorId": "145928182", "name": "T. Ball"}]}}, {"intents":
      ["background"], "isInfluential": false, "contexts": ["FaceID GAN aims at synthesizing
      high quality images with different facial expressions or poses [8]."], "citedPaper":
      {"paperId": "a35483c9becc95faa16bf70a8c6355566a205091", "externalIds": {"MAG":
      "2799209711", "DBLP": "conf/cvpr/Shen0YWT18", "DOI": "10.1109/CVPR.2018.00092",
      "CorpusId": 52243869}, "corpusId": 52243869, "publicationVenue": null, "url":
      "https://www.semanticscholar.org/paper/a35483c9becc95faa16bf70a8c6355566a205091",
      "title": "FaceID-GAN: Learning a Symmetry Three-Player GAN for Identity-Preserving
      Face Synthesis", "abstract": "Face synthesis has achieved advanced development
      by using generative adversarial networks (GANs). Existing methods typically
      formulate GAN as a two-player game, where a discriminator distinguishes face
      images from the real and synthesized domains, while a generator reduces its
      discriminativeness by synthesizing a face of photorealistic quality. Their competition
      converges when the discriminator is unable to differentiate these two domains.
      Unlike two-player GANs, this work generates identity-preserving faces by proposing
      FaceID-GAN, which treats a classifier of face identity as the third player,
      competing with the generator by distinguishing the identities of the real and
      synthesized faces (see Fig.1). A stationary point is reached when the generator
      produces faces that have high quality as well as preserve identity. Instead
      of simply modeling the identity classifier as an additional discriminator, FaceID-GAN
      is formulated by satisfying information symmetry, which ensures that the real
      and synthesized images are projected into the same feature space. In other words,
      the identity classifier is used to extract identity features from both input
      (real) and output (synthesized) face images of the generator, substantially
      alleviating training difficulty of GAN. Extensive experiments show that FaceID-GAN
      is able to generate faces of arbitrary viewpoint while preserve identity, outperforming
      recent advanced approaches.", "venue": "2018 IEEE/CVF Conference on Computer
      Vision and Pattern Recognition", "year": 2018, "referenceCount": 40, "citationCount":
      149, "influentialCitationCount": 13, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2018-06-01", "journal": {"pages": "821-830", "name": "2018
      IEEE/CVF Conference on Computer Vision and Pattern Recognition"}, "authors":
      [{"authorId": "2117687899", "name": "Yujun Shen"}, {"authorId": "47571885",
      "name": "Ping Luo"}, {"authorId": "1721677", "name": "Junjie Yan"}, {"authorId":
      "93768810", "name": "Xiaogang Wang"}, {"authorId": "50295995", "name": "Xiaoou
      Tang"}]}}, {"intents": [], "isInfluential": false, "contexts": [], "citedPaper":
      {"paperId": "88a4b0ec1392f2a7d974ecce41b04aa46d47b4a9", "externalIds": {"ACL":
      "W18-5041", "ArXiv": "1805.11762", "DBLP": "conf/sigdial/LiuL18", "MAG": "2951222010",
      "DOI": "10.18653/v1/W18-5041", "CorpusId": 44117930}, "corpusId": 44117930,
      "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/88a4b0ec1392f2a7d974ecce41b04aa46d47b4a9",
      "title": "Adversarial Learning of Task-Oriented Neural Dialog Models", "abstract":
      "In this work, we propose an adversarial learning method for reward estimation
      in reinforcement learning (RL) based task-oriented dialog models. Most of the
      current RL based task-oriented dialog systems require the access to a reward
      signal from either user feedback or user ratings. Such user ratings, however,
      may not always be consistent or available in practice. Furthermore, online dialog
      policy learning with RL typically requires a large number of queries to users,
      suffering from sample efficiency problem. To address these challenges, we propose
      an adversarial learning method to learn dialog rewards directly from dialog
      samples. Such rewards are further used to optimize the dialog policy with policy
      gradient based RL. In the evaluation in a restaurant search domain, we show
      that the proposed adversarial dialog learning method achieves advanced dialog
      success rate comparing to strong baseline methods. We further discuss the covariate
      shift problem in online adversarial dialog learning and show how we can address
      that with partial access to user feedback.", "venue": "SIGDIAL Conference",
      "year": 2018, "referenceCount": 37, "citationCount": 34, "influentialCitationCount":
      3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/W18-5041.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2018-05-30", "journal": {"pages": "350-359"}, "authors":
      [{"authorId": "143830417", "name": "Bing Liu"}, {"authorId": "1765892", "name":
      "Ian Lane"}]}}, {"intents": ["methodology", "background"], "isInfluential":
      false, "contexts": ["Many types of new GAN architectures have been proposed
      since 2014 ([6, 8, 13, 29, 57, 77, 30, 16]).", "Its architecture is based on
      the Self-attention GAN (SAGAN) [77], which employs a self-attention mechanism
      in both \ud835\udc37 and \ud835\udc3a , to capture a large receptive \ufb01eld
      without sacri\ufb01cing computational e\ufb03ciency for CNNs [69]."], "citedPaper":
      {"paperId": "a8f3dc53e321fbb2565f5925def4365b9f68d1af", "externalIds": {"ArXiv":
      "1805.08318", "MAG": "2950893734", "DBLP": "journals/corr/abs-1805-08318", "CorpusId":
      46898260}, "corpusId": 46898260, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/a8f3dc53e321fbb2565f5925def4365b9f68d1af",
      "title": "Self-Attention Generative Adversarial Networks", "abstract": "In this
      paper, we propose the Self-Attention Generative Adversarial Network (SAGAN)
      which allows attention-driven, long-range dependency modeling for image generation
      tasks. Traditional convolutional GANs generate high-resolution details as a
      function of only spatially local points in lower-resolution feature maps. In
      SAGAN, details can be generated using cues from all feature locations. Moreover,
      the discriminator can check that highly detailed features in distant portions
      of the image are consistent with each other. Furthermore, recent work has shown
      that generator conditioning affects GAN performance. Leveraging this insight,
      we apply spectral normalization to the GAN generator and find that this improves
      training dynamics. The proposed SAGAN achieves the state-of-the-art results,
      boosting the best published Inception score from 36.8 to 52.52 and reducing
      Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset.
      Visualization of the attention layers shows that the generator leverages neighborhoods
      that correspond to object shapes rather than local regions of fixed shape.",
      "venue": "International Conference on Machine Learning", "year": 2018, "referenceCount":
      54, "citationCount": 3021, "influentialCitationCount": 293, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
      "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2018-05-21", "journal": {"volume": "abs/1805.08318", "name":
      "ArXiv"}, "authors": [{"authorId": "48213346", "name": "Han Zhang"}, {"authorId":
      "153440022", "name": "I. Goodfellow"}, {"authorId": "1711560", "name": "Dimitris
      N. Metaxas"}, {"authorId": "2624088", "name": "Augustus Odena"}]}}, {"intents":
      [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId": "46f9a738f1d238dc37813ae02ed3cbfadae30b2e",
      "externalIds": {"MAG": "2797899988", "DBLP": "conf/aistats/GidelPL18", "ArXiv":
      "1804.03176", "CorpusId": 4739494}, "corpusId": 4739494, "publicationVenue":
      {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e", "name": "International Conference
      on Artificial Intelligence and Statistics", "type": "conference", "alternate_names":
      ["AISTATS", "Int Conf Artif Intell Stat"]}, "url": "https://www.semanticscholar.org/paper/46f9a738f1d238dc37813ae02ed3cbfadae30b2e",
      "title": "Frank-Wolfe Splitting via Augmented Lagrangian Method", "abstract":
      "Minimizing a function over an intersection of convex sets is an important task
      in optimization that is often much more challenging than minimizing it over
      each individual constraint set. While traditional methods such as Frank-Wolfe
      (FW) or proximal gradient descent assume access to a linear or quadratic oracle
      on the intersection, splitting techniques take advantage of the structure of
      each sets, and only require access to the oracle on the individual constraints.
      In this work, we develop and analyze the Frank-Wolfe Augmented Lagrangian (FW-AL)
      algorithm, a method for minimizing a smooth function over convex compact sets
      related by a \"linear consistency\" constraint that only requires access to
      a linear minimization oracle over the individual constraints. It is based on
      the Augmented Lagrangian Method (ALM), also known as Method of Multipliers,
      but unlike most existing splitting methods, it only requires access to linear
      (instead of quadratic) minimization oracles. We use recent advances in the analysis
      of Frank-Wolfe and the alternating direction method of multipliers algorithms
      to prove a sublinear convergence rate for FW-AL over general convex compact
      sets and a linear convergence rate for polytopes.", "venue": "International
      Conference on Artificial Intelligence and Statistics", "year": 2018, "referenceCount":
      50, "citationCount": 24, "influentialCitationCount": 8, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
      "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2018-03-31", "journal":
      {"pages": "1456-1465"}, "authors": [{"authorId": "8150760", "name": "Gauthier
      Gidel"}, {"authorId": "2570016", "name": "Fabian Pedregosa"}, {"authorId": "1388317459",
      "name": "Simon Lacoste-Julien"}]}}, {"intents": ["methodology"], "isInfluential":
      false, "contexts": ["Inconceivably, we can even modify the linguistic content
      by using the VoiceGAN model [13]."], "citedPaper": {"paperId": "5d52c1debe9e3282970c2157289dc42c56f60133",
      "externalIds": {"MAG": "2788830260", "DBLP": "journals/corr/abs-1802-06840",
      "ArXiv": "1802.06840", "DOI": "10.1109/ICASSP.2018.8462018", "CorpusId": 3416173},
      "corpusId": 3416173, "publicationVenue": {"id": "0d6f7fba-7092-46b3-8039-93458dba736b",
      "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "type": "conference", "alternate_names": ["Int Conf Acoust Speech Signal Process",
      "IEEE Int Conf Acoust Speech Signal Process", "ICASSP", "International Conference
      on Acoustics, Speech, and Signal Processing"], "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"},
      "url": "https://www.semanticscholar.org/paper/5d52c1debe9e3282970c2157289dc42c56f60133",
      "title": "Voice Impersonation Using Generative Adversarial Networks", "abstract":
      "Voice impersonation is not the same as voice transformation, although the latter
      is an essential element of it. In voice impersonation, the resultant voice must
      convincingly convey the impression of having been naturally produced by the
      target speaker, mimicking not only the pitch and other perceivable signal qualities,
      but also the style of the target speaker. In this paper, we propose a novel
      neural-network based speech quality- and style-mimicry framework for the synthesis
      of impersonated voices. The framework is built upon a fast and accurate generative
      adversarial network model. Given spectrographic representations of source and
      target speakers'' voices, the model learns to mimic the target speaker''s voice
      quality and style, regardless of the linguistic content of either''s voice,
      generating a synthetic spectrogram from which the time-domain signal is reconstructed
      using the Griffin-Lim method. In effect, this model reframes the well-known
      problem of style-transfer for images as the problem of style-transfer for speech
      signals, while intrinsically addressing the problem of durational variability
      of speech sounds. Experiments demonstrate that the model can generate extremely
      convincing samples of impersonated speech. It is even able to impersonate voices
      across different genders effectively. Results are qualitatively evaluated using
      standard procedures for evaluating synthesized voices.", "venue": "IEEE International
      Conference on Acoustics, Speech, and Signal Processing", "year": 2018, "referenceCount":
      20, "citationCount": 84, "influentialCitationCount": 5, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Engineering"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Engineering", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2018-02-19", "journal": {"pages": "2506-2510", "name": "2018
      IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"},
      "authors": [{"authorId": "2145971975", "name": "Yang Gao"}, {"authorId": "153915824",
      "name": "Rita Singh"}, {"authorId": "1681921", "name": "B. Raj"}]}}, {"intents":
      ["background"], "isInfluential": false, "contexts": ["SNGAN [46] proposes to
      add weight normalization to stabilize the training of the discriminator.", "SNGAN
      controls the Lipschitz constant of \ud835\udc37 by literally constraining the
      spectral norm of each layer, normalizing each weight matrix \ud835\udc4a so
      it satis\ufb01es the spectral constraint \ud835\udf0e ( W ) = 1 (i.e., the largest
      singular value of the weight matrix of each layer is 1).", "SNGAN achieves an
      extraordinary advance on ImageNet, and better or equal quality on CIFAR-10 and
      STL-10, compared to the previous training stabilization techniques that include
      weight clipping [4], gradient penalty [74, 43], batch normalization [26], weight
      normalization [63], layer normalization [5], and orthonormal regularization
      [7]."], "citedPaper": {"paperId": "84de7d27e2f6160f634a483e8548c499a2cda7fa",
      "externalIds": {"MAG": "2785678896", "DBLP": "journals/corr/abs-1802-05957",
      "ArXiv": "1802.05957", "CorpusId": 3366315}, "corpusId": 3366315, "publicationVenue":
      {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
      on Learning Representations", "type": "conference", "alternate_names": ["Int
      Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/84de7d27e2f6160f634a483e8548c499a2cda7fa",
      "title": "Spectral Normalization for Generative Adversarial Networks", "abstract":
      "One of the challenges in the study of generative adversarial networks is the
      instability of its training. In this paper, we propose a novel weight normalization
      technique called spectral normalization to stabilize the training of the discriminator.
      Our new normalization technique is computationally light and easy to incorporate
      into existing implementations. We tested the efficacy of spectral normalization
      on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed
      that spectrally normalized GANs (SN-GANs) is capable of generating images of
      better or equal quality relative to the previous training stabilization techniques.",
      "venue": "International Conference on Learning Representations", "year": 2018,
      "referenceCount": 48, "citationCount": 3738, "influentialCitationCount": 591,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science",
      "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2018-02-15", "journal": {"volume": "abs/1802.05957", "name":
      "ArXiv"}, "authors": [{"authorId": "3213400", "name": "Takeru Miyato"}, {"authorId":
      "2056971870", "name": "Toshiki Kataoka"}, {"authorId": "2877296", "name": "Masanori
      Koyama"}, {"authorId": "51462146", "name": "Yuichi Yoshida"}]}}, {"intents":
      ["background"], "isInfluential": false, "contexts": ["It was proved successful
      in many domains such as computer vision [14, 29, 25, 38], semantic segmentation
      [39, 27, 70, 24], time-series synthesis [9, 23], image editing [61, 36, 19,
      3, 75], natural language processing [15, 28, 22], text-to-image generation [59,
      58, 54], and many more."], "citedPaper": {"paperId": "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d",
      "externalIds": {"ArXiv": "1801.07736", "DBLP": "journals/corr/abs-1801-07736",
      "MAG": "2951433039", "CorpusId": 3655946}, "corpusId": 3655946, "publicationVenue":
      {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
      on Learning Representations", "type": "conference", "alternate_names": ["Int
      Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d",
      "title": "MaskGAN: Better Text Generation via Filling in the ______", "abstract":
      "Neural text generation models are often autoregressive language models or seq2seq
      models. These models generate text by sampling words sequentially, with each
      word conditioned on the previous word, and are state-of-the-art for several
      machine translation and summarization benchmarks. These benchmarks are often
      defined by validation perplexity even though this is not a direct measure of
      the quality of the generated text. Additionally, these models are typically
      trained via maxi- mum likelihood and teacher forcing. These methods are well-suited
      to optimizing perplexity but can result in poor sample quality since generating
      text requires conditioning on sequences of words that may have never been observed
      at training time. We propose to improve sample quality using Generative Adversarial
      Networks (GANs), which explicitly train the generator to produce high quality
      samples and have shown a lot of success in image generation. GANs were originally
      designed to output differentiable values, so discrete language generation is
      challenging for them. We claim that validation perplexity alone is not indicative
      of the quality of text generated by a model. We introduce an actor-critic conditional
      GAN that fills in missing text conditioned on the surrounding context. We show
      qualitatively and quantitatively, evidence that this produces more realistic
      conditional and unconditional text samples compared to a maximum likelihood
      trained model.", "venue": "International Conference on Learning Representations",
      "year": 2018, "referenceCount": 38, "citationCount": 435, "influentialCitationCount":
      34, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2018-01-23", "journal": {"volume": "abs/1801.07736", "name":
      "ArXiv"}, "authors": [{"authorId": "26958176", "name": "W. Fedus"}, {"authorId":
      "153440022", "name": "I. Goodfellow"}, {"authorId": "2555924", "name": "Andrew
      M. Dai"}]}}, {"intents": ["result"], "isInfluential": false, "contexts": ["SNGAN
      achieves an extraordinary advance on ImageNet, and better or equal quality on
      CIFAR-10 and STL-10, compared to the previous training stabilization techniques
      that include weight clipping [4], gradient penalty [74, 43], batch normalization
      [26], weight normalization [63], layer normalization [5], and orthonormal regularization
      [7]."], "citedPaper": {"paperId": "6b4ca249b3b28d3fee65f69714440c08d42cee64",
      "externalIds": {"ArXiv": "1801.04406", "DBLP": "conf/icml/MeschederGN18", "MAG":
      "2953030256", "CorpusId": 3345317}, "corpusId": 3345317, "publicationVenue":
      {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
      on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
      Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/6b4ca249b3b28d3fee65f69714440c08d42cee64",
      "title": "Which Training Methods for GANs do actually Converge?", "abstract":
      "Recent work has shown local convergence of GAN training for absolutely continuous
      data and generator distributions. In this paper, we show that the requirement
      of absolute continuity is necessary: we describe a simple yet prototypical counterexample
      showing that in the more realistic case of distributions that are not absolutely
      continuous, unregularized GAN training is not always convergent. Furthermore,
      we discuss regularization strategies that were recently proposed to stabilize
      GAN training. Our analysis shows that GAN training with instance noise or zero-centered
      gradient penalties converges. On the other hand, we show that Wasserstein-GANs
      and WGAN-GP with a finite number of discriminator updates per generator update
      do not always converge to the equilibrium point. We discuss these results, leading
      us to a new explanation for the stability problems of GAN training. Based on
      our analysis, we extend our convergence results to more general GANs and prove
      local convergence for simplified gradient penalties even if the generator and
      data distribution lie on lower dimensional manifolds. We find these penalties
      to work well in practice and use them to learn high-resolution generative image
      models for a variety of datasets with little hyperparameter tuning.", "venue":
      "International Conference on Machine Learning", "year": 2018, "referenceCount":
      45, "citationCount": 1224, "influentialCitationCount": 119, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-01-13",
      "journal": {"pages": "3478-3487"}, "authors": [{"authorId": "8226549", "name":
      "L. Mescheder"}, {"authorId": "47237027", "name": "Andreas Geiger"}, {"authorId":
      "2388416", "name": "Sebastian Nowozin"}]}}, {"intents": [], "isInfluential":
      false, "contexts": [], "citedPaper": {"paperId": "45cd88aa4f6c34bf5c8fba16863ffa35fcf53ba2",
      "externalIds": {"DBLP": "journals/corr/abs-1801-03749", "MAG": "2783257164",
      "ArXiv": "1801.03749", "CorpusId": 22126900}, "corpusId": 22126900, "publicationVenue":
      {"id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780", "name": "Journal of machine learning
      research", "type": "journal", "alternate_names": ["Journal of Machine Learning
      Research", "J mach learn res", "J Mach Learn Res"], "issn": "1532-4435", "alternate_issns":
      ["1533-7928"], "url": "http://www.ai.mit.edu/projects/jmlr/", "alternate_urls":
      ["http://jmlr.csail.mit.edu/", "http://www.jmlr.org/", "http://portal.acm.org/affiliated/jmlr"]},
      "url": "https://www.semanticscholar.org/paper/45cd88aa4f6c34bf5c8fba16863ffa35fcf53ba2",
      "title": "Improved asynchronous parallel optimization analysis for stochastic
      incremental methods", "abstract": "As datasets continue to increase in size
      and multi-core computer architectures are developed, asynchronous parallel optimization
      algorithms become more and more essential to the field of Machine Learning.
      Unfortunately, conducting the theoretical analysis asynchronous methods is difficult,
      notably due to the introduction of delay and inconsistency in inherently sequential
      algorithms. Handling these issues often requires resorting to simplifying but
      unrealistic assumptions. Through a novel perspective, we revisit and clarify
      a subtle but important technical issue present in a large fraction of the recent
      convergence rate proofs for asynchronous parallel optimization algorithms, and
      propose a simplification of the recently introduced \"perturbed iterate\" framework
      that resolves it. We demonstrate the usefulness of our new framework by analyzing
      three distinct asynchronous parallel incremental optimization algorithms: Hogwild
      (asynchronous SGD), KROMAGNON (asynchronous SVRG) and ASAGA, a novel asynchronous
      parallel version of the incremental gradient algorithm SAGA that enjoys fast
      linear convergence rates. We are able to both remove problematic assumptions
      and obtain better theoretical results. Notably, we prove that ASAGA and KROMAGNON
      can obtain a theoretical linear speedup on multi-core systems even without sparsity
      assumptions. We present results of an implementation on a 40-core architecture
      illustrating the practical speedups as well as the hardware overhead. Finally,
      we investigate the overlap constant, an ill-understood but central quantity
      for the theoretical analysis of asynchronous parallel algorithms. We find that
      it encompasses much more complexity than suggested in previous work, and often
      is order-of-magnitude bigger than traditionally thought.", "venue": "Journal
      of machine learning research", "year": 2018, "referenceCount": 32, "citationCount":
      64, "influentialCitationCount": 14, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy":
      [{"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"},
      {"category": "Mathematics", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2018-01-11", "journal": {"volume": "19",
      "pages": "81:1-81:68", "name": "J. Mach. Learn. Res."}, "authors": [{"authorId":
      "37212795", "name": "R\u00e9mi Leblond"}, {"authorId": "2570016", "name": "Fabian
      Pedregosa"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}},
      {"intents": [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId":
      "a62b6352a820418fce3284eef0ad94fe5817b52a", "externalIds": {"MAG": "2963438599",
      "DBLP": "journals/corr/abs-1712-08577", "ArXiv": "1712.08577", "CorpusId": 31093580},
      "corpusId": 31093580, "publicationVenue": {"id": "f9af8000-42f8-410d-a622-e8811e41660a",
      "name": "Conference on Uncertainty in Artificial Intelligence", "type": "conference",
      "alternate_names": ["Uncertainty in Artificial Intelligence", "UAI", "Conf Uncertain
      Artif Intell", "Uncertain Artif Intell"], "url": "http://www.auai.org/"}, "url":
      "https://www.semanticscholar.org/paper/a62b6352a820418fce3284eef0ad94fe5817b52a",
      "title": "Adaptive Stochastic Dual Coordinate Ascent for Conditional Random
      Fields", "abstract": "This work investigates training Conditional Random Fields
      (CRF) by Stochastic Dual Coordinate Ascent (SDCA). SDCA enjoys a linear convergence
      rate and a strong empirical performance for independent classification problems.
      However, it has never been used to train CRF. Yet it benefits from an exact
      line search with a single marginalization oracle call, unlike previous approaches.
      In this paper, we adapt SDCA to train CRF and we enhance it with an adaptive
      non-uniform sampling strategy. Our preliminary experiments suggest that this
      method matches state-of-the-art CRF optimization techniques.", "venue": "Conference
      on Uncertainty in Artificial Intelligence", "year": 2017, "referenceCount":
      26, "citationCount": 5, "influentialCitationCount": 1, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-12-01",
      "journal": {"pages": "815-824"}, "authors": [{"authorId": "10712297", "name":
      "R\u00e9mi Le Priol"}, {"authorId": "49504044", "name": "Alexandre Pich\u00e9"},
      {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"intents": ["background"],
      "isInfluential": false, "contexts": ["It was proved successful in many domains
      such as computer vision [14, 29, 25, 38], semantic segmentation [39, 27, 70,
      24], time-series synthesis [9, 23], image editing [61, 36, 19, 3, 75], natural
      language processing [15, 28, 22], text-to-image generation [59, 58, 54], and
      many more."], "citedPaper": {"paperId": "f0a0c0f0d6a7ff53abea40a8c0c678ed570bf851",
      "externalIds": {"MAG": "2963800363", "DBLP": "conf/cvpr/Wang0ZTKC18", "ArXiv":
      "1711.11585", "DOI": "10.1109/CVPR.2018.00917", "CorpusId": 41805341}, "corpusId":
      41805341, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/f0a0c0f0d6a7ff53abea40a8c0c678ed570bf851",
      "title": "High-Resolution Image Synthesis and Semantic Manipulation with Conditional
      GANs", "abstract": "We present a new method for synthesizing high-resolution
      photo-realistic images from semantic label maps using conditional generative
      adversarial networks (conditional GANs). Conditional GANs have enabled a variety
      of applications, but the results are often limited to low-resolution and still
      far from realistic. In this work, we generate 2048 \u00c3\u2014 1024 visually
      appealing results with a novel adversarial loss, as well as new multi-scale
      generator and discriminator architectures. Furthermore, we extend our framework
      to interactive visual manipulation with two additional features. First, we incorporate
      object instance segmentation information, which enables object manipulations
      such as removing/adding objects and changing the object category. Second, we
      propose a method to generate diverse results given the same input, allowing
      users to edit the object appearance interactively. Human opinion studies demonstrate
      that our method significantly outperforms existing methods, advancing both the
      quality and the resolution of deep image synthesis and editing.", "venue": "2018
      IEEE/CVF Conference on Computer Vision and Pattern Recognition", "year": 2017,
      "referenceCount": 67, "citationCount": 3238, "influentialCitationCount": 497,
      "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1711.11585",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2017-11-30", "journal": {"pages": "8798-8807", "name": "2018
      IEEE/CVF Conference on Computer Vision and Pattern Recognition"}, "authors":
      [{"authorId": "2195314", "name": "Ting-Chun Wang"}, {"authorId": "39793900",
      "name": "Ming-Yu Liu"}, {"authorId": "2436356", "name": "Jun-Yan Zhu"}, {"authorId":
      "29955511", "name": "Andrew Tao"}, {"authorId": "1690538", "name": "J. Kautz"},
      {"authorId": "2301680", "name": "Bryan Catanzaro"}]}}, {"intents": [], "isInfluential":
      false, "contexts": [], "citedPaper": {"paperId": "c88e8d85fd5160b0793598bda037f977366acf7a",
      "externalIds": {"DBLP": "conf/nips/LucicKMGB18", "MAG": "2768599997", "ArXiv":
      "1711.10337", "CorpusId": 4053393}, "corpusId": 4053393, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/c88e8d85fd5160b0793598bda037f977366acf7a",
      "title": "Are GANs Created Equal? A Large-Scale Study", "abstract": "Generative
      adversarial networks (GAN) are a powerful subclass of generative models. Despite
      a very rich research activity leading to numerous interesting GAN algorithms,
      it is still very hard to assess which algorithm(s) perform better than others.
      We conduct a neutral, multi-faceted large-scale empirical study on state-of-the
      art models and evaluation measures. We find that most models can reach similar
      scores with enough hyperparameter optimization and random restarts. This suggests
      that improvements can arise from a higher computational budget and tuning more
      than fundamental algorithmic changes. To overcome some limitations of the current
      metrics, we also propose several data sets on which precision and recall can
      be computed. Our experimental results suggest that future GAN research should
      be based on more systematic and objective evaluation procedures. Finally, we
      did not find evidence that any of the tested algorithms consistently outperforms
      the non-saturating GAN introduced in \\cite{goodfellow2014generative}.", "venue":
      "Neural Information Processing Systems", "year": 2017, "referenceCount": 28,
      "citationCount": 893, "influentialCitationCount": 82, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
      "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2017-11-28", "journal": {"pages": "698-707"}, "authors": [{"authorId": "34302129",
      "name": "Mario Lucic"}, {"authorId": "2006889", "name": "Karol Kurach"}, {"authorId":
      "145605490", "name": "Marcin Michalski"}, {"authorId": "1802148", "name": "S.
      Gelly"}, {"authorId": "1698617", "name": "O. Bousquet"}]}}, {"intents": [],
      "isInfluential": false, "contexts": [], "citedPaper": {"paperId": "907a90967f68da4311802247408e0515e363f930",
      "externalIds": {"MAG": "2962808524", "DBLP": "conf/icml/HoffmanTPZISED18", "ArXiv":
      "1711.03213", "CorpusId": 7646250}, "corpusId": 7646250, "publicationVenue":
      {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
      on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
      Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/907a90967f68da4311802247408e0515e363f930",
      "title": "CyCADA: Cycle-Consistent Adversarial Domain Adaptation", "abstract":
      "Domain adaptation is critical for success in new, unseen environments. Adversarial
      adaptation models applied in feature spaces discover domain invariant representations,
      but are difficult to visualize and sometimes fail to capture pixel-level and
      low-level domain shifts. Recent work has shown that generative adversarial networks
      combined with cycle-consistency constraints are surprisingly effective at mapping
      images between domains, even without the use of aligned image pairs. We propose
      a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation
      model. CyCADA adapts representations at both the pixel-level and feature-level,
      enforces cycle-consistency while leveraging a task loss, and does not require
      aligned pairs. Our model can be applied in a variety of visual recognition and
      prediction settings. We show new state-of-the-art results across multiple adaptation
      tasks, including digit classification and semantic segmentation of road scenes
      demonstrating transfer from synthetic to real world domains.", "venue": "International
      Conference on Machine Learning", "year": 2017, "referenceCount": 52, "citationCount":
      2520, "influentialCitationCount": 269, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2017-11-08", "journal": {"volume": "abs/1711.03213", "name":
      "ArXiv"}, "authors": [{"authorId": "50196944", "name": "Judy Hoffman"}, {"authorId":
      "2368132", "name": "Eric Tzeng"}, {"authorId": "2071929129", "name": "Taesung
      Park"}, {"authorId": "2436356", "name": "Jun-Yan Zhu"}, {"authorId": "2094770",
      "name": "Phillip Isola"}, {"authorId": "2903226", "name": "Kate Saenko"}, {"authorId":
      "1763086", "name": "Alexei A. Efros"}, {"authorId": "1753210", "name": "Trevor
      Darrell"}]}}, {"intents": [], "isInfluential": false, "contexts": [], "citedPaper":
      {"paperId": "e871d4ef27e4341e93a6d2d3785bd8a6de775266", "externalIds": {"MAG":
      "2772481884", "CorpusId": 64691374}, "corpusId": 64691374, "publicationVenue":
      null, "url": "https://www.semanticscholar.org/paper/e871d4ef27e4341e93a6d2d3785bd8a6de775266",
      "title": "Generative Adversarial Networks\u3092\u7528\u3044\u305f\u78ba\u7387\u7684\u8b58\u5225\u30e2\u30c7\u30eb\u304b\u3089\u8a13\u7df4\u30c7\u30fc\u30bf\u751f\u6210\u5206\u5e03\u306e\u63a8\u5b9a",
      "abstract": null, "venue": "", "year": 2017, "referenceCount": 0, "citationCount":
      1, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2017-11-02",
      "journal": {"volume": "117", "pages": "301-308", "name": ""}, "authors": [{"authorId":
      "71773697", "name": "\u8349\u91ce \u5149\u4eae"}, {"authorId": "1520866776",
      "name": "\u6df3\u5b50 \u4f50\u4e45\u9593"}]}}, {"intents": ["methodology", "background"],
      "isInfluential": true, "contexts": ["They used the same D architecture as in
      [29].", "Many state-of-the-art GAN architectures utilize this type of progressive
      training scheme, and it has resulted in very credible images [29, 30, 8, 32,
      60], and more stable learning for both D and G.", "Many types of new GAN architectures
      have been proposed since 2014 ([6, 8, 13, 29, 57, 77, 30, 16]).", "It was proved
      successful in many domains such as computer vision [14, 29, 25, 38], semantic
      segmentation [39, 27, 70, 24], time-series synthesis [9, 23], image editing
      [61, 36, 19, 3, 75], natural language processing [15, 28, 22], text-to-image
      generation [59, 58, 54], and many more.", "PROGAN described a novel training
      methodology for GANs, involving progressive steps toward the development of
      the entire network architecture [29]."], "citedPaper": {"paperId": "744fe47157477235032f7bb3777800f9f2f45e52",
      "externalIds": {"MAG": "2766527293", "DBLP": "conf/iclr/KarrasALL18", "ArXiv":
      "1710.10196", "CorpusId": 3568073}, "corpusId": 3568073, "publicationVenue":
      {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
      on Learning Representations", "type": "conference", "alternate_names": ["Int
      Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/744fe47157477235032f7bb3777800f9f2f45e52",
      "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
      "abstract": "We describe a new training methodology for generative adversarial
      networks. The key idea is to grow both the generator and discriminator progressively:
      starting from a low resolution, we add new layers that model increasingly fine
      details as training progresses. This both speeds the training up and greatly
      stabilizes it, allowing us to produce images of unprecedented quality, e.g.,
      CelebA images at 1024^2. We also propose a simple way to increase the variation
      in generated images, and achieve a record inception score of 8.80 in unsupervised
      CIFAR10. Additionally, we describe several implementation details that are important
      for discouraging unhealthy competition between the generator and discriminator.
      Finally, we suggest a new metric for evaluating GAN results, both in terms of
      image quality and variation. As an additional contribution, we construct a higher-quality
      version of the CelebA dataset.", "venue": "International Conference on Learning
      Representations", "year": 2017, "referenceCount": 64, "citationCount": 5777,
      "influentialCitationCount": 1098, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2017-10-27", "journal": {"volume": "abs/1710.10196",
      "name": "ArXiv"}, "authors": [{"authorId": "2976930", "name": "Tero Karras"},
      {"authorId": "1761103", "name": "Timo Aila"}, {"authorId": "36436218", "name":
      "S. Laine"}, {"authorId": "49244945", "name": "J. Lehtinen"}]}}, {"intents":
      ["background"], "isInfluential": false, "contexts": ["It was proved successful
      in many domains such as computer vision [14, 29, 25, 38], semantic segmentation
      [39, 27, 70, 24], time-series synthesis [9, 23], image editing [61, 36, 19,
      3, 75], natural language processing [15, 28, 22], text-to-image generation [59,
      58, 54], and many more."], "citedPaper": {"paperId": "485552d2711868b54d5fcddc92c746b09afeab07",
      "externalIds": {"DBLP": "journals/corr/abs-1709-08624", "MAG": "2949061076",
      "ArXiv": "1709.08624", "DOI": "10.1609/aaai.v32i1.11957", "CorpusId": 3389583},
      "corpusId": 3389583, "publicationVenue": {"id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
      "name": "AAAI Conference on Artificial Intelligence", "type": "conference",
      "alternate_names": ["National Conference on Artificial Intelligence", "National
      Conf Artif Intell", "AAAI Conf Artif Intell", "AAAI"], "url": "http://www.aaai.org/"},
      "url": "https://www.semanticscholar.org/paper/485552d2711868b54d5fcddc92c746b09afeab07",
      "title": "Long Text Generation via Adversarial Training with Leaked Information",
      "abstract": "\n \n Automatically generating coherent and semantically meaningful
      text has many applications in machine translation, dialogue systems, image captioning,
      etc. Recently, by combining with policy gradient, Generative Adversarial Nets(GAN)
      that use a discriminative model to guide the training of the generative model
      as a reinforcement learning policy has shown promising results in text generation.
      However, the scalar guiding signal is only available after the entire text has
      been generated and lacks intermediate information about text structure during
      the generative process. As such, it limits its success when the length of the
      generated text samples is long (more than 20 words). In this paper, we propose
      a new framework, called LeakGAN, to address the problem for long text generation.
      We allow the discriminative net to leak its own high-level extracted features
      to the generative net to further help the guidance. The generator incorporates
      such informative signals into all generation steps through an additional MANAGER
      module, which takes the extracted features of current generated words and outputs
      a latent vector to guide the WORKER module for next-word generation.Our extensive
      experiments on synthetic data and various real-world tasks with Turing test
      demonstrate that LeakGAN is highly effective in long text generation and also
      improves the performance in short text generation scenarios. More importantly,
      without any supervision, LeakGAN would be able to implicitly learn sentence
      structures only through the interaction between MANAGER and WORKER.\n \n", "venue":
      "AAAI Conference on Artificial Intelligence", "year": 2017, "referenceCount":
      31, "citationCount": 415, "influentialCitationCount": 61, "isOpenAccess": true,
      "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/11957/11816",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2017-09-24", "journal": {"volume": "abs/1709.08624", "name":
      "ArXiv"}, "authors": [{"authorId": "15563286", "name": "Jiaxian Guo"}, {"authorId":
      "2051646999", "name": "Sidi Lu"}, {"authorId": "145834074", "name": "Han Cai"},
      {"authorId": "2108309275", "name": "Weinan Zhang"}, {"authorId": "1811427",
      "name": "Yong Yu"}, {"authorId": "39055225", "name": "Jun Wang"}]}}, {"intents":
      [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId": "ea35ade941d16c0a62cbda3046fb1b08389ad497",
      "externalIds": {"DBLP": "conf/nips/PedregosaLL17", "ArXiv": "1707.06468", "MAG":
      "2952031106", "CorpusId": 780327}, "corpusId": 780327, "publicationVenue": {"id":
      "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/ea35ade941d16c0a62cbda3046fb1b08389ad497",
      "title": "Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite
      Optimization", "abstract": "Due to their simplicity and excellent performance,
      parallel asynchronous variants of stochastic gradient descent have become popular
      methods to solve a wide range of large-scale optimization problems on multi-core
      architectures. Yet, despite their practical success, support for nonsmooth objectives
      is still lacking, making them unsuitable for many problems of interest in machine
      learning, such as the Lasso, group Lasso or empirical risk minimization with
      convex constraints. \nIn this work, we propose and analyze ProxASAGA, a fully
      asynchronous sparse method inspired by SAGA, a variance reduced incremental
      gradient algorithm. The proposed method is easy to implement and significantly
      outperforms the state of the art on several nonsmooth, large-scale problems.
      We prove that our method achieves a theoretical linear speedup with respect
      to the sequential version under assumptions on the sparsity of gradients and
      block-separability of the proximal term. Empirical benchmarks on a multi-core
      architecture illustrate practical speedups of up to 12x on a 20-core machine.",
      "venue": "Neural Information Processing Systems", "year": 2017, "referenceCount":
      42, "citationCount": 36, "influentialCitationCount": 6, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-07-20",
      "journal": {"pages": "56-65"}, "authors": [{"authorId": "2570016", "name": "Fabian
      Pedregosa"}, {"authorId": "37212795", "name": "R\u00e9mi Leblond"}, {"authorId":
      "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"intents": ["methodology",
      "background", "result"], "isInfluential": true, "contexts": ["Arjovsky et al.
      [4] experimented with these two \ud835\udc3a losses and claimed that in both
      cases the gradients cause instability to the GAN training.", "WGAN [4] has solved
      the vanishing gradient and mode collapse problems of the original GAN by replacing
      the cost in Eq.", "\u2026on ImageNet, and better or equal quality on CIFAR-10
      and STL-10, compared to the previous training stabilization techniques that
      include weight clipping [4], gradient penalty [74, 43], batch normalization
      [26], weight normalization [63], layer normalization [5], and orthonormal regularization
      [7].", "In previous works that stabilized GAN training ([21, 4, 56]) it was
      emphasized that \ud835\udc37 should be a K-Lipshitz continuous function, forcing
      it not to change rapidly.", "Plots are taken from [4] problems by using di\ufb00erent
      architectures for \ud835\udc37 or \ud835\udc3a , modifying the cost function,
      and more.", "(11) is highly intractable, so the authors estimate the EM cost
      with: where { \ud835\udc53 \ud835\udc64 } \ud835\udc64 \u2208W is a parameterized
      family of all functions that are \ud835\udc3e -Lipschitz for some \ud835\udc3e
      ( || \ud835\udc53 || \u2264 \ud835\udc3e )."], "citedPaper": {"paperId": "acd87843a451d18b4dc6474ddce1ae946429eaf1",
      "externalIds": {"MAG": "2739748921", "DBLP": "conf/icml/ArjovskyCB17", "CorpusId":
      2057420}, "corpusId": 2057420, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/acd87843a451d18b4dc6474ddce1ae946429eaf1",
      "title": "Wasserstein Generative Adversarial Networks", "abstract": "We introduce
      a new algorithm named WGAN, an alternative to traditional GAN training. In this
      new model, we show that we can improve the stability of learning, get rid of
      problems like mode collapse, and provide meaningful learning curves useful for
      debugging and hyperparameter searches. Furthermore, we show that the corresponding
      optimization problem is sound, and provide extensive theoretical work highlighting
      the deep connections to different distances between distributions.", "venue":
      "International Conference on Machine Learning", "year": 2017, "referenceCount":
      25, "citationCount": 6205, "influentialCitationCount": 1154, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-07-17",
      "journal": {"pages": "214-223"}, "authors": [{"authorId": "2877311", "name":
      "Mart\u00edn Arjovsky"}, {"authorId": "2127604", "name": "Soumith Chintala"},
      {"authorId": "52184096", "name": "L. Bottou"}]}}, {"intents": [], "isInfluential":
      false, "contexts": [], "citedPaper": {"paperId": "5ddd38a5df945e4afee68d96ed51fd6ca1f7d4cf",
      "externalIds": {"MAG": "2682189153", "DBLP": "journals/corr/ArpitJBKBKMFCBL17",
      "ArXiv": "1706.05394", "CorpusId": 11455421}, "corpusId": 11455421, "publicationVenue":
      {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
      on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
      Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/5ddd38a5df945e4afee68d96ed51fd6ca1f7d4cf",
      "title": "A Closer Look at Memorization in Deep Networks", "abstract": "We examine
      the role of memorization in deep learning, drawing connections to capacity,
      generalization, and adversarial robustness. While deep networks are capable
      of memorizing noise data, our results suggest that they tend to prioritize learning
      simple patterns first. In our experiments, we expose qualitative differences
      in gradient-based optimization of deep neural networks (DNNs) on noise vs. real
      data. We also demonstrate that for appropriately tuned explicit regularization
      (e.g., dropout) we can degrade DNN training performance on noise datasets without
      compromising generalization on real data. Our analysis suggests that the notions
      of effective capacity which are dataset independent are unlikely to explain
      the generalization performance of deep networks when trained with gradient based
      methods because training data itself plays an important role in determining
      the degree of memorization.", "venue": "International Conference on Machine
      Learning", "year": 2017, "referenceCount": 42, "citationCount": 1367, "influentialCitationCount":
      132, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2017-06-16", "journal": {"pages": "233-242"},
      "authors": [{"authorId": "2309967", "name": "Devansh Arpit"}, {"authorId": "40569328",
      "name": "Stanislaw Jastrzebski"}, {"authorId": "2482072", "name": "Nicolas Ballas"},
      {"authorId": "145055042", "name": "David Krueger"}, {"authorId": "2416433",
      "name": "Emmanuel Bengio"}, {"authorId": "19308176", "name": "Maxinder S. Kanwal"},
      {"authorId": "3422058", "name": "Tegan Maharaj"}, {"authorId": "35988982", "name":
      "Asja Fischer"}, {"authorId": "1760871", "name": "Aaron C. Courville"}, {"authorId":
      "1751762", "name": "Yoshua Bengio"}, {"authorId": "1388317459", "name": "Simon
      Lacoste-Julien"}]}}, {"intents": [], "isInfluential": false, "contexts": [],
      "citedPaper": {"paperId": "345bbd344177815dfb9214c61403cb7eac6de450", "externalIds":
      {"DBLP": "conf/iclr/LeblondAOL18", "ArXiv": "1706.04499", "MAG": "2950486958",
      "CorpusId": 34984289}, "corpusId": 34984289, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/345bbd344177815dfb9214c61403cb7eac6de450",
      "title": "SEARNN: Training RNNs with Global-Local Losses", "abstract": "We propose
      SEARNN, a novel training algorithm for recurrent neural networks (RNNs) inspired
      by the \"learning to search\" (L2S) approach to structured prediction. RNNs
      have been widely successful in structured prediction applications such as machine
      translation or parsing, and are commonly trained using maximum likelihood estimation
      (MLE). Unfortunately, this training loss is not always an appropriate surrogate
      for the test error: by only maximizing the ground truth probability, it fails
      to exploit the wealth of information offered by structured losses. Further,
      it introduces discrepancies between training and predicting (such as exposure
      bias) that may hurt test performance. Instead, SEARNN leverages test-alike search
      space exploration to introduce global-local losses that are closer to the test
      error. We first demonstrate improved performance over MLE on two different tasks:
      OCR and spelling correction. Then, we propose a subsampling strategy to enable
      SEARNN to scale to large vocabulary sizes. This allows us to validate the benefits
      of our approach on a machine translation task.", "venue": "International Conference
      on Learning Representations", "year": 2017, "referenceCount": 34, "citationCount":
      49, "influentialCitationCount": 10, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2017-06-14", "journal":
      {"volume": "abs/1706.04499", "name": "ArXiv"}, "authors": [{"authorId": "37212795",
      "name": "R\u00e9mi Leblond"}, {"authorId": "2285263", "name": "Jean-Baptiste
      Alayrac"}, {"authorId": "145319877", "name": "A. Osokin"}, {"authorId": "1388317459",
      "name": "Simon Lacoste-Julien"}]}}, {"intents": ["methodology"], "isInfluential":
      true, "contexts": ["[76] showed that deconvolutional layers achieve good visualization
      for CNNs; this allowed the DCGAN generator to create high-resolution images
      for the \ufb01rst time.", "CNNs: Convolutional neural networks (CNNs) were proposed
      by LeCun et al. [37]; These networks consist of trained spatial \ufb01lters
      applied on hidden activations throughout their architecture.", "Its architecture
      is based on the Self-attention GAN (SAGAN) [77], which employs a self-attention
      mechanism in both D and G, to capture a large receptive field without sacrificing
      computational efficiency for CNNs [69].", "Deconvolutional networks can be conceived
      as CNNs that use the same components but in reverse, projecting features into
      the image pixel space.", "Its architecture is based on the Self-attention GAN
      (SAGAN) [77], which employs a self-attention mechanism in both \ud835\udc37
      and \ud835\udc3a , to capture a large receptive \ufb01eld without sacri\ufb01cing
      computational e\ufb03ciency for CNNs [69]."], "citedPaper": {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
      "externalIds": {"MAG": "2950858113", "DBLP": "conf/nips/VaswaniSPUJGKP17", "ArXiv":
      "1706.03762", "CorpusId": 13756489}, "corpusId": 13756489, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776",
      "title": "Attention is All you Need", "abstract": "The dominant sequence transduction
      models are based on complex recurrent or convolutional neural networks in an
      encoder-decoder configuration. The best performing models also connect the encoder
      and decoder through an attention mechanism. We propose a new simple network
      architecture, the Transformer, based solely on attention mechanisms, dispensing
      with recurrence and convolutions entirely. Experiments on two machine translation
      tasks show these models to be superior in quality while being more parallelizable
      and requiring significantly less time to train. Our model achieves 28.4 BLEU
      on the WMT 2014 English-to-German translation task, improving over the existing
      best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French
      translation task, our model establishes a new single-model state-of-the-art
      BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction
      of the training costs of the best models from the literature. We show that the
      Transformer generalizes well to other tasks by applying it successfully to English
      constituency parsing both with large and limited training data.", "venue": "Neural
      Information Processing Systems", "year": 2017, "referenceCount": 42, "citationCount":
      75366, "influentialCitationCount": 14351, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2017-06-12", "journal": {"pages": "5998-6008"}, "authors":
      [{"authorId": "40348417", "name": "Ashish Vaswani"}, {"authorId": "1846258",
      "name": "Noam M. Shazeer"}, {"authorId": "3877127", "name": "Niki Parmar"},
      {"authorId": "39328010", "name": "Jakob Uszkoreit"}, {"authorId": "145024664",
      "name": "Llion Jones"}, {"authorId": "19177000", "name": "Aidan N. Gomez"},
      {"authorId": "40527594", "name": "Lukasz Kaiser"}, {"authorId": "3443442", "name":
      "Illia Polosukhin"}]}}, {"intents": ["background"], "isInfluential": false,
      "contexts": ["It was proved successful in many domains such as computer vision
      [14, 29, 25, 38], semantic segmentation [39, 27, 70, 24], time-series synthesis
      [9, 23], image editing [61, 36, 19, 3, 75], natural language processing [15,
      28, 22], text-to-image generation [59, 58, 54], and many more."], "citedPaper":
      {"paperId": "9d8978ee319d671283a90761aaed150c7cc9154b", "externalIds": {"MAG":
      "2951809604", "ArXiv": "1706.00409", "DBLP": "journals/corr/LampleZUBDR17",
      "CorpusId": 27009824}, "corpusId": 27009824, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/9d8978ee319d671283a90761aaed150c7cc9154b",
      "title": "Fader Networks: Manipulating Images by Sliding Attributes", "abstract":
      "This paper introduces a new encoder-decoder architecture that is trained to
      reconstruct images by disentangling the salient information of the image and
      the values of attributes directly in the latent space. As a result, after training,
      our model can generate different realistic versions of an input image by varying
      the attribute values. By using continuous attribute values, we can choose how
      much a specific attribute is perceivable in the generated image. This property
      could allow for applications where users can modify an image using sliding knobs,
      like faders on a mixing console, to change the facial expression of a portrait,
      or to update the color of some objects. Compared to the state-of-the-art which
      mostly relies on training adversarial networks in pixel space by altering attribute
      values at train time, our approach results in much simpler training schemes
      and nicely scales to multiple attributes. We present evidence that our model
      can significantly change the perceived value of the attributes while preserving
      the naturalness of images.", "venue": "Neural Information Processing Systems",
      "year": 2017, "referenceCount": 26, "citationCount": 511, "influentialCitationCount":
      73, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2017-06-01", "journal":
      {"volume": "abs/1706.00409", "name": "ArXiv"}, "authors": [{"authorId": "1830914",
      "name": "Guillaume Lample"}, {"authorId": "3404556", "name": "Neil Zeghidour"},
      {"authorId": "1746841", "name": "Nicolas Usunier"}, {"authorId": "1713934",
      "name": "Antoine Bordes"}, {"authorId": "8905591", "name": "Ludovic Denoyer"},
      {"authorId": "1706809", "name": "Marc''Aurelio Ranzato"}]}}, {"intents": ["background"],
      "isInfluential": false, "contexts": ["An overview of the cross-alignment method
      [12]", "have demonstrated that the equivalent content distribution of the two
      corpora is sufficient to learn to map a sentence from one style to a style-independent
      content vector, which can be decoded to a sentence in the other style while
      reserving the content [12]."], "citedPaper": {"paperId": "66721162f712690bb10928132d402d9bd4460c1b",
      "externalIds": {"MAG": "2617566453", "DBLP": "journals/corr/ShenLBJ17", "ArXiv":
      "1705.09655", "CorpusId": 7296803}, "corpusId": 7296803, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/66721162f712690bb10928132d402d9bd4460c1b",
      "title": "Style Transfer from Non-Parallel Text by Cross-Alignment", "abstract":
      "This paper focuses on style transfer on the basis of non-parallel text. This
      is an instance of a broad family of problems including machine translation,
      decipherment, and sentiment modification. The key challenge is to separate the
      content from other aspects such as style. We assume a shared latent content
      distribution across different text corpora, and propose a method that leverages
      refined alignment of latent representations to perform style transfer. The transferred
      sentences from one style should match example sentences from the other style
      as a population. We demonstrate the effectiveness of this cross-alignment method
      on three tasks: sentiment modification, decipherment of word substitution ciphers,
      and recovery of word order.", "venue": "Neural Information Processing Systems",
      "year": 2017, "referenceCount": 29, "citationCount": 677, "influentialCitationCount":
      172, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2017-05-01", "journal":
      {"volume": "abs/1705.09655", "name": "ArXiv"}, "authors": [{"authorId": "3456570",
      "name": "T. Shen"}, {"authorId": "49986267", "name": "Tao Lei"}, {"authorId":
      "1741283", "name": "R. Barzilay"}, {"authorId": "35132120", "name": "T. Jaakkola"}]}},
      {"intents": [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId":
      "566a2ede36a6493010ea42a7df49916739e00c9d", "externalIds": {"ArXiv": "1704.04086",
      "MAG": "2964337551", "DBLP": "conf/iccv/HuangZLH17", "DOI": "10.1109/ICCV.2017.267",
      "CorpusId": 13058841}, "corpusId": 13058841, "publicationVenue": {"id": "7654260e-79f9-45c5-9663-d72027cf88f3",
      "name": "IEEE International Conference on Computer Vision", "type": "conference",
      "alternate_names": ["ICCV", "IEEE Int Conf Comput Vis", "ICCV Workshops", "ICCV
      Work"], "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"},
      "url": "https://www.semanticscholar.org/paper/566a2ede36a6493010ea42a7df49916739e00c9d",
      "title": "Beyond Face Rotation: Global and Local Perception GAN for Photorealistic
      and Identity Preserving Frontal View Synthesis", "abstract": "Photorealistic
      frontal view synthesis from a single face image has a wide range of applications
      in the field of face recognition. Although data-driven deep learning methods
      have been proposed to address this problem by seeking solutions from ample face
      data, this problem is still challenging because it is intrinsically ill-posed.
      This paper proposes a Two-Pathway Generative Adversarial Network (TP-GAN) for
      photorealistic frontal view synthesis by simultaneously perceiving global structures
      and local details. Four landmark located patch networks are proposed to attend
      to local textures in addition to the commonly used global encoderdecoder network.
      Except for the novel architecture, we make this ill-posed problem well constrained
      by introducing a combination of adversarial loss, symmetry loss and identity
      preserving loss. The combined loss function leverages both frontal face distribution
      and pre-trained discriminative deep face models to guide an identity preserving
      inference of frontal views from profiles. Different from previous deep learning
      methods that mainly rely on intermediate features for recognition, our method
      directly leverages the synthesized identity preserving image for downstream
      tasks like face recognition and attribution estimation. Experimental results
      demonstrate that our method not only presents compelling perceptual results
      but also outperforms state-of-theart results on large pose face recognition.",
      "venue": "IEEE International Conference on Computer Vision", "year": 2017, "referenceCount":
      47, "citationCount": 582, "influentialCitationCount": 81, "isOpenAccess": true,
      "openAccessPdf": {"url": "https://arxiv.org/pdf/1704.04086", "status": null},
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2017-04-13", "journal":
      {"pages": "2458-2467", "name": "2017 IEEE International Conference on Computer
      Vision (ICCV)"}, "authors": [{"authorId": "2140386002", "name": "Rui Huang"},
      {"authorId": "2157156034", "name": "Shu Zhang"}, {"authorId": "2118909553",
      "name": "Tianyu Li"}, {"authorId": "143712929", "name": "R. He"}]}}, {"intents":
      [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId": "aa6f7ad0a06b52a8be89dbd8d056561418276ff2",
      "externalIds": {"MAG": "2605195953", "ArXiv": "1703.10717", "DBLP": "journals/corr/BerthelotSM17",
      "CorpusId": 9957731}, "corpusId": 9957731, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/aa6f7ad0a06b52a8be89dbd8d056561418276ff2",
      "title": "BEGAN: Boundary Equilibrium Generative Adversarial Networks", "abstract":
      "We propose a new equilibrium enforcing method paired with a loss derived from
      the Wasserstein distance for training auto-encoder based Generative Adversarial
      Networks. This method balances the generator and discriminator during training.
      Additionally, it provides a new approximate convergence measure, fast and stable
      training and high visual quality. We also derive a way of controlling the trade-off
      between image diversity and visual quality. We focus on the image generation
      task, setting a new milestone in visual quality, even at higher resolutions.
      This is achieved while using a relatively simple model architecture and a standard
      training procedure.", "venue": "arXiv.org", "year": 2017, "referenceCount":
      21, "citationCount": 1093, "influentialCitationCount": 167, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2017-03-31", "journal": {"volume": "abs/1703.10717", "name": "ArXiv"}, "authors":
      [{"authorId": "39835551", "name": "David Berthelot"}, {"authorId": "22622093",
      "name": "Tom Schumm"}, {"authorId": "2096458", "name": "Luke Metz"}]}}, {"intents":
      ["methodology"], "isInfluential": false, "contexts": ["WGAN is a GAN variant
      in which 1-Wasserstein distance is used to measure the model''s and target distributions
      [2]."], "citedPaper": {"paperId": "edf73ab12595c6709f646f542a0d2b33eb20a3f4",
      "externalIds": {"ArXiv": "1704.00028", "MAG": "2605135824", "DBLP": "conf/nips/GulrajaniAADC17",
      "CorpusId": 10894094}, "corpusId": 10894094, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/edf73ab12595c6709f646f542a0d2b33eb20a3f4",
      "title": "Improved Training of Wasserstein GANs", "abstract": "Generative Adversarial
      Networks (GANs) are powerful generative models, but suffer from training instability.
      The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training
      of GANs, but sometimes can still generate only low-quality samples or fail to
      converge. We find that these problems are often due to the use of weight clipping
      in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired
      behavior. We propose an alternative to clipping weights: penalize the norm of
      gradient of the critic with respect to its input. Our proposed method performs
      better than standard WGAN and enables stable training of a wide variety of GAN
      architectures with almost no hyperparameter tuning, including 101-layer ResNets
      and language models over discrete data. We also achieve high quality generations
      on CIFAR-10 and LSUN bedrooms.", "venue": "Neural Information Processing Systems",
      "year": 2017, "referenceCount": 37, "citationCount": 7787, "influentialCitationCount":
      1502, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
      "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2017-03-31", "journal": {"pages": "5767-5777"},
      "authors": [{"authorId": "2708454", "name": "Ishaan Gulrajani"}, {"authorId":
      "2054472270", "name": "Faruk Ahmed"}, {"authorId": "2877311", "name": "Mart\u00edn
      Arjovsky"}, {"authorId": "3074927", "name": "Vincent Dumoulin"}, {"authorId":
      "1760871", "name": "Aaron C. Courville"}]}}, {"intents": ["background"], "isInfluential":
      false, "contexts": ["Meanwhile, the style transfer utilized high level semantic
      features, which highly improve the quality of the output image and save a great
      amount of time for the producer [9]."], "citedPaper": {"paperId": "c43d954cf8133e6254499f3d68e45218067e4941",
      "externalIds": {"MAG": "2951488027", "ArXiv": "1703.10593", "DBLP": "conf/iccv/ZhuPIE17",
      "DOI": "10.1109/ICCV.2017.244", "CorpusId": 206770979}, "corpusId": 206770979,
      "publicationVenue": {"id": "7654260e-79f9-45c5-9663-d72027cf88f3", "name": "IEEE
      International Conference on Computer Vision", "type": "conference", "alternate_names":
      ["ICCV", "IEEE Int Conf Comput Vis", "ICCV Workshops", "ICCV Work"], "url":
      "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"}, "url": "https://www.semanticscholar.org/paper/c43d954cf8133e6254499f3d68e45218067e4941",
      "title": "Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial
      Networks", "abstract": "Image-to-image translation is a class of vision and
      graphics problems where the goal is to learn the mapping between an input image
      and an output image using a training set of aligned image pairs. However, for
      many tasks, paired training data will not be available. We present an approach
      for learning to translate an image from a source domain X to a target domain
      Y in the absence of paired examples. Our goal is to learn a mapping G : X \u2192
      Y such that the distribution of images from G(X) is indistinguishable from the
      distribution Y using an adversarial loss. Because this mapping is highly under-constrained,
      we couple it with an inverse mapping F : Y \u2192 X and introduce a cycle consistency
      loss to push F(G(X)) \u2248 X (and vice versa). Qualitative results are presented
      on several tasks where paired training data does not exist, including collection
      style transfer, object transfiguration, season transfer, photo enhancement,
      etc. Quantitative comparisons against several prior methods demonstrate the
      superiority of our approach.", "venue": "IEEE International Conference on Computer
      Vision", "year": 2017, "referenceCount": 68, "citationCount": 4470, "influentialCitationCount":
      701, "isOpenAccess": true, "openAccessPdf": {"url": "https://repositorio.unal.edu.co/bitstream/unal/82529/2/98562187.2022.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2017-03-30", "journal": {"pages": "2242-2251", "name": "2017
      IEEE International Conference on Computer Vision (ICCV)"}, "authors": [{"authorId":
      "2436356", "name": "Jun-Yan Zhu"}, {"authorId": "2071929129", "name": "Taesung
      Park"}, {"authorId": "2094770", "name": "Phillip Isola"}, {"authorId": "1763086",
      "name": "Alexei A. Efros"}]}}, {"intents": ["result"], "isInfluential": false,
      "contexts": ["\u2026on ImageNet, and better or equal quality on CIFAR-10 and
      STL-10, compared to the previous training stabilization techniques that include
      weight clipping [4], gradient penalty [74, 43], batch normalization [26], weight
      normalization [63], layer normalization [5], and orthonormal regularization
      [7]."], "citedPaper": {"paperId": "110ee8ab8f652c16fcc3bb767687e1c695c2500b",
      "externalIds": {"ArXiv": "1703.07195", "DBLP": "journals/corr/Wu0ZH17", "MAG":
      "2982014123", "DOI": "10.1145/3343031.3350944", "CorpusId": 10804187}, "corpusId":
      10804187, "publicationVenue": {"id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
      "name": "ACM Multimedia", "type": "conference", "alternate_names": ["MM"]},
      "url": "https://www.semanticscholar.org/paper/110ee8ab8f652c16fcc3bb767687e1c695c2500b",
      "title": "GP-GAN: Towards Realistic High-Resolution Image Blending", "abstract":
      "It is common but challenging to address high-resolution image blending in the
      automatic photo editing application. In this paper, we would like to focus on
      solving the problem of high-resolution image blending, where the composite images
      are provided. We propose a framework called Gaussian-Poisson Generative Adversarial
      Network (GP-GAN) to leverage the strengths of the classical gradient-based approach
      and Generative Adversarial Networks. To the best of our knowledge, it''s the
      first work that explores the capability of GANs in high-resolution image blending
      task. Concretely, we propose Gaussian-Poisson Equation to formulate the high-resolution
      image blending problem, which is a joint optimization constrained by the gradient
      and color information. Inspired by the prior works, we obtain gradient information
      via applying gradient filters. To generate the color information, we propose
      a Blending GAN to learn the mapping between the composite images and the well-blended
      ones. Compared to the alternative methods, our approach can deliver high-resolution,
      realistic images with fewer bleedings and unpleasant artifacts. Experiments
      confirm that our approach achieves the state-of-the-art performance on Transient
      Attributes dataset. A user study on Amazon Mechanical Turk finds that the majority
      of workers are in favor of the proposed method. The source code is available
      in \\urlhttps://github.com/wuhuikai/GP-GAN, and there''s also an online demo
      in \\urlhttp://wuhuikai.me/DeepJS.", "venue": "ACM Multimedia", "year": 2017,
      "referenceCount": 47, "citationCount": 214, "influentialCitationCount": 16,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Book"], "publicationDate": "2017-03-21", "journal": {"name":
      "Proceedings of the 27th ACM International Conference on Multimedia"}, "authors":
      [{"authorId": "9947552", "name": "Huikai Wu"}, {"authorId": "40474289", "name":
      "Shuai Zheng"}, {"authorId": "2086001", "name": "Junge Zhang"}, {"authorId":
      "2887871", "name": "Kaiqi Huang"}]}}, {"intents": [], "isInfluential": false,
      "contexts": [], "citedPaper": {"paperId": "4ebb5535b762c649ddc9b6619e937d245e5e249b",
      "externalIds": {"MAG": "2950672178", "DBLP": "journals/corr/OsokinBL17", "ArXiv":
      "1703.02403", "CorpusId": 14780228}, "corpusId": 14780228, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/4ebb5535b762c649ddc9b6619e937d245e5e249b",
      "title": "On Structured Prediction Theory with Calibrated Convex Surrogate Losses",
      "abstract": "We provide novel theoretical insights on structured prediction
      in the context of efficient convex surrogate loss minimization with consistency
      guarantees. For any task loss, we construct a convex surrogate that can be optimized
      via stochastic gradient descent and we prove tight bounds on the so-called \"calibration
      function\" relating the excess surrogate risk to the actual risk. In contrast
      to prior related work, we carefully monitor the effect of the exponential number
      of classes in the learning guarantees as well as on the optimization complexity.
      As an interesting consequence, we formalize the intuition that some task losses
      make learning harder than others, and that the classical 0-1 loss is ill-suited
      for structured prediction.", "venue": "Neural Information Processing Systems",
      "year": 2017, "referenceCount": 54, "citationCount": 50, "influentialCitationCount":
      2, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2017-03-07", "journal": {"pages": "302-313"}, "authors": [{"authorId": "145319877",
      "name": "A. Osokin"}, {"authorId": "144570279", "name": "F. Bach"}, {"authorId":
      "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"intents": [], "isInfluential":
      false, "contexts": [], "citedPaper": {"paperId": "993f0793ca7217e03afbd29346d03c01109acc49",
      "externalIds": {"DBLP": "conf/iccv/AlayracSLL17", "ArXiv": "1702.02738", "MAG":
      "2594270457", "DOI": "10.1109/ICCV.2017.234", "CorpusId": 2292730}, "corpusId":
      2292730, "publicationVenue": {"id": "7654260e-79f9-45c5-9663-d72027cf88f3",
      "name": "IEEE International Conference on Computer Vision", "type": "conference",
      "alternate_names": ["ICCV", "IEEE Int Conf Comput Vis", "ICCV Workshops", "ICCV
      Work"], "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"},
      "url": "https://www.semanticscholar.org/paper/993f0793ca7217e03afbd29346d03c01109acc49",
      "title": "Joint Discovery of Object States and Manipulation Actions", "abstract":
      "Many human activities involve object manipulations aiming to modify the object
      state. Examples of common state changes include full/empty bottle, open/closed
      door, and attached/detached car wheel. In this work, we seek to automatically
      discover the states of objects and the associated manipulation actions. Given
      a set of videos for a particular task, we propose a joint model that learns
      to identify object states and to localize state-modifying actions. Our model
      is formulated as a discriminative clustering cost with constraints. We assume
      a consistent temporal order for the changes in object states and manipulation
      actions, and introduce new optimization techniques to learn model parameters
      without additional supervision. We demonstrate successful discovery of seven
      manipulation actions and corresponding object states on a new dataset of videos
      depicting real-life object manipulations. We show that our joint formulation
      results in an improvement of object state discovery by action recognition and
      vice versa.", "venue": "IEEE International Conference on Computer Vision", "year":
      2017, "referenceCount": 47, "citationCount": 64, "influentialCitationCount":
      4, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1702.02738",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2017-02-09", "journal": {"pages": "2146-2155", "name": "2017
      IEEE International Conference on Computer Vision (ICCV)"}, "authors": [{"authorId":
      "2285263", "name": "Jean-Baptiste Alayrac"}, {"authorId": "1782755", "name":
      "Josef Sivic"}, {"authorId": "143991676", "name": "I. Laptev"}, {"authorId":
      "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"intents": ["background"],
      "isInfluential": false, "contexts": ["In previous works that stabilized GAN
      training ([21, 4, 56]) it was emphasized that D should be a K-Lipshitz continuous
      function, forcing it not to change rapidly."], "citedPaper": {"paperId": "4b71ab38d3ac6eee0da5e68ef666d33d1028bad8",
      "externalIds": {"ArXiv": "1701.06264", "MAG": "2580360036", "DBLP": "journals/ijcv/Qi20",
      "DOI": "10.1007/s11263-019-01265-2", "CorpusId": 35563}, "corpusId": 35563,
      "publicationVenue": {"id": "939ee07c-6009-43f8-b884-69238b40659e", "name": "International
      Journal of Computer Vision", "type": "journal", "alternate_names": ["Int J Comput
      Vis"], "issn": "0920-5691", "url": "https://www.springer.com/computer/image+processing/journal/11263",
      "alternate_urls": ["https://link.springer.com/journal/11263", "http://link.springer.com/journal/11263"]},
      "url": "https://www.semanticscholar.org/paper/4b71ab38d3ac6eee0da5e68ef666d33d1028bad8",
      "title": "Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities",
      "abstract": null, "venue": "International Journal of Computer Vision", "year":
      2017, "referenceCount": 53, "citationCount": 311, "influentialCitationCount":
      33, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1701.06264",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2017-01-23", "journal":
      {"volume": "128", "pages": "1118 - 1140", "name": "International Journal of
      Computer Vision"}, "authors": [{"authorId": "2272096", "name": "Guo-Jun Qi"}]}},
      {"intents": [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId":
      "176f1d608b918eec8dc4b75e7b6e0acaba84a447", "externalIds": {"ArXiv": "1701.06547",
      "ACL": "D17-1230", "DBLP": "journals/corr/LiMSRJ17", "MAG": "2951520714", "DOI":
      "10.18653/v1/D17-1230", "CorpusId": 98180}, "corpusId": 98180, "publicationVenue":
      {"id": "41bf9ed3-85b3-4c90-b015-150e31690253", "name": "Conference on Empirical
      Methods in Natural Language Processing", "type": "conference", "alternate_names":
      ["Empir Method Nat Lang Process", "Empirical Methods in Natural Language Processing",
      "Conf Empir Method Nat Lang Process", "EMNLP"], "url": "https://www.aclweb.org/portal/emnlp"},
      "url": "https://www.semanticscholar.org/paper/176f1d608b918eec8dc4b75e7b6e0acaba84a447",
      "title": "Adversarial Learning for Neural Dialogue Generation", "abstract":
      "We apply adversarial training to open-domain dialogue generation, training
      a system to produce sequences that are indistinguishable from human-generated
      dialogue utterances. We cast the task as a reinforcement learning problem where
      we jointly train two systems: a generative model to produce response sequences,
      and a discriminator\u2014analagous to the human evaluator in the Turing test\u2014
      to distinguish between the human-generated dialogues and the machine-generated
      ones. In this generative adversarial network approach, the outputs from the
      discriminator are used to encourage the system towards more human-like dialogue.
      Further, we investigate models for adversarial evaluation that uses success
      in fooling an adversary as a dialogue evaluation metric, while avoiding a number
      of potential pitfalls. Experimental results on several metrics, including adversarial
      evaluation, demonstrate that the adversarially-trained system generates higher-quality
      responses than previous baselines", "venue": "Conference on Empirical Methods
      in Natural Language Processing", "year": 2017, "referenceCount": 52, "citationCount":
      861, "influentialCitationCount": 114, "isOpenAccess": true, "openAccessPdf":
      {"url": "https://www.aclweb.org/anthology/D17-1230.pdf", "status": null}, "fieldsOfStudy":
      ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2017-01-23", "journal":
      {"volume": "abs/1701.06547", "name": "ArXiv"}, "authors": [{"authorId": "49298465",
      "name": "Jiwei Li"}, {"authorId": "145768639", "name": "Will Monroe"}, {"authorId":
      "10238549", "name": "Tianlin Shi"}, {"authorId": "152857609", "name": "S\u00e9bastien
      Jean"}, {"authorId": "1863425", "name": "Alan Ritter"}, {"authorId": "1746807",
      "name": "Dan Jurafsky"}]}}, {"intents": [], "isInfluential": false, "contexts":
      [], "citedPaper": {"paperId": "ea67d2d5f2a7d5760ec6b67ea93d11dd5affa921", "externalIds":
      {"DBLP": "journals/corr/ZhangXLZHWM16", "MAG": "2949526397", "ArXiv": "1612.03242",
      "DOI": "10.1109/ICCV.2017.629", "CorpusId": 1277217}, "corpusId": 1277217, "publicationVenue":
      {"id": "7654260e-79f9-45c5-9663-d72027cf88f3", "name": "IEEE International Conference
      on Computer Vision", "type": "conference", "alternate_names": ["ICCV", "IEEE
      Int Conf Comput Vis", "ICCV Workshops", "ICCV Work"], "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"},
      "url": "https://www.semanticscholar.org/paper/ea67d2d5f2a7d5760ec6b67ea93d11dd5affa921",
      "title": "StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative
      Adversarial Networks", "abstract": "Synthesizing high-quality images from text
      descriptions is a challenging problem in computer vision and has many practical
      applications. Samples generated by existing textto- image approaches can roughly
      reflect the meaning of the given descriptions, but they fail to contain necessary
      details and vivid object parts. In this paper, we propose Stacked Generative
      Adversarial Networks (StackGAN) to generate 256.256 photo-realistic images conditioned
      on text descriptions. We decompose the hard problem into more manageable sub-problems
      through a sketch-refinement process. The Stage-I GAN sketches the primitive
      shape and colors of the object based on the given text description, yielding
      Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text
      descriptions as inputs, and generates high-resolution images with photo-realistic
      details. It is able to rectify defects in Stage-I results and add compelling
      details with the refinement process. To improve the diversity of the synthesized
      images and stabilize the training of the conditional-GAN, we introduce a novel
      Conditioning Augmentation technique that encourages smoothness in the latent
      conditioning manifold. Extensive experiments and comparisons with state-of-the-arts
      on benchmark datasets demonstrate that the proposed method achieves significant
      improvements on generating photo-realistic images conditioned on text descriptions.",
      "venue": "IEEE International Conference on Computer Vision", "year": 2016, "referenceCount":
      46, "citationCount": 2379, "influentialCitationCount": 267, "isOpenAccess":
      true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1612.03242", "status":
      null}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2016-12-10",
      "journal": {"pages": "5908-5916", "name": "2017 IEEE International Conference
      on Computer Vision (ICCV)"}, "authors": [{"authorId": "120811666", "name": "Han
      Zhang"}, {"authorId": "2118716442", "name": "Tao Xu"}, {"authorId": "47893312",
      "name": "Hongsheng Li"}, {"authorId": "1753384", "name": "Shaoting Zhang"},
      {"authorId": "31843833", "name": "Xiaogang Wang"}, {"authorId": "143713756",
      "name": "Xiaolei Huang"}, {"authorId": "1711560", "name": "Dimitris N. Metaxas"}]}},
      {"intents": [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId":
      "2e55ba6c97ce5eb55abd959909403fe8da7e9fe9", "externalIds": {"ArXiv": "1612.00796",
      "MAG": "2560647685", "DBLP": "journals/corr/KirkpatrickPRVD16", "DOI": "10.1073/pnas.1611835114",
      "CorpusId": 4704285, "PubMed": "28292907"}, "corpusId": 4704285, "publicationVenue":
      {"id": "bb95bf2e-8383-4748-bf9d-d6906d091085", "name": "Proceedings of the National
      Academy of Sciences of the United States of America", "type": "journal", "alternate_names":
      ["PNAS", "PNAS online", "Proceedings of the National Academy of Sciences of
      the United States of America.", "Proc National Acad Sci", "Proceedings of the
      National Academy of Sciences", "Proc National Acad Sci u s Am"], "issn": "0027-8424",
      "alternate_issns": ["1091-6490"], "url": "https://www.jstor.org/journal/procnatiacadscie",
      "alternate_urls": ["http://www.pnas.org/", "https://www.pnas.org/", "http://www.jstor.org/journals/00278424.html",
      "www.pnas.org/"]}, "url": "https://www.semanticscholar.org/paper/2e55ba6c97ce5eb55abd959909403fe8da7e9fe9",
      "title": "Overcoming catastrophic forgetting in neural networks", "abstract":
      "Significance Deep neural networks are currently the most successful machine-learning
      technique for solving a variety of tasks, including language translation, image
      classification, and image generation. One weakness of such models is that, unlike
      humans, they are unable to learn multiple tasks sequentially. In this work we
      propose a practical solution to train such models sequentially by protecting
      the weights important for previous tasks. This approach, inspired by synaptic
      consolidation in neuroscience, enables state of the art results on multiple
      reinforcement learning problems experienced sequentially. The ability to learn
      tasks in a sequential fashion is crucial to the development of artificial intelligence.
      Until now neural networks have not been capable of this and it has been widely
      thought that catastrophic forgetting is an inevitable feature of connectionist
      models. We show that it is possible to overcome this limitation and train networks
      that can maintain expertise on tasks that they have not experienced for a long
      time. Our approach remembers old tasks by selectively slowing down learning
      on the weights important for those tasks. We demonstrate our approach is scalable
      and effective by solving a set of classification tasks based on a hand-written
      digit dataset and by learning several Atari 2600 games sequentially.", "venue":
      "Proceedings of the National Academy of Sciences of the United States of America",
      "year": 2016, "referenceCount": 48, "citationCount": 4713, "influentialCitationCount":
      949, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.pnas.org/content/pnas/114/13/3521.full.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science", "Medicine", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Medicine", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2016-12-02", "journal":
      {"volume": "114", "pages": "3521 - 3526", "name": "Proceedings of the National
      Academy of Sciences"}, "authors": [{"authorId": "2066516991", "name": "J. Kirkpatrick"},
      {"authorId": "1996134", "name": "Razvan Pascanu"}, {"authorId": "3422052", "name":
      "Neil C. Rabinowitz"}, {"authorId": "144056327", "name": "J. Veness"}, {"authorId":
      "2755582", "name": "Guillaume Desjardins"}, {"authorId": "2228824", "name":
      "Andrei A. Rusu"}, {"authorId": "8181864", "name": "Kieran Milan"}, {"authorId":
      "34660073", "name": "John Quan"}, {"authorId": "34505275", "name": "Tiago Ramalho"},
      {"authorId": "1398898827", "name": "A. Grabska-Barwinska"}, {"authorId": "48987704",
      "name": "D. Hassabis"}, {"authorId": "2388737", "name": "C. Clopath"}, {"authorId":
      "2106164", "name": "D. Kumaran"}, {"authorId": "2315504", "name": "R. Hadsell"}]}},
      {"intents": ["background"], "isInfluential": false, "contexts": ["It was proved
      successful in many domains such as computer vision [14, 29, 25, 38], semantic
      segmentation [39, 27, 70, 24], time-series synthesis [9, 23], image editing
      [61, 36, 19, 3, 75], natural language processing [15, 28, 22], text-to-image
      generation [59, 58, 54], and many more."], "citedPaper": {"paperId": "fcabf1c0f4a26431d4df95ddeec2b1dff9b3e928",
      "externalIds": {"ArXiv": "1611.08408", "DBLP": "journals/corr/LucCCV16", "MAG":
      "2950040358", "CorpusId": 16991836}, "corpusId": 16991836, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/fcabf1c0f4a26431d4df95ddeec2b1dff9b3e928",
      "title": "Semantic Segmentation using Adversarial Networks", "abstract": "Adversarial
      training has been shown to produce state of the art results for generative image
      modeling. In this paper we propose an adversarial training approach to train
      semantic segmentation models. We train a convolutional semantic segmentation
      network along with an adversarial network that discriminates segmentation maps
      coming either from the ground truth or from the segmentation network. The motivation
      for our approach is that it can detect and correct higher-order inconsistencies
      between ground truth segmentation maps and the ones produced by the segmentation
      net. Our experiments show that our adversarial training approach leads to improved
      accuracy on the Stanford Background and PASCAL VOC 2012 datasets.", "venue":
      "Neural Information Processing Systems", "year": 2016, "referenceCount": 36,
      "citationCount": 663, "influentialCitationCount": 48, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2016-11-25", "journal": {"volume": "abs/1611.08408", "name":
      "ArXiv"}, "authors": [{"authorId": "152831141", "name": "Pauline Luc"}, {"authorId":
      "2341378", "name": "C. Couprie"}, {"authorId": "2127604", "name": "Soumith Chintala"},
      {"authorId": "34602236", "name": "Jakob Verbeek"}]}}, {"intents": ["background"],
      "isInfluential": false, "contexts": ["It was proved successful in many domains
      such as computer vision [14, 29, 25, 38], semantic segmentation [39, 27, 70,
      24], time-series synthesis [9, 23], image editing [61, 36, 19, 3, 75], natural
      language processing [15, 28, 22], text-to-image generation [59, 58, 54], and
      many more."], "citedPaper": {"paperId": "60b3fb579734593ebedaa177569052e90a778009",
      "externalIds": {"DBLP": "journals/corr/JetchevBV16", "ArXiv": "1611.08207",
      "MAG": "2557969682", "CorpusId": 2581941}, "corpusId": 2581941, "publicationVenue":
      {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
      ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/60b3fb579734593ebedaa177569052e90a778009",
      "title": "Texture Synthesis with Spatial Generative Adversarial Networks", "abstract":
      "Generative adversarial networks (GANs) are a recent approach to train generative
      models of data, which have been shown to work particularly well on image data.
      In the current paper we introduce a new model for texture synthesis based on
      GAN learning. By extending the input noise distribution space from a single
      vector to a whole spatial tensor, we create an architecture with properties
      well suited to the task of texture synthesis, which we call spatial GAN (SGAN).
      To our knowledge, this is the first successful completely data-driven texture
      synthesis method based on GANs. \nOur method has the following features which
      make it a state of the art algorithm for texture synthesis: high image quality
      of the generated textures, very high scalability w.r.t. the output texture size,
      fast real-time forward generation, the ability to fuse multiple diverse source
      images in complex textures. To illustrate these capabilities we present multiple
      experiments with different classes of texture images and use cases. We also
      discuss some limitations of our method with respect to the types of texture
      images it can synthesize, and compare it to other neural techniques for texture
      generation.", "venue": "arXiv.org", "year": 2016, "referenceCount": 20, "citationCount":
      188, "influentialCitationCount": 25, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2016-11-24", "journal":
      {"volume": "abs/1611.08207", "name": "ArXiv"}, "authors": [{"authorId": "2035925",
      "name": "Nikolay Jetchev"}, {"authorId": "35959543", "name": "Urs M. Bergmann"},
      {"authorId": "2742129", "name": "Roland Vollgraf"}]}}, {"intents": ["background"],
      "isInfluential": false, "contexts": ["It was proved successful in many domains
      such as computer vision [14, 29, 25, 38], semantic segmentation [39, 27, 70,
      24], time-series synthesis [9, 23], image editing [61, 36, 19, 3, 75], natural
      language processing [15, 28, 22], text-to-image generation [59, 58, 54], and
      many more."], "citedPaper": {"paperId": "8acbe90d5b852dadea7810345451a99608ee54c7",
      "externalIds": {"MAG": "2963073614", "DBLP": "conf/cvpr/IsolaZZE17", "ArXiv":
      "1611.07004", "DOI": "10.1109/CVPR.2017.632", "CorpusId": 6200260}, "corpusId":
      6200260, "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
      "name": "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
      ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
      "url": "https://www.semanticscholar.org/paper/8acbe90d5b852dadea7810345451a99608ee54c7",
      "title": "Image-to-Image Translation with Conditional Adversarial Networks",
      "abstract": "We investigate conditional adversarial networks as a general-purpose
      solution to image-to-image translation problems. These networks not only learn
      the mapping from input image to output image, but also learn a loss function
      to train this mapping. This makes it possible to apply the same generic approach
      to problems that traditionally would require very different loss formulations.
      We demonstrate that this approach is effective at synthesizing photos from label
      maps, reconstructing objects from edge maps, and colorizing images, among other
      tasks. Moreover, since the release of the pix2pix software associated with this
      paper, hundreds of twitter users have posted their own artistic experiments
      using our system. As a community, we no longer hand-engineer our mapping functions,
      and this work suggests we can achieve reasonable results without handengineering
      our loss functions either.", "venue": "Computer Vision and Pattern Recognition",
      "year": 2016, "referenceCount": 70, "citationCount": 15920, "influentialCitationCount":
      2860, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1611.07004",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2016-11-21", "journal": {"pages": "5967-5976", "name": "2017
      IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"}, "authors":
      [{"authorId": "2094770", "name": "Phillip Isola"}, {"authorId": "2436356", "name":
      "Jun-Yan Zhu"}, {"authorId": "1822702", "name": "Tinghui Zhou"}, {"authorId":
      "1763086", "name": "Alexei A. Efros"}]}}, {"intents": ["background"], "isInfluential":
      false, "contexts": ["LSGAN will result in more stable performance [3]."], "citedPaper":
      {"paperId": "74ff6d48f9c62e937023106629d27ef2d2ddf8bc", "externalIds": {"MAG":
      "2949496494", "DBLP": "conf/iccv/MaoLXLWS17", "ArXiv": "1611.04076", "DOI":
      "10.1109/ICCV.2017.304", "CorpusId": 206771128}, "corpusId": 206771128, "publicationVenue":
      {"id": "7654260e-79f9-45c5-9663-d72027cf88f3", "name": "IEEE International Conference
      on Computer Vision", "type": "conference", "alternate_names": ["ICCV", "IEEE
      Int Conf Comput Vis", "ICCV Workshops", "ICCV Work"], "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"},
      "url": "https://www.semanticscholar.org/paper/74ff6d48f9c62e937023106629d27ef2d2ddf8bc",
      "title": "Least Squares Generative Adversarial Networks", "abstract": "Unsupervised
      learning with generative adversarial networks (GANs) has proven hugely successful.
      Regular GANs hypothesize the discriminator as a classifier with the sigmoid
      cross entropy loss function. However, we found that this loss function may lead
      to the vanishing gradients problem during the learning process. To overcome
      such a problem, we propose in this paper the Least Squares Generative Adversarial
      Networks (LSGANs) which adopt the least squares loss function for the discriminator.
      We show that minimizing the objective function of LSGAN yields minimizing the
      Pearson X2 divergence. There are two benefits of LSGANs over regular GANs. First,
      LSGANs are able to generate higher quality images than regular GANs. Second,
      LSGANs perform more stable during the learning process. We evaluate LSGANs on
      LSUN and CIFAR-10 datasets and the experimental results show that the images
      generated by LSGANs are of better quality than the ones generated by regular
      GANs. We also conduct two comparison experiments between LSGANs and regular
      GANs to illustrate the stability of LSGANs.", "venue": "IEEE International Conference
      on Computer Vision", "year": 2016, "referenceCount": 38, "citationCount": 3905,
      "influentialCitationCount": 477, "isOpenAccess": true, "openAccessPdf": {"url":
      "https://arxiv.org/pdf/1611.04076", "status": null}, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2016-11-13", "journal": {"pages": "2813-2821", "name": "2017 IEEE International
      Conference on Computer Vision (ICCV)"}, "authors": [{"authorId": "34443348",
      "name": "Xudong Mao"}, {"authorId": "2117895101", "name": "Qing Li"}, {"authorId":
      "3607957", "name": "Haoran Xie"}, {"authorId": "144031692", "name": "Raymond
      Y. K. Lau"}, {"authorId": "2118453660", "name": "Zhen Wang"}, {"authorId": "32309056",
      "name": "Stephen Paul Smolley"}]}}, {"intents": [], "isInfluential": false,
      "contexts": [], "citedPaper": {"paperId": "024d30897e0a2b036bc122163a954b7f1a1d0679",
      "externalIds": {"MAG": "2585630030", "ArXiv": "1612.02136", "DBLP": "journals/corr/CheLJBL16",
      "CorpusId": 13002849}, "corpusId": 13002849, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/024d30897e0a2b036bc122163a954b7f1a1d0679",
      "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although
      Generative Adversarial Networks achieve state-of-the-art results on a variety
      of generative tasks, they are regarded as highly unstable and prone to miss
      modes. We argue that these bad behaviors of GANs are due to the very particular
      functional shape of the trained discriminators in high dimensional spaces, which
      can easily make training stuck or push probability mass in the wrong direction,
      towards that of higher concentration than that of the data generating distribution.
      We introduce several ways of regularizing the objective, which can dramatically
      stabilize the training of GAN models. We also show that our regularizers can
      help the fair distribution of probability mass across the modes of the data
      generating distribution, during the early phases of training and thus providing
      a unified solution to the missing modes problem.", "venue": "International Conference
      on Learning Representations", "year": 2016, "referenceCount": 28, "citationCount":
      493, "influentialCitationCount": 50, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"},
      {"category": "Mathematics", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2016-11-05", "journal": {"volume": "abs/1612.02136",
      "name": "ArXiv"}, "authors": [{"authorId": "47828117", "name": "Tong Che"},
      {"authorId": "3305402", "name": "Yanran Li"}, {"authorId": "12782441", "name":
      "Athul Paul Jacob"}, {"authorId": "1751762", "name": "Yoshua Bengio"}, {"authorId":
      "2138991880", "name": "Wenjie Li"}]}}, {"intents": [], "isInfluential": false,
      "contexts": [], "citedPaper": {"paperId": "488bb25e0b1777847f04c943e6dbc4f84415b712",
      "externalIds": {"DBLP": "journals/corr/MetzPPS16", "ArXiv": "1611.02163", "MAG":
      "2554314924", "CorpusId": 6610705}, "corpusId": 6610705, "publicationVenue":
      {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
      on Learning Representations", "type": "conference", "alternate_names": ["Int
      Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/488bb25e0b1777847f04c943e6dbc4f84415b712",
      "title": "Unrolled Generative Adversarial Networks", "abstract": "We introduce
      a method to stabilize Generative Adversarial Networks (GANs) by defining the
      generator objective with respect to an unrolled optimization of the discriminator.
      This allows training to be adjusted between using the optimal discriminator
      in the generator''s objective, which is ideal but infeasible in practice, and
      using the current value of the discriminator, which is often unstable and leads
      to poor solutions. We show how this technique solves the common problem of mode
      collapse, stabilizes training of GANs with complex recurrent generators, and
      increases diversity and coverage of the data distribution by the generator.",
      "venue": "International Conference on Learning Representations", "year": 2016,
      "referenceCount": 55, "citationCount": 904, "influentialCitationCount": 118,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science",
      "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2016-11-04", "journal": {"volume": "abs/1611.02163", "name": "ArXiv"}, "authors":
      [{"authorId": "2096458", "name": "Luke Metz"}, {"authorId": "16443937", "name":
      "Ben Poole"}, {"authorId": "144846367", "name": "David Pfau"}, {"authorId":
      "1407546424", "name": "Jascha Narain Sohl-Dickstein"}]}}, {"intents": ["background"],
      "isInfluential": false, "contexts": ["CGAN was originally proposed as an extension
      of the original GAN, where both the discriminator and generator were fed by
      an additional class of the image [45, 50]."], "citedPaper": {"paperId": "ecc0edd450ae7e52f65ddf61405b30ad6dbabdd7",
      "externalIds": {"ArXiv": "1610.09585", "MAG": "2548275288", "DBLP": "conf/icml/OdenaOS17",
      "CorpusId": 1099052}, "corpusId": 1099052, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/ecc0edd450ae7e52f65ddf61405b30ad6dbabdd7",
      "title": "Conditional Image Synthesis with Auxiliary Classifier GANs", "abstract":
      "In this paper we introduce new methods for the improved training of generative
      adversarial networks (GANs) for image synthesis. We construct a variant of GANs
      employing label conditioning that results in 128 x 128 resolution image samples
      exhibiting global coherence. We expand on previous work for image quality assessment
      to provide two new analyses for assessing the discriminability and diversity
      of samples from class-conditional image synthesis models. These analyses demonstrate
      that high resolution samples provide class information not present in low resolution
      samples. Across 1000 ImageNet classes, 128 x 128 samples are more than twice
      as discriminable as artificially resized 32 x 32 samples. In addition, 84.7%
      of the classes have samples exhibiting diversity comparable to real ImageNet
      data.", "venue": "International Conference on Machine Learning", "year": 2016,
      "referenceCount": 44, "citationCount": 2771, "influentialCitationCount": 431,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science",
      "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2016-10-30", "journal": {"pages": "2642-2651"},
      "authors": [{"authorId": "2624088", "name": "Augustus Odena"}, {"authorId":
      "37232298", "name": "C. Olah"}, {"authorId": "1789737", "name": "Jonathon Shlens"}]}},
      {"intents": [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId":
      "f10ac91e32f162ea2d73372ae1d8b5d61d0c3294", "externalIds": {"ArXiv": "1610.07797",
      "MAG": "2543937159", "DBLP": "conf/aistats/GidelJL17", "CorpusId": 3949784},
      "corpusId": 3949784, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
      "name": "International Conference on Artificial Intelligence and Statistics",
      "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
      Stat"]}, "url": "https://www.semanticscholar.org/paper/f10ac91e32f162ea2d73372ae1d8b5d61d0c3294",
      "title": "Frank-Wolfe Algorithms for Saddle Point Problems", "abstract": "We
      extend the Frank-Wolfe (FW) optimization algorithm to solve constrained smooth
      convex-concave saddle point (SP) problems. Remarkably, the method only requires
      access to linear minimization oracles. Leveraging recent advances in FW optimization,
      we provide the first proof of convergence of a FW-type saddle point solver over
      polytopes, thereby partially answering a 30 year-old conjecture. We also survey
      other convergence results and highlight gaps in the theoretical underpinnings
      of FW-style algorithms. Motivating applications without known efficient alternatives
      are explored through structured prediction with combinatorial penalties as well
      as games over matching polytopes involving an exponential number of constraints.",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "year": 2016, "referenceCount": 53, "citationCount": 60, "influentialCitationCount":
      6, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
      "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate":
      "2016-10-25", "journal": {"pages": "362-371"}, "authors": [{"authorId": "8150760",
      "name": "Gauthier Gidel"}, {"authorId": "1768120", "name": "T. Jebara"}, {"authorId":
      "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"intents": ["result"], "isInfluential":
      false, "contexts": ["\u2026on ImageNet, and better or equal quality on CIFAR-10
      and STL-10, compared to the previous training stabilization techniques that
      include weight clipping [4], gradient penalty [74, 43], batch normalization
      [26], weight normalization [63], layer normalization [5], and orthonormal regularization
      [7]."], "citedPaper": {"paperId": "1dcda6ac3fd33bf938574aca9542aec5e5cace26",
      "externalIds": {"MAG": "2964144352", "DBLP": "conf/iclr/BrockLRW17", "ArXiv":
      "1609.07093", "CorpusId": 13890001}, "corpusId": 13890001, "publicationVenue":
      {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
      on Learning Representations", "type": "conference", "alternate_names": ["Int
      Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/1dcda6ac3fd33bf938574aca9542aec5e5cace26",
      "title": "Neural Photo Editing with Introspective Adversarial Networks", "abstract":
      "The increasingly photorealistic sample quality of generative image models suggests
      their feasibility in applications beyond image generation. We present the Neural
      Photo Editor, an interface that leverages the power of generative neural networks
      to make large, semantically coherent changes to existing images. To tackle the
      challenge of achieving accurate reconstructions without loss of feature quality,
      we introduce the Introspective Adversarial Network, a novel hybridization of
      the VAE and GAN. Our model efficiently captures long-range dependencies through
      use of a computational block based on weight-shared dilated convolutions, and
      improves generalization performance with Orthogonal Regularization, a novel
      weight regularization method. We validate our contributions on CelebA, SVHN,
      and CIFAR-100, and produce samples and reconstructions with high visual fidelity.",
      "venue": "International Conference on Learning Representations", "year": 2016,
      "referenceCount": 41, "citationCount": 416, "influentialCitationCount": 20,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
      "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2016-09-22", "journal": {"volume": "abs/1609.07093", "name":
      "ArXiv"}, "authors": [{"authorId": "2065040247", "name": "Andrew Brock"}, {"authorId":
      "2067962067", "name": "Theodore Lim"}, {"authorId": "50361260", "name": "J.
      Ritchie"}, {"authorId": "32233090", "name": "Nick Weston"}]}}, {"intents": [],
      "isInfluential": false, "contexts": [], "citedPaper": {"paperId": "32c4e19f4a757f6c6984416b97d69e287d1d0ecd",
      "externalIds": {"ArXiv": "1609.05473", "DBLP": "conf/aaai/YuZWY17", "MAG": "2964268978",
      "DOI": "10.1609/aaai.v31i1.10804", "CorpusId": 3439214}, "corpusId": 3439214,
      "publicationVenue": {"id": "bdc2e585-4e48-4e36-8af1-6d859763d405", "name": "AAAI
      Conference on Artificial Intelligence", "type": "conference", "alternate_names":
      ["National Conference on Artificial Intelligence", "National Conf Artif Intell",
      "AAAI Conf Artif Intell", "AAAI"], "url": "http://www.aaai.org/"}, "url": "https://www.semanticscholar.org/paper/32c4e19f4a757f6c6984416b97d69e287d1d0ecd",
      "title": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient",
      "abstract": "\n \n As a new way of training generative models, Generative Adversarial
      Net (GAN) that uses a discriminative model to guide the training of the generative
      model has enjoyed considerable success in generating real-valued data. However,
      it has limitations when the goal is for generating sequences of discrete tokens.
      A major reason lies in that the discrete outputs from the generative model make
      it difficult to pass the gradient update from the discriminative model to the
      generative model. Also, the discriminative model can only assess a complete
      sequence, while for a partially generated sequence, it is non-trivial to balance
      its current score and the future one once the entire sequence has been generated.
      In this paper, we propose a sequence generation framework, called SeqGAN, to
      solve the problems. Modeling the data generator as a stochastic policy in reinforcement
      learning (RL), SeqGAN bypasses the generator differentiation problem by directly
      performing gradient policy update. The RL reward signal comes from the GAN discriminator
      judged on a complete sequence, and is passed back to the intermediate state-action
      steps using Monte Carlo search. Extensive experiments on synthetic data and
      real-world tasks demonstrate significant improvements over strong baselines.\n
      \n", "venue": "AAAI Conference on Artificial Intelligence", "year": 2016, "referenceCount":
      42, "citationCount": 2061, "influentialCitationCount": 275, "isOpenAccess":
      true, "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/10804/10663",
      "status": null}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2016-09-18",
      "journal": {"pages": "2852-2858"}, "authors": [{"authorId": "3469209", "name":
      "Lantao Yu"}, {"authorId": "2108309275", "name": "Weinan Zhang"}, {"authorId":
      "39055225", "name": "Jun Wang"}, {"authorId": "1811427", "name": "Yong Yu"}]}},
      {"intents": ["background"], "isInfluential": false, "contexts": ["Different
      GAN architectures were proposed for different tasks, such as image super-resolution
      [38] and image-to-image transfer [79, 53].", "It was proved successful in many
      domains such as computer vision [14, 29, 25, 38], semantic segmentation [39,
      27, 70, 24], time-series synthesis [9, 23], image editing [61, 36, 19, 3, 75],
      natural language processing [15, 28, 22], text-to-image generation [59, 58,
      54], and many more."], "citedPaper": {"paperId": "df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3",
      "externalIds": {"DBLP": "conf/cvpr/LedigTHCCAATTWS17", "MAG": "2963470893",
      "ArXiv": "1609.04802", "DOI": "10.1109/CVPR.2017.19", "CorpusId": 211227}, "corpusId":
      211227, "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf", "name":
      "Computer Vision and Pattern Recognition", "type": "conference", "alternate_names":
      ["CVPR", "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
      "url": "https://www.semanticscholar.org/paper/df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3",
      "title": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial
      Network", "abstract": "Despite the breakthroughs in accuracy and speed of single
      image super-resolution using faster and deeper convolutional neural networks,
      one central problem remains largely unsolved: how do we recover the finer texture
      details when we super-resolve at large upscaling factors? The behavior of optimization-based
      super-resolution methods is principally driven by the choice of the objective
      function. Recent work has largely focused on minimizing the mean squared reconstruction
      error. The resulting estimates have high peak signal-to-noise ratios, but they
      are often lacking high-frequency details and are perceptually unsatisfying in
      the sense that they fail to match the fidelity expected at the higher resolution.
      In this paper, we present SRGAN, a generative adversarial network (GAN) for
      image super-resolution (SR). To our knowledge, it is the first framework capable
      of inferring photo-realistic natural images for 4x upscaling factors. To achieve
      this, we propose a perceptual loss function which consists of an adversarial
      loss and a content loss. The adversarial loss pushes our solution to the natural
      image manifold using a discriminator network that is trained to differentiate
      between the super-resolved images and original photo-realistic images. In addition,
      we use a content loss motivated by perceptual similarity instead of similarity
      in pixel space. Our deep residual network is able to recover photo-realistic
      textures from heavily downsampled images on public benchmarks. An extensive
      mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality
      using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original
      high-resolution images than to those obtained with any state-of-the-art method.",
      "venue": "Computer Vision and Pattern Recognition", "year": 2016, "referenceCount":
      77, "citationCount": 8730, "influentialCitationCount": 1182, "isOpenAccess":
      true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1609.04802", "status":
      null}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2016-09-15",
      "journal": {"pages": "105-114", "name": "2017 IEEE Conference on Computer Vision
      and Pattern Recognition (CVPR)"}, "authors": [{"authorId": "1779917", "name":
      "C. Ledig"}, {"authorId": "2073063", "name": "Lucas Theis"}, {"authorId": "3108066",
      "name": "Ferenc Husz\u00e1r"}, {"authorId": "145372820", "name": "Jose Caballero"},
      {"authorId": "49931957", "name": "Andrew P. Aitken"}, {"authorId": "41203992",
      "name": "A. Tejani"}, {"authorId": "1853456", "name": "J. Totz"}, {"authorId":
      "34627233", "name": "Zehan Wang"}, {"authorId": "46810836", "name": "Wenzhe
      Shi"}]}}, {"intents": ["methodology"], "isInfluential": false, "contexts": ["This
      progressive architecture uses the idea of progressive neural networks \ufb01rst
      proposed by Rusu et al. [62]."], "citedPaper": {"paperId": "53c9443e4e667170acc60ca1b31a0ec7151fe753",
      "externalIds": {"ArXiv": "1606.04671", "DBLP": "journals/corr/RusuRDSKKPH16",
      "CorpusId": 15350923}, "corpusId": 15350923, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/53c9443e4e667170acc60ca1b31a0ec7151fe753",
      "title": "Progressive Neural Networks", "abstract": "Learning to solve complex
      sequences of tasks--while both leveraging transfer and avoiding catastrophic
      forgetting--remains a key obstacle to achieving human-level intelligence. The
      progressive networks approach represents a step forward in this direction: they
      are immune to forgetting and can leverage prior knowledge via lateral connections
      to previously learned features. We evaluate this architecture extensively on
      a wide variety of reinforcement learning tasks (Atari and 3D maze games), and
      show that it outperforms common baselines based on pretraining and finetuning.
      Using a novel sensitivity measure, we demonstrate that transfer occurs at both
      low-level sensory and high-level control layers of the learned policy.", "venue":
      "arXiv.org", "year": 2016, "referenceCount": 24, "citationCount": 1680, "influentialCitationCount":
      163, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2016-06-15", "journal": {"volume": "abs/1606.04671",
      "name": "ArXiv"}, "authors": [{"authorId": "2228824", "name": "Andrei A. Rusu"},
      {"authorId": "3422052", "name": "Neil C. Rabinowitz"}, {"authorId": "2755582",
      "name": "Guillaume Desjardins"}, {"authorId": "2794457", "name": "Hubert Soyer"},
      {"authorId": "2066516991", "name": "J. Kirkpatrick"}, {"authorId": "2645384",
      "name": "K. Kavukcuoglu"}, {"authorId": "1996134", "name": "Razvan Pascanu"},
      {"authorId": "2315504", "name": "R. Hadsell"}]}}, {"intents": [], "isInfluential":
      false, "contexts": [], "citedPaper": {"paperId": "eb7ee0bc355652654990bcf9f92f124688fde493",
      "externalIds": {"MAG": "2963226019", "ArXiv": "1606.03657", "DBLP": "journals/corr/ChenDHSSA16",
      "CorpusId": 5002792}, "corpusId": 5002792, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/eb7ee0bc355652654990bcf9f92f124688fde493",
      "title": "InfoGAN: Interpretable Representation Learning by Information Maximizing
      Generative Adversarial Nets", "abstract": "This paper describes InfoGAN, an
      information-theoretic extension to the Generative Adversarial Network that is
      able to learn disentangled representations in a completely unsupervised manner.
      InfoGAN is a generative adversarial network that also maximizes the mutual information
      between a small subset of the latent variables and the observation. We derive
      a lower bound to the mutual information objective that can be optimized efficiently,
      and show that our training procedure can be interpreted as a variation of the
      Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing
      styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered
      images, and background digits from the central digit on the SVHN dataset. It
      also discovers visual concepts that include hair styles, presence/absence of
      eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN
      learns interpretable representations that are competitive with representations
      learned by existing fully supervised methods.", "venue": "Neural Information
      Processing Systems", "year": 2016, "referenceCount": 42, "citationCount": 3776,
      "influentialCitationCount": 433, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2016-06-12", "journal":
      {"pages": "2172-2180"}, "authors": [{"authorId": "41192764", "name": "Xi Chen"},
      {"authorId": "144581158", "name": "Yan Duan"}, {"authorId": "3127100", "name":
      "Rein Houthooft"}, {"authorId": "47971768", "name": "J. Schulman"}, {"authorId":
      "1701686", "name": "Ilya Sutskever"}, {"authorId": "1689992", "name": "P. Abbeel"}]}},
      {"intents": ["background"], "isInfluential": false, "contexts": ["SGAN [49]
      extended the original GAN learning to the semi-supervised context by adding
      to the discriminator network an additional task of classifying the image labels
      (Fig."], "citedPaper": {"paperId": "a9d83b30c3e615286d3e24b6a2e2228872b39bc8",
      "externalIds": {"MAG": "2412510955", "DBLP": "journals/corr/Odena16a", "ArXiv":
      "1606.01583", "CorpusId": 16173204}, "corpusId": 16173204, "publicationVenue":
      {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
      ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/a9d83b30c3e615286d3e24b6a2e2228872b39bc8",
      "title": "Semi-Supervised Learning with Generative Adversarial Networks", "abstract":
      "We extend Generative Adversarial Networks (GANs) to the semi-supervised context
      by forcing the discriminator network to output class labels. We train a generative
      model G and a discriminator D on a dataset with inputs belonging to one of N
      classes. At training time, D is made to predict which of N+1 classes the input
      belongs to, where an extra class is added to correspond to the outputs of G.
      We show that this method can be used to create a more data-efficient classifier
      and that it allows for generating higher quality samples than a regular GAN.",
      "venue": "arXiv.org", "year": 2016, "referenceCount": 14, "citationCount": 611,
      "influentialCitationCount": 67, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category":
      "Mathematics", "source": "external"}, {"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category":
      "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2016-06-05", "journal": {"volume": "abs/1606.01583", "name":
      "ArXiv"}, "authors": [{"authorId": "2624088", "name": "Augustus Odena"}]}},
      {"intents": [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId":
      "b7624bfd4099b9e0d32875c7480ca04a3d8deb03", "externalIds": {"MAG": "2416555906",
      "DBLP": "journals/corr/OsokinALDL16", "ArXiv": "1605.09346", "CorpusId": 8085785},
      "corpusId": 8085785, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/b7624bfd4099b9e0d32875c7480ca04a3d8deb03",
      "title": "Minding the Gaps for Block Frank-Wolfe Optimization of Structured
      SVMs", "abstract": "In this paper, we propose several improvements on the block-coordinate
      Frank-Wolfe (BCFW) algorithm from Lacoste-Julien et al. (2013) recently used
      to optimize the structured support vector machine (SSVM) objective in the context
      of structured prediction, though it has wider applications. The key intuition
      behind our improvements is that the estimates of block gaps maintained by BCFW
      reveal the block suboptimality that can be used as an adaptive criterion. First,
      we sample objects at each iteration of BCFW in an adaptive non-uniform way via
      gapbased sampling. Second, we incorporate pairwise and away-step variants of
      Frank-Wolfe into the block-coordinate setting. Third, we cache oracle calls
      with a cache-hit criterion based on the block gaps. Fourth, we provide the first
      method to compute an approximate regularization path for SSVM. Finally, we provide
      an exhaustive empirical evaluation of all our methods on four structured prediction
      datasets.", "venue": "International Conference on Machine Learning", "year":
      2016, "referenceCount": 59, "citationCount": 62, "influentialCitationCount":
      6, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2016-05-30", "journal": {"volume": "abs/1605.09346", "name": "ArXiv"}, "authors":
      [{"authorId": "145319877", "name": "A. Osokin"}, {"authorId": "2285263", "name":
      "Jean-Baptiste Alayrac"}, {"authorId": "1419451349", "name": "Isabella Lukasewitz"},
      {"authorId": "144679302", "name": "P. Dokania"}, {"authorId": "1388317459",
      "name": "Simon Lacoste-Julien"}]}}, {"intents": [], "isInfluential": false,
      "contexts": [], "citedPaper": {"paperId": "af65eadf393d3470281a3838ec81f29a35777773",
      "externalIds": {"MAG": "2963663068", "ArXiv": "1605.08636", "DBLP": "journals/corr/GermainBLL16",
      "CorpusId": 930133}, "corpusId": 930133, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/af65eadf393d3470281a3838ec81f29a35777773",
      "title": "PAC-Bayesian Theory Meets Bayesian Inference", "abstract": "We exhibit
      a strong link between frequentist PAC-Bayesian risk bounds and the Bayesian
      marginal likelihood. That is, for the negative log-likelihood loss function,
      we show that the minimization of PAC-Bayesian generalization risk bounds maximizes
      the Bayesian marginal likelihood. This provides an alternative explanation to
      the Bayesian Occam''s razor criteria, under the assumption that the data is
      generated by an i.i.d distribution. Moreover, as the negative log-likelihood
      is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem
      tailored for the sub-gamma loss family, and we show that our approach is sound
      on classical Bayesian linear regression tasks.", "venue": "Neural Information
      Processing Systems", "year": 2016, "referenceCount": 51, "citationCount": 164,
      "influentialCitationCount": 35, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category":
      "Mathematics", "source": "external"}, {"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category":
      "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2016-05-27", "journal": {"volume": "abs/1605.08636",
      "name": "ArXiv"}, "authors": [{"authorId": "31580144", "name": "Pascal Germain"},
      {"authorId": "144570279", "name": "F. Bach"}, {"authorId": "8651990", "name":
      "Alexandre Lacoste"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}},
      {"intents": [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId":
      "6c7f040a150abf21dbcefe1f22e0f98fa184f41a", "externalIds": {"DBLP": "journals/corr/ReedAYLSL16",
      "ArXiv": "1605.05396", "MAG": "2949999304", "CorpusId": 1563370}, "corpusId":
      1563370, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/6c7f040a150abf21dbcefe1f22e0f98fa184f41a",
      "title": "Generative Adversarial Text to Image Synthesis", "abstract": "Automatic
      synthesis of realistic images from text would be interesting and useful, but
      current AI systems are still far from this goal. However, in recent years generic
      and powerful recurrent neural network architectures have been developed to learn
      discriminative text feature representations. Meanwhile, deep convolutional generative
      adversarial networks (GANs) have begun to generate highly compelling images
      of specific categories, such as faces, album covers, and room interiors. In
      this work, we develop a novel deep architecture and GAN formulation to effectively
      bridge these advances in text and image modeling, translating visual concepts
      from characters to pixels. We demonstrate the capability of our model to generate
      plausible images of birds and flowers from detailed text descriptions.", "venue":
      "International Conference on Machine Learning", "year": 2016, "referenceCount":
      42, "citationCount": 2723, "influentialCitationCount": 219, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2016-05-17", "journal": {"pages": "1060-1069"},
      "authors": [{"authorId": "144828948", "name": "Scott E. Reed"}, {"authorId":
      "2893664", "name": "Zeynep Akata"}, {"authorId": "3084614", "name": "Xinchen
      Yan"}, {"authorId": "2876316", "name": "Lajanugen Logeswaran"}, {"authorId":
      "48920094", "name": "B. Schiele"}, {"authorId": "1697141", "name": "Honglak
      Lee"}]}}, {"intents": [], "isInfluential": false, "contexts": [], "citedPaper":
      {"paperId": "f19284f6ab802c8a1fcde076fcb3fba195a71723", "externalIds": {"DBLP":
      "journals/corr/DumoulinV16", "ArXiv": "1603.07285", "MAG": "2304648132", "CorpusId":
      6662846}, "corpusId": 6662846, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/f19284f6ab802c8a1fcde076fcb3fba195a71723",
      "title": "A guide to convolution arithmetic for deep learning", "abstract":
      "We introduce a guide to help deep learning practitioners understand and manipulate
      convolutional neural network architectures. The guide clarifies the relationship
      between various properties (input shape, kernel shape, zero padding, strides
      and output shape) of convolutional, pooling and transposed convolutional layers,
      as well as the relationship between convolutional and transposed convolutional
      layers. Relationships are derived for various cases, and are illustrated in
      order to make them intuitive.", "venue": "arXiv.org", "year": 2016, "referenceCount":
      24, "citationCount": 1354, "influentialCitationCount": 70, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2016-03-23", "journal":
      {"volume": "abs/1603.07285", "name": "ArXiv"}, "authors": [{"authorId": "3074927",
      "name": "Vincent Dumoulin"}, {"authorId": "2077146", "name": "Francesco Visin"}]}},
      {"intents": [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId":
      "d175093098decfb26d30f3d74e6cfadff16a7b8b", "externalIds": {"MAG": "2950455576",
      "DBLP": "conf/icml/PodosinnikovaBL16", "ArXiv": "1602.09013", "CorpusId": 11886704},
      "corpusId": 11886704, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/d175093098decfb26d30f3d74e6cfadff16a7b8b",
      "title": "Beyond CCA: Moment Matching for Multi-View Models", "abstract": "We
      introduce three novel semi-parametric extensions of probabilistic canonical
      correlation analysis with identifiability guarantees. We consider moment matching
      techniques for estimation in these models. For that, by drawing explicit links
      between the new models and a discrete version of independent component analysis
      (DICA), we first extend the DICA cumulant tensors to the new discrete version
      of CCA. By further using a close connection with independent component analysis,
      we introduce generalized covariance matrices, which can replace the cumulant
      tensors in the moment matching framework, and, therefore, improve sample complexity
      and simplify derivations and algorithms significantly. As the tensor power method
      or orthogonal joint diagonalization are not applicable in the new setting, we
      use non-orthogonal joint diagonalization techniques for matching the cumulants.
      We demonstrate performance of the proposed models and estimation techniques
      on experiments with both synthetic and real datasets.", "venue": "International
      Conference on Machine Learning", "year": 2016, "referenceCount": 65, "citationCount":
      16, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category":
      "Mathematics", "source": "external"}, {"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category":
      "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2016-02-29", "journal": {"volume": "abs/1602.09013",
      "name": "ArXiv"}, "authors": [{"authorId": "3343377", "name": "A. Podosinnikova"},
      {"authorId": "144570279", "name": "F. Bach"}, {"authorId": "1388317459", "name":
      "Simon Lacoste-Julien"}]}}, {"intents": ["result"], "isInfluential": false,
      "contexts": ["\u2026on ImageNet, and better or equal quality on CIFAR-10 and
      STL-10, compared to the previous training stabilization techniques that include
      weight clipping [4], gradient penalty [74, 43], batch normalization [26], weight
      normalization [63], layer normalization [5], and orthonormal regularization
      [7]."], "citedPaper": {"paperId": "3d2c6941a9b4608ba52b328369a3352db2092ae0",
      "externalIds": {"MAG": "2284050935", "DBLP": "conf/nips/SalimansK16", "ArXiv":
      "1602.07868", "CorpusId": 151231}, "corpusId": 151231, "publicationVenue": {"id":
      "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/3d2c6941a9b4608ba52b328369a3352db2092ae0",
      "title": "Weight Normalization: A Simple Reparameterization to Accelerate Training
      of Deep Neural Networks", "abstract": "We present weight normalization: a reparameterization
      of the weight vectors in a neural network that decouples the length of those
      weight vectors from their direction. By reparameterizing the weights in this
      way we improve the conditioning of the optimization problem and we speed up
      convergence of stochastic gradient descent. Our reparameterization is inspired
      by batch normalization but does not introduce any dependencies between the examples
      in a minibatch. This means that our method can also be applied successfully
      to recurrent models such as LSTMs and to noise-sensitive applications such as
      deep reinforcement learning or generative models, for which batch normalization
      is less well suited. Although our method is much simpler, it still provides
      much of the speed-up of full batch normalization. In addition, the computational
      overhead of our method is lower, permitting more optimization steps to be taken
      in the same amount of time. We demonstrate the usefulness of our method on applications
      in supervised image recognition, generative modelling, and deep reinforcement
      learning.", "venue": "Neural Information Processing Systems", "year": 2016,
      "referenceCount": 34, "citationCount": 1631, "influentialCitationCount": 120,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
      "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2016-02-25", "journal": {"volume": "abs/1602.07868",
      "name": "ArXiv"}, "authors": [{"authorId": "2887364", "name": "Tim Salimans"},
      {"authorId": "1726807", "name": "Diederik P. Kingma"}]}}, {"intents": ["methodology",
      "background"], "isInfluential": false, "contexts": ["Figure 6 compares original
      HR images with the SR images generated by bicubic, SRResNet[6] and proposed
      SRGAN, respectively, which shows that the GAN framework outperforms the other
      two methods significantly.", "introduced the first GAN framework that leverages
      the very deep ResNet neural network [6] as a generator and discriminator for
      the SR tasks [7]."], "citedPaper": {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
      "externalIds": {"DBLP": "conf/cvpr/HeZRS16", "MAG": "2949650786", "ArXiv": "1512.03385",
      "DOI": "10.1109/cvpr.2016.90", "CorpusId": 206594692}, "corpusId": 206594692,
      "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf", "name": "Computer
      Vision and Pattern Recognition", "type": "conference", "alternate_names": ["CVPR",
      "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
      "url": "https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d",
      "title": "Deep Residual Learning for Image Recognition", "abstract": "Deeper
      neural networks are more difficult to train. We present a residual learning
      framework to ease the training of networks that are substantially deeper than
      those used previously. We explicitly reformulate the layers as learning residual
      functions with reference to the layer inputs, instead of learning unreferenced
      functions. We provide comprehensive empirical evidence showing that these residual
      networks are easier to optimize, and can gain accuracy from considerably increased
      depth. On the ImageNet dataset we evaluate residual nets with a depth of up
      to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity.
      An ensemble of these residual nets achieves 3.57% error on the ImageNet test
      set. This result won the 1st place on the ILSVRC 2015 classification task. We
      also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations
      is of central importance for many visual recognition tasks. Solely due to our
      extremely deep representations, we obtain a 28% relative improvement on the
      COCO object detection dataset. Deep residual nets are foundations of our submissions
      to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the
      tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO
      segmentation.", "venue": "Computer Vision and Pattern Recognition", "year":
      2015, "referenceCount": 54, "citationCount": 147019, "influentialCitationCount":
      30476, "isOpenAccess": true, "openAccessPdf": {"url": "https://repositorio.unal.edu.co/bitstream/unal/81443/1/98670607.2022.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2015-12-10", "journal": {"pages": "770-778", "name": "2016
      IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"}, "authors":
      [{"authorId": "39353098", "name": "Kaiming He"}, {"authorId": "1771551", "name":
      "X. Zhang"}, {"authorId": "3080683", "name": "Shaoqing Ren"}, {"authorId": null,
      "name": "Jian Sun"}]}}, {"intents": ["methodology", "background"], "isInfluential":
      false, "contexts": ["DCGAN proposed to create images using solely deconvolutional
      networks in their generator [57].", "To show that they measured GANs quality
      by considering them as feature extractors on supervised datasets, and evaluating
      the performance of linear models trained on these features (for more information
      see [57]).", "Many types of new GAN architectures have been proposed since 2014
      ([6, 8, 13, 29, 57, 77, 30, 16])."], "citedPaper": {"paperId": "8388f1be26329fa45e5807e968a641ce170ea078",
      "externalIds": {"MAG": "2949811265", "ArXiv": "1511.06434", "DBLP": "journals/corr/RadfordMC15",
      "CorpusId": 11758569}, "corpusId": 11758569, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/8388f1be26329fa45e5807e968a641ce170ea078",
      "title": "Unsupervised Representation Learning with Deep Convolutional Generative
      Adversarial Networks", "abstract": "In recent years, supervised learning with
      convolutional networks (CNNs) has seen huge adoption in computer vision applications.
      Comparatively, unsupervised learning with CNNs has received less attention.
      In this work we hope to help bridge the gap between the success of CNNs for
      supervised learning and unsupervised learning. We introduce a class of CNNs
      called deep convolutional generative adversarial networks (DCGANs), that have
      certain architectural constraints, and demonstrate that they are a strong candidate
      for unsupervised learning. Training on various image datasets, we show convincing
      evidence that our deep convolutional adversarial pair learns a hierarchy of
      representations from object parts to scenes in both the generator and discriminator.
      Additionally, we use the learned features for novel tasks - demonstrating their
      applicability as general image representations.", "venue": "International Conference
      on Learning Representations", "year": 2015, "referenceCount": 46, "citationCount":
      12362, "influentialCitationCount": 1827, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy":
      [{"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2015-11-19", "journal":
      {"volume": "abs/1511.06434", "name": "CoRR"}, "authors": [{"authorId": "38909097",
      "name": "Alec Radford"}, {"authorId": "2096458", "name": "Luke Metz"}, {"authorId":
      "2127604", "name": "Soumith Chintala"}]}}, {"intents": [], "isInfluential":
      false, "contexts": [], "citedPaper": {"paperId": "8b56449fa9aeadcf86181697e9ec2269419d8817",
      "externalIds": {"DBLP": "conf/nips/Lacoste-JulienJ15", "MAG": "2949435166",
      "ArXiv": "1511.05932", "CorpusId": 14606229}, "corpusId": 14606229, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/8b56449fa9aeadcf86181697e9ec2269419d8817",
      "title": "On the Global Linear Convergence of Frank-Wolfe Optimization Variants",
      "abstract": "The Frank-Wolfe (FW) optimization algorithm has lately re-gained
      popularity thanks in particular to its ability to nicely handle the structured
      constraints appearing in machine learning applications. However, its convergence
      rate is known to be slow (sublinear) when the solution lies at the boundary.
      A simple less-known fix is to add the possibility to take ''away steps'' during
      optimization, an operation that importantly does not require a feasibility oracle.
      In this paper, we highlight and clarify several variants of the Frank-Wolfe
      optimization algorithm that have been successfully applied in practice: away-steps
      FW, pairwise FW, fully-corrective FW and Wolfe''s minimum norm point algorithm,
      and prove for the first time that they all enjoy global linear convergence,
      under a weaker condition than strong convexity of the objective. The constant
      in the convergence rate has an elegant interpretation as the product of the
      (classical) condition number of the function with a novel geometric quantity
      that plays the role of a ''condition number'' of the constraint set. We provide
      pointers to where these algorithms have made a difference in practice, in particular
      with the flow polytope, the marginal polytope and the base polytope for submodular
      optimization.", "venue": "Neural Information Processing Systems", "year": 2015,
      "referenceCount": 39, "citationCount": 380, "influentialCitationCount": 85,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science",
      "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2015-11-18", "journal": {"pages": "496-504"}, "authors": [{"authorId": "1388317459",
      "name": "Simon Lacoste-Julien"}, {"authorId": "2456863", "name": "Martin Jaggi"}]}},
      {"intents": [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId":
      "ccf9b939330f73d7f87d3d606349c3352bf2713d", "externalIds": {"DBLP": "conf/nips/KrishnanLS15",
      "MAG": "2964168821", "ArXiv": "1511.02124", "CorpusId": 6403759}, "corpusId":
      6403759, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/ccf9b939330f73d7f87d3d606349c3352bf2713d",
      "title": "Barrier Frank-Wolfe for Marginal Inference", "abstract": "We introduce
      a globally-convergent algorithm for optimizing the tree-reweighted (TRW) variational
      objective over the marginal polytope. The algorithm is based on the conditional
      gradient method (Frank-Wolfe) and moves pseudomarginals within the marginal
      polytope through repeated maximum a posteriori (MAP) calls. This modular structure
      enables us to leverage black-box MAP solvers (both exact and approximate) for
      variational inference, and obtains more accurate results than tree-reweighted
      algorithms that optimize over the local consistency relaxation. Theoretically,
      we bound the sub-optimality for the proposed algorithm despite the TRW objective
      having unbounded gradients at the boundary of the marginal polytope. Empirically,
      we demonstrate the increased quality of results found by tightening the relaxation
      over the marginal polytope as well as the spanning tree polytope on synthetic
      and real-world instances.", "venue": "Neural Information Processing Systems",
      "year": 2015, "referenceCount": 34, "citationCount": 35, "influentialCitationCount":
      3, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2015-11-06", "journal": {"pages": "532-540"}, "authors": [{"authorId": "145253891",
      "name": "R. G. Krishnan"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"},
      {"authorId": "1746662", "name": "D. Sontag"}]}}, {"intents": [], "isInfluential":
      false, "contexts": [], "citedPaper": {"paperId": "2d9f970000a4309f32d37afcf7833c213e530810",
      "externalIds": {"DBLP": "conf/nips/PodosinnikovaBL15", "MAG": "1783362711",
      "ArXiv": "1507.01784", "CorpusId": 10035651}, "corpusId": 10035651, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/2d9f970000a4309f32d37afcf7833c213e530810",
      "title": "Rethinking LDA: Moment Matching for Discrete ICA", "abstract": "We
      consider moment matching techniques for estimation in latent Dirichlet allocation
      (LDA). By drawing explicit links between LDA and discrete versions of independent
      component analysis (ICA), we first derive a new set of cumulant-based tensors,
      with an improved sample complexity. Moreover, we reuse standard ICA techniques
      such as joint diagonalization of tensors to improve over existing methods based
      on the tensor power method. In an extensive set of experiments on both synthetic
      and real datasets, we show that our new combination of tensors and orthogonal
      joint diagonalization techniques outperforms existing moment matching methods.",
      "venue": "Neural Information Processing Systems", "year": 2015, "referenceCount":
      36, "citationCount": 24, "influentialCitationCount": 5, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
      "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2015-07-07",
      "journal": {"volume": "abs/1507.01784", "name": "ArXiv"}, "authors": [{"authorId":
      "3343377", "name": "A. Podosinnikova"}, {"authorId": "144570279", "name": "F.
      Bach"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"intents":
      [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId": "35e3fa06068a39996a7185a4b74f332ac8196566",
      "externalIds": {"MAG": "2963507802", "DBLP": "journals/corr/AlayracBASLL15",
      "DOI": "10.1109/TPAMI.2017.2749223", "CorpusId": 215811147, "PubMed": "28885149"},
      "corpusId": 215811147, "publicationVenue": {"id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
      "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "type":
      "journal", "alternate_names": ["IEEE Trans Pattern Anal Mach Intell"], "issn":
      "0162-8828", "url": "http://www.computer.org/tpami/", "alternate_urls": ["http://www.computer.org/portal/web/tpami",
      "http://ieeexplore.ieee.org/servlet/opac?punumber=34"]}, "url": "https://www.semanticscholar.org/paper/35e3fa06068a39996a7185a4b74f332ac8196566",
      "title": "Learning from Narrated Instruction Videos", "abstract": "Automatic
      assistants could guide a person or a robot in performing new tasks, such as
      changing a car tire or repotting a plant. Creating such assistants, however,
      is non-trivial and requires understanding of visual and verbal content of a
      video. Towards this goal, we here address the problem of automatically learning
      the main steps of a task from a set of narrated instruction videos. We develop
      a new unsupervised learning approach that takes advantage of the complementary
      nature of the input video and the associated narration. The method sequentially
      clusters textual and visual representations of a task, where the two clustering
      problems are linked by joint constraints to obtain a single coherent sequence
      of steps in both modalities. To evaluate our method, we collect and annotate
      a new challenging dataset of real-world instruction videos from the Internet.
      The dataset contains videos for five different tasks with complex interactions
      between people and objects, captured in a variety of indoor and outdoor settings.
      We experimentally demonstrate that the proposed method can automatically discover,
      learn and localize the main steps of a task in input videos.", "venue": "IEEE
      Transactions on Pattern Analysis and Machine Intelligence", "year": 2015, "referenceCount":
      44, "citationCount": 21, "influentialCitationCount": 1, "isOpenAccess": true,
      "openAccessPdf": {"url": "https://hal.archives-ouvertes.fr/hal-01580630/file/pami2016alayrac.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Medicine",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2015-06-30", "journal":
      {"volume": "40", "pages": "2194-2208", "name": "IEEE Transactions on Pattern
      Analysis and Machine Intelligence"}, "authors": [{"authorId": "2285263", "name":
      "Jean-Baptiste Alayrac"}, {"authorId": "2329288", "name": "Piotr Bojanowski"},
      {"authorId": "143688116", "name": "Nishant Agrawal"}, {"authorId": "1782755",
      "name": "Josef Sivic"}, {"authorId": "143991676", "name": "I. Laptev"}, {"authorId":
      "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"intents": [], "isInfluential":
      false, "contexts": [], "citedPaper": {"paperId": "e17ba2b5d0769e7f2602d859ea77a153846cf27d",
      "externalIds": {"MAG": "2185243164", "ArXiv": "1506.09215", "DBLP": "conf/cvpr/AlayracBASLL16",
      "DOI": "10.1109/CVPR.2016.495", "CorpusId": 2617244}, "corpusId": 2617244, "publicationVenue":
      {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf", "name": "Computer Vision and
      Pattern Recognition", "type": "conference", "alternate_names": ["CVPR", "Comput
      Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
      "url": "https://www.semanticscholar.org/paper/e17ba2b5d0769e7f2602d859ea77a153846cf27d",
      "title": "Unsupervised Learning from Narrated Instruction Videos", "abstract":
      "We address the problem of automatically learning the main steps to complete
      a certain task, such as changing a car tire, from a set of narrated instruction
      videos. The contributions of this paper are three-fold. First, we develop a
      new unsupervised learning approach that takes advantage of the complementary
      nature of the input video and the associated narration. The method solves two
      clustering problems, one in text and one in video, applied one after each other
      and linked by joint constraints to obtain a single coherent sequence of steps
      in both modalities. Second, we collect and annotate a new challenging dataset
      of real-world instruction videos from the Internet. The dataset contains about
      800,000 frames for five different tasks1 that include complex interactions between
      people and objects, and are captured in a variety of indoor and outdoor settings.
      Third, we experimentally demonstrate that the proposed method can automatically
      discover, in an unsupervised manner, the main steps to achieve the task and
      locate the steps in the input videos.", "venue": "Computer Vision and Pattern
      Recognition", "year": 2015, "referenceCount": 36, "citationCount": 256, "influentialCitationCount":
      37, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1506.09215",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2015-06-30", "journal": {"pages": "4575-4583", "name": "2016
      IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"}, "authors":
      [{"authorId": "2285263", "name": "Jean-Baptiste Alayrac"}, {"authorId": "2329288",
      "name": "Piotr Bojanowski"}, {"authorId": "143688116", "name": "Nishant Agrawal"},
      {"authorId": "1782755", "name": "Josef Sivic"}, {"authorId": "143991676", "name":
      "I. Laptev"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}]}},
      {"intents": ["background"], "isInfluential": false, "contexts": ["Many types
      of new GAN architectures have been proposed since 2014 ([6, 8, 13, 29, 57, 77,
      30, 16])."], "citedPaper": {"paperId": "47900aca2f0b50da3010ad59b394c870f0e6c02e",
      "externalIds": {"MAG": "648143168", "DBLP": "conf/nips/DentonCSF15", "ArXiv":
      "1506.05751", "CorpusId": 1282515}, "corpusId": 1282515, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/47900aca2f0b50da3010ad59b394c870f0e6c02e",
      "title": "Deep Generative Image Models using a Laplacian Pyramid of Adversarial
      Networks", "abstract": "In this paper we introduce a generative parametric model
      capable of producing high quality samples of natural images. Our approach uses
      a cascade of convolutional networks within a Laplacian pyramid framework to
      generate images in a coarse-to-fine fashion. At each level of the pyramid, a
      separate generative convnet model is trained using the Generative Adversarial
      Nets (GAN) approach [11]. Samples drawn from our model are of significantly
      higher quality than alternate approaches. In a quantitative assessment by human
      evaluators, our CIFAR10 samples were mistaken for real images around 40% of
      the time, compared to 10% for samples drawn from a GAN baseline model. We also
      show samples from models trained on the higher resolution images of the LSUN
      scene dataset.", "venue": "Neural Information Processing Systems", "year": 2015,
      "referenceCount": 36, "citationCount": 2119, "influentialCitationCount": 108,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2015-06-18", "journal":
      {"volume": "abs/1506.05751", "name": "ArXiv"}, "authors": [{"authorId": "40081727",
      "name": "Emily L. Denton"}, {"authorId": "2127604", "name": "Soumith Chintala"},
      {"authorId": "3149531", "name": "Arthur Szlam"}, {"authorId": "2276554", "name":
      "R. Fergus"}]}}, {"intents": [], "isInfluential": false, "contexts": [], "citedPaper":
      {"paperId": "1291c0b301f77f6f24ac654689a45f2df34ddbfb", "externalIds": {"MAG":
      "2136667596", "DBLP": "conf/nips/HofmannLLM15", "ArXiv": "1506.03662", "CorpusId":
      8779016}, "corpusId": 8779016, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/1291c0b301f77f6f24ac654689a45f2df34ddbfb",
      "title": "Variance Reduced Stochastic Gradient Descent with Neighbors", "abstract":
      "Stochastic Gradient Descent (SGD) is a workhorse in machine learning, yet its
      slow convergence can be a computational bottleneck. Variance reduction techniques
      such as SAG, SVRG and SAGA have been proposed to overcome this weakness, achieving
      linear convergence. However, these methods are either based on computations
      of full gradients at pivot points, or on keeping per data point corrections
      in memory. Therefore speed-ups relative to SGD may need a minimal number of
      epochs in order to materialize. This paper investigates algorithms that can
      exploit neighborhood structure in the training data to share and re-use information
      about past stochastic gradients across data points, which offers advantages
      in the transient optimization phase. As a side-product we provide a unified
      convergence analysis for a family of variance reduction algorithms, which we
      call memorization algorithms. We provide experimental results supporting our
      theory.", "venue": "Neural Information Processing Systems", "year": 2015, "referenceCount":
      13, "citationCount": 141, "influentialCitationCount": 31, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2015-06-11",
      "journal": {"pages": "2305-2313"}, "authors": [{"authorId": "153379696", "name":
      "T. Hofmann"}, {"authorId": "40401747", "name": "Aur\u00e9lien Lucchi"}, {"authorId":
      "1388317459", "name": "Simon Lacoste-Julien"}, {"authorId": "2953855", "name":
      "B. McWilliams"}]}}, {"intents": ["background"], "isInfluential": false, "contexts":
      ["It was proved successful in many domains such as computer vision [14, 29,
      25, 38], semantic segmentation [39, 27, 70, 24], time-series synthesis [9, 23],
      image editing [61, 36, 19, 3, 75], natural language processing [15, 28, 22],
      text-to-image generation [59, 58, 54], and many more."], "citedPaper": {"paperId":
      "4e2f6b4bc889eed1afe5833d5190f6f02e501061", "externalIds": {"MAG": "930928758",
      "ArXiv": "1505.03906", "DBLP": "conf/uai/DziugaiteRG15", "CorpusId": 9127770},
      "corpusId": 9127770, "publicationVenue": {"id": "f9af8000-42f8-410d-a622-e8811e41660a",
      "name": "Conference on Uncertainty in Artificial Intelligence", "type": "conference",
      "alternate_names": ["Uncertainty in Artificial Intelligence", "UAI", "Conf Uncertain
      Artif Intell", "Uncertain Artif Intell"], "url": "http://www.auai.org/"}, "url":
      "https://www.semanticscholar.org/paper/4e2f6b4bc889eed1afe5833d5190f6f02e501061",
      "title": "Training generative neural networks via Maximum Mean Discrepancy optimization",
      "abstract": "We consider training a deep neural network to generate samples
      from an unknown distribution given i.i.d. data. We frame learning as an optimization
      minimizing a two-sample test statistic\u2014informally speaking, a good generator
      network produces samples that cause a two-sample test to fail to reject the
      null hypothesis. As our two-sample test statistic, we use an unbiased estimate
      of the maximum mean discrepancy, which is the centerpiece of the nonparametric
      kernel two-sample test proposed by Gretton et al. [2]. We compare to the adversarial
      nets framework introduced by Goodfellow et al. [1], in which learning is a two-player
      game between a generator network and an adversarial discriminator network, both
      trained to outwit the other. From this perspective, the MMD statistic plays
      the role of the discriminator. In addition to empirical comparisons, we prove
      bounds on the generalization error incurred by optimizing the empirical MMD.",
      "venue": "Conference on Uncertainty in Artificial Intelligence", "year": 2015,
      "referenceCount": 15, "citationCount": 461, "influentialCitationCount": 61,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
      "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2015-05-14", "journal": {"pages": "258-267"}, "authors": [{"authorId": "2533850",
      "name": "G. Dziugaite"}, {"authorId": "39331522", "name": "Daniel M. Roy"},
      {"authorId": "1744700", "name": "Zoubin Ghahramani"}]}}, {"intents": ["result"],
      "isInfluential": false, "contexts": ["\u2026on ImageNet, and better or equal
      quality on CIFAR-10 and STL-10, compared to the previous training stabilization
      techniques that include weight clipping [4], gradient penalty [74, 43], batch
      normalization [26], weight normalization [63], layer normalization [5], and
      orthonormal regularization [7]."], "citedPaper": {"paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
      "externalIds": {"ArXiv": "1502.03167", "DBLP": "conf/icml/IoffeS15", "MAG":
      "1836465849", "CorpusId": 5808102}, "corpusId": 5808102, "publicationVenue":
      {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
      on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
      Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/995c5f5e62614fcb4d2796ad2faab969da51713e",
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing
      Internal Covariate Shift", "abstract": "Training Deep Neural Networks is complicated
      by the fact that the distribution of each layer''s inputs changes during training,
      as the parameters of the previous layers change. This slows down the training
      by requiring lower learning rates and careful parameter initialization, and
      makes it notoriously hard to train models with saturating nonlinearities. We
      refer to this phenomenon as internal covariate shift, and address the problem
      by normalizing layer inputs. Our method draws its strength from making normalization
      a part of the model architecture and performing the normalization for each training
      mini-batch. Batch Normalization allows us to use much higher learning rates
      and be less careful about initialization, and in some cases eliminates the need
      for Dropout. Applied to a state-of-the-art image classification model, Batch
      Normalization achieves the same accuracy with 14 times fewer training steps,
      and beats the original model by a significant margin. Using an ensemble of batch-normalized
      networks, we improve upon the best published result on ImageNet classification:
      reaching 4.82% top-5 test error, exceeding the accuracy of human raters.", "venue":
      "International Conference on Machine Learning", "year": 2015, "referenceCount":
      34, "citationCount": 37936, "influentialCitationCount": 1982, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2015-02-10", "journal": {"volume": "abs/1502.03167",
      "name": "ArXiv"}, "authors": [{"authorId": "2054165706", "name": "Sergey Ioffe"},
      {"authorId": "2574060", "name": "Christian Szegedy"}]}}, {"intents": [], "isInfluential":
      false, "contexts": [], "citedPaper": {"paperId": "3dba006990a306420dcde24bdd48420def1c7e17",
      "externalIds": {"ArXiv": "1501.02056", "DBLP": "conf/aistats/Lacoste-JulienL15",
      "MAG": "2962808554", "CorpusId": 215825724}, "corpusId": 215825724, "publicationVenue":
      {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e", "name": "International Conference
      on Artificial Intelligence and Statistics", "type": "conference", "alternate_names":
      ["AISTATS", "Int Conf Artif Intell Stat"]}, "url": "https://www.semanticscholar.org/paper/3dba006990a306420dcde24bdd48420def1c7e17",
      "title": "Sequential Kernel Herding: Frank-Wolfe Optimization for Particle Filtering",
      "abstract": "Recently, the Frank-Wolfe optimization algorithm was suggested
      as a procedure to obtain adaptive quadrature rules for integrals of functions
      in a reproducing kernel Hilbert space (RKHS) with a potentially faster rate
      of convergence than Monte Carlo integration (and \"kernel herding\" was shown
      to be a special case of this procedure). In this paper, we propose to replace
      the random sampling step in a particle filter by Frank-Wolfe optimization. By
      optimizing the position of the particles, we can obtain better accuracy than
      random or quasi-Monte Carlo sampling. In applications where the evaluation of
      the emission probabilities is expensive (such as in robot localization), the
      additional computational cost to generate the particles through optimization
      can be justified. Experiments on standard synthetic examples as well as on a
      robot localization task indicate indeed an improvement of accuracy over random
      and quasi-Monte Carlo sampling.", "venue": "International Conference on Artificial
      Intelligence and Statistics", "year": 2015, "referenceCount": 33, "citationCount":
      74, "influentialCitationCount": 8, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2015-01-08", "journal": {"volume": "abs/1501.02056",
      "name": "ArXiv"}, "authors": [{"authorId": "1388317459", "name": "Simon Lacoste-Julien"},
      {"authorId": "1759780", "name": "F. Lindsten"}, {"authorId": "144570279", "name":
      "F. Bach"}]}}, {"intents": ["background"], "isInfluential": false, "contexts":
      ["This improves the ability of the discriminator to classify real/fake images
      and enhances the generator\u2019s ability to control the modalities of the generated
      images.", "CGAN was originally proposed as an extension of the original GAN,
      where both the discriminator and generator were fed by an additional class of
      the image [45, 50]."], "citedPaper": {"paperId": "353ecf7b66b3e9ff5e9f41145a147e899a2eea5c",
      "externalIds": {"DBLP": "journals/corr/MirzaO14", "ArXiv": "1411.1784", "MAG":
      "2125389028", "CorpusId": 12803511}, "corpusId": 12803511, "publicationVenue":
      {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
      ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "url": "https://www.semanticscholar.org/paper/353ecf7b66b3e9ff5e9f41145a147e899a2eea5c",
      "title": "Conditional Generative Adversarial Nets", "abstract": "Generative
      Adversarial Nets [8] were recently introduced as a novel way to train generative
      models. In this work we introduce the conditional version of generative adversarial
      nets, which can be constructed by simply feeding the data, y, we wish to condition
      on to both the generator and discriminator. We show that this model can generate
      MNIST digits conditioned on class labels. We also illustrate how this model
      could be used to learn a multi-modal model, and provide preliminary examples
      of an application to image tagging in which we demonstrate how this approach
      can generate descriptive tags which are not part of training labels.", "venue":
      "arXiv.org", "year": 2014, "referenceCount": 18, "citationCount": 8492, "influentialCitationCount":
      1275, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2014-11-06", "journal": {"volume": "abs/1411.1784", "name":
      "ArXiv"}, "authors": [{"authorId": "153583218", "name": "Mehdi Mirza"}, {"authorId":
      "2217144", "name": "Simon Osindero"}]}}, {"intents": [], "isInfluential": false,
      "contexts": [], "citedPaper": {"paperId": "c291a92d89e88a8efb493fe39b6a4b033f1bbef7",
      "externalIds": {"DBLP": "conf/cvpr/ChariLLS15", "MAG": "2951115265", "DOI":
      "10.1109/CVPR.2015.7299193", "CorpusId": 8974920}, "corpusId": 8974920, "publicationVenue":
      {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf", "name": "Computer Vision and
      Pattern Recognition", "type": "conference", "alternate_names": ["CVPR", "Comput
      Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
      "url": "https://www.semanticscholar.org/paper/c291a92d89e88a8efb493fe39b6a4b033f1bbef7",
      "title": "On pairwise costs for network flow multi-object tracking", "abstract":
      "Multi-object tracking has been recently approached with the min-cost network
      flow optimization techniques. Such methods simultaneously resolve multiple object
      tracks in a video and enable modeling of dependencies among tracks. Min-cost
      network flow methods also fit well within the \u201ctracking-by-detection\u201d
      paradigm where object trajectories are obtained by connecting per-frame outputs
      of an object detector. Object detectors, however, often fail due to occlusions
      and clutter in the video. To cope with such situations, we propose to add pairwise
      costs to the min-cost network flow framework. While integer solutions to such
      a problem become NP-hard, we design a convex relaxation solution with an efficient
      rounding heuristic which empirically gives certificates of small suboptimality.
      We evaluate two particular types of pairwise costs and demonstrate improvements
      over recent tracking methods in real-world video sequences.", "venue": "Computer
      Vision and Pattern Recognition", "year": 2014, "referenceCount": 29, "citationCount":
      136, "influentialCitationCount": 8, "isOpenAccess": true, "openAccessPdf": {"url":
      "https://arxiv.org/pdf/1408.3304", "status": null}, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2014-08-14", "journal": {"pages": "5537-5545",
      "name": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"},
      "authors": [{"authorId": "13614028", "name": "Visesh Chari"}, {"authorId": "1388317459",
      "name": "Simon Lacoste-Julien"}, {"authorId": "143991676", "name": "I. Laptev"},
      {"authorId": "1782755", "name": "Josef Sivic"}]}}, {"intents": [], "isInfluential":
      false, "contexts": [], "citedPaper": {"paperId": "4daec165c1f4aa1206b0d91c0b26f0287d1ef52d",
      "externalIds": {"DBLP": "journals/corr/DefazioBL14", "MAG": "2135482703", "ArXiv":
      "1407.0202", "CorpusId": 218654665}, "corpusId": 218654665, "publicationVenue":
      {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing
      Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst",
      "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "url": "https://www.semanticscholar.org/paper/4daec165c1f4aa1206b0d91c0b26f0287d1ef52d",
      "title": "SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly
      Convex Composite Objectives", "abstract": "In this work we introduce a new optimisation
      method called SAGA in the spirit of SAG, SDCA, MISO and SVRG, a set of recently
      proposed incremental gradient algorithms with fast linear convergence rates.
      SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence
      rates, and has support for composite objectives where a proximal operator is
      used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems
      directly, and is adaptive to any inherent strong convexity of the problem. We
      give experimental results showing the effectiveness of our method.", "venue":
      "Neural Information Processing Systems", "year": 2014, "referenceCount": 13,
      "citationCount": 1652, "influentialCitationCount": 323, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2014-07-01",
      "journal": {"pages": "1646-1654"}, "authors": [{"authorId": "34597877", "name":
      "Aaron Defazio"}, {"authorId": "144570279", "name": "F. Bach"}, {"authorId":
      "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"intents": ["background"],
      "isInfluential": false, "contexts": ["Since their first introduction in 2014,
      GANs have attracted a growing interest all over the academia and industry, thanks
      to many advantages over other generative models (mainly Variational Auto-encoders
      (VAEs) [34]):"], "citedPaper": {"paperId": "5f5dc5b9a2ba710937e2c413b37b053cd673df02",
      "externalIds": {"MAG": "2951004968", "DBLP": "journals/corr/KingmaW13", "ArXiv":
      "1312.6114", "CorpusId": 216078090}, "corpusId": 216078090, "publicationVenue":
      {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
      on Learning Representations", "type": "conference", "alternate_names": ["Int
      Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/5f5dc5b9a2ba710937e2c413b37b053cd673df02",
      "title": "Auto-Encoding Variational Bayes", "abstract": "Abstract: How can we
      perform efficient inference and learning in directed probabilistic models, in
      the presence of continuous latent variables with intractable posterior distributions,
      and large datasets? We introduce a stochastic variational inference and learning
      algorithm that scales to large datasets and, under some mild differentiability
      conditions, even works in the intractable case. Our contributions is two-fold.
      First, we show that a reparameterization of the variational lower bound yields
      a lower bound estimator that can be straightforwardly optimized using standard
      stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous
      latent variables per datapoint, posterior inference can be made especially efficient
      by fitting an approximate inference model (also called a recognition model)
      to the intractable posterior using the proposed lower bound estimator. Theoretical
      advantages are reflected in experimental results.", "venue": "International
      Conference on Learning Representations", "year": 2013, "referenceCount": 26,
      "citationCount": 21670, "influentialCitationCount": 4561, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2013-12-20", "journal":
      {"volume": "abs/1312.6114", "name": "CoRR"}, "authors": [{"authorId": "1726807",
      "name": "Diederik P. Kingma"}, {"authorId": "1678311", "name": "M. Welling"}]}},
      {"intents": ["methodology"], "isInfluential": false, "contexts": ["[76] showed
      that deconvolutional layers achieve good visualization for CNNs; this allowed
      the DCGAN generator to create high-resolution images for the \ufb01rst time."],
      "citedPaper": {"paperId": "1a2a770d23b4a171fa81de62a78a3deb0588f238", "externalIds":
      {"ArXiv": "1311.2901", "DBLP": "conf/eccv/ZeilerF14", "MAG": "1849277567", "DOI":
      "10.1007/978-3-319-10590-1_53", "CorpusId": 3960646}, "corpusId": 3960646, "publicationVenue":
      {"id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806", "name": "European Conference
      on Computer Vision", "type": "conference", "alternate_names": ["ECCV", "Eur
      Conf Comput Vis"], "url": "https://link.springer.com/conference/eccv"}, "url":
      "https://www.semanticscholar.org/paper/1a2a770d23b4a171fa81de62a78a3deb0588f238",
      "title": "Visualizing and Understanding Convolutional Networks", "abstract":
      null, "venue": "European Conference on Computer Vision", "year": 2013, "referenceCount":
      31, "citationCount": 14219, "influentialCitationCount": 1024, "isOpenAccess":
      true, "openAccessPdf": {"url": "https://link.springer.com/content/pdf/10.1007/978-3-319-10590-1_53.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2013-11-12", "journal": {"volume": "abs/1311.2901", "name":
      "ArXiv"}, "authors": [{"authorId": "48799969", "name": "Matthew D. Zeiler"},
      {"authorId": "2276554", "name": "R. Fergus"}]}}, {"intents": [], "isInfluential":
      false, "contexts": [], "citedPaper": {"paperId": "b832e1ce81132533c01c2170a88009673d902fa9",
      "externalIds": {"DBLP": "journals/corr/abs-1207-4747", "MAG": "2950261534",
      "ArXiv": "1207.4747", "CorpusId": 16199335}, "corpusId": 16199335, "publicationVenue":
      {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
      on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
      Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/b832e1ce81132533c01c2170a88009673d902fa9",
      "title": "Block-Coordinate Frank-Wolfe Optimization for Structural SVMs", "abstract":
      "Reference EPFL-ARTICLE-229252 URL: http://arxiv.org/abs/1207.4747 Record created
      on 2017-06-21, modified on 2017-06-21", "venue": "International Conference on
      Machine Learning", "year": 2012, "referenceCount": 43, "citationCount": 358,
      "influentialCitationCount": 54, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category":
      "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2012-07-19", "journal": {"pages": "53-61"},
      "authors": [{"authorId": "1388317459", "name": "Simon Lacoste-Julien"}, {"authorId":
      "2456863", "name": "Martin Jaggi"}, {"authorId": "145610994", "name": "Mark
      W. Schmidt"}, {"authorId": "7344844", "name": "Patrick A. Pletscher"}]}}, {"intents":
      [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId": "6acc357015305f8299e228524ccc464bf77ea885",
      "externalIds": {"MAG": "2951938755", "DBLP": "conf/kdd/Lacoste-JulienPDKGG13",
      "ArXiv": "1207.4525", "DOI": "10.1145/2487575.2487592", "CorpusId": 6814936},
      "corpusId": 6814936, "publicationVenue": {"id": "a0edb93b-1e95-4128-a295-6b1659149cef",
      "name": "Knowledge Discovery and Data Mining", "type": "conference", "alternate_names":
      ["KDD", "Knowl Discov Data Min"], "url": "http://www.acm.org/sigkdd/"}, "url":
      "https://www.semanticscholar.org/paper/6acc357015305f8299e228524ccc464bf77ea885",
      "title": "SIGMa: simple greedy matching for aligning large knowledge bases",
      "abstract": "The Internet has enabled the creation of a growing number of large-scale
      knowledge bases in a variety of domains containing complementary information.
      Tools for automatically aligning these knowledge bases would make it possible
      to unify many sources of structured knowledge and answer complex queries. However,
      the efficient alignment of large-scale knowledge bases still poses a considerable
      challenge. Here, we present Simple Greedy Matching (SiGMa), a simple algorithm
      for aligning knowledge bases with millions of entities and facts. SiGMa is an
      iterative propagation algorithm that leverages both the structural information
      from the relationship graph and flexible similarity measures between entity
      properties in a greedy local search, which makes it scalable. Despite its greedy
      nature, our experiments indicate that SiGMa can efficiently match some of the
      world''s largest knowledge bases with high accuracy. We provide additional experiments
      on benchmark datasets which demonstrate that SiGMa can outperform state-of-the-art
      approaches both in accuracy and efficiency.", "venue": "Knowledge Discovery
      and Data Mining", "year": 2012, "referenceCount": 36, "citationCount": 174,
      "influentialCitationCount": 22, "isOpenAccess": true, "openAccessPdf": {"url":
      "https://hal.inria.fr/hal-00918671/file/fp0172-Lacoste-Julien.pdf", "status":
      null}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["Book", "JournalArticle", "Conference"], "publicationDate":
      "2012-07-18", "journal": {"name": "Proceedings of the 19th ACM SIGKDD international
      conference on Knowledge discovery and data mining"}, "authors": [{"authorId":
      "1388317459", "name": "Simon Lacoste-Julien"}, {"authorId": "2524644", "name":
      "Konstantina Palla"}, {"authorId": "144731918", "name": "A. Davies"}, {"authorId":
      "1686448", "name": "G. Kasneci"}, {"authorId": "1686971", "name": "T. Graepel"},
      {"authorId": "1744700", "name": "Zoubin Ghahramani"}]}}, {"intents": [], "isInfluential":
      false, "contexts": [], "citedPaper": {"paperId": "6f0aef1646a83f81029204794c1950d61388ab9f",
      "externalIds": {"DBLP": "conf/icml/BachLO12", "MAG": "2963169589", "ArXiv":
      "1203.4523", "CorpusId": 3815860}, "corpusId": 3815860, "publicationVenue":
      {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
      on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
      Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/6f0aef1646a83f81029204794c1950d61388ab9f",
      "title": "On the Equivalence between Herding and Conditional Gradient Algorithms",
      "abstract": "We show that the herding procedure of Welling (2009) takes exactly
      the form of a standard convex optimization algorithm--namely a conditional gradient
      algorithm minimizing a quadratic moment discrepancy. This link enables us to
      invoke convergence results from convex optimization and to consider faster alternatives
      for the task of approximating integrals in a reproducing kernel Hilbert space.
      We study the behavior of the different variants through numerical simulations.
      The experiments indicate that while we can improve over herding on the task
      of approximating integrals, the original herding algorithm tends to approach
      more often the maximum entropy distribution, shedding more light on the learning
      bias behind herding.", "venue": "International Conference on Machine Learning",
      "year": 2012, "referenceCount": 19, "citationCount": 158, "influentialCitationCount":
      32, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
      "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "external"}, {"category":
      "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2012-03-20", "journal": {"volume": "abs/1203.4523",
      "name": "ArXiv"}, "authors": [{"authorId": "144570279", "name": "F. Bach"},
      {"authorId": "1388317459", "name": "Simon Lacoste-Julien"}, {"authorId": "2533906",
      "name": "G. Obozinski"}]}}, {"intents": [], "isInfluential": false, "contexts":
      [], "citedPaper": {"paperId": "0fe79ea0d4e3db83662d1123a9d73f4eba477ff4", "externalIds":
      {"DBLP": "journals/jmlr/Lacoste-JulienHG11", "MAG": "2099667085", "CorpusId":
      8138646}, "corpusId": 8138646, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
      "name": "International Conference on Artificial Intelligence and Statistics",
      "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
      Stat"]}, "url": "https://www.semanticscholar.org/paper/0fe79ea0d4e3db83662d1123a9d73f4eba477ff4",
      "title": "Approximate inference for the loss-calibrated Bayesian", "abstract":
      "We consider the problem of approximate inference in the context of Bayesian
      decision theory. Traditional approaches focus on approximating general properties
      of the posterior, ignoring the decision task { and associated losses { for which
      the posterior could be used. We argue that this can be suboptimal and propose
      instead to loss-calibrate the approximate inference methods with respect to
      the decision task at hand. We present a general framework rooted in Bayesian
      decision theory to analyze approximate inference from the perspective of losses,
      opening up several research directions. As a rst loss-calibrated approximate
      inference attempt, we propose an EM-like algorithm on the Bayesian posterior
      risk and show how it can improve a standard approach to Gaussian process classication
      when losses are asymmetric.", "venue": "International Conference on Artificial
      Intelligence and Statistics", "year": 2011, "referenceCount": 22, "citationCount":
      63, "influentialCitationCount": 10, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy":
      [{"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"},
      {"category": "Mathematics", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2011-06-14", "journal": {"pages": "416-424"},
      "authors": [{"authorId": "1388317459", "name": "Simon Lacoste-Julien"}, {"authorId":
      "3108066", "name": "Ferenc Husz\u00e1r"}, {"authorId": "1744700", "name": "Zoubin
      Ghahramani"}]}}, {"intents": [], "isInfluential": false, "contexts": [], "citedPaper":
      {"paperId": "2e20ed644e7d6e04dd7ab70084f1bf28f93f75e9", "externalIds": {"MAG":
      "2122683976", "DBLP": "conf/nips/Lacoste-JulienSJ08", "CorpusId": 6500077},
      "corpusId": 6500077, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/2e20ed644e7d6e04dd7ab70084f1bf28f93f75e9",
      "title": "DiscLDA: Discriminative Learning for Dimensionality Reduction and
      Classification", "abstract": "Probabilistic topic models have become popular
      as methods for dimensionality reduction in collections of text documents or
      images. These models are usually treated as generative models and trained using
      maximum likelihood or Bayesian methods. In this paper, we discuss an alternative:
      a discriminative framework in which we assume that supervised side information
      is present, and in which we wish to take that side information into account
      in finding a reduced dimensionality representation. Specifically, we present
      DiscLDA, a discriminative variation on Latent Dirichlet Allocation (LDA) in
      which a class-dependent linear transformation is introduced on the topic mixture
      proportions. This parameter is estimated by maximizing the conditional likelihood.
      By using the transformed topic mixture proportions as a new representation of
      documents, we obtain a supervised dimensionality reduction algorithm that uncovers
      the latent structure in a document collection while preserving predictive power
      for the task of classification. We compare the predictive power of the latent
      structure of DiscLDA with unsupervised LDA on the 20 Newsgroups document classification
      task and show how our model can identify shared topics across classes as well
      as class-dependent topics.", "venue": "Neural Information Processing Systems",
      "year": 2008, "referenceCount": 11, "citationCount": 438, "influentialCitationCount":
      33, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2008-12-08", "journal": {"pages": "897-904"}, "authors": [{"authorId": "1388317459",
      "name": "Simon Lacoste-Julien"}, {"authorId": "145757665", "name": "Fei Sha"},
      {"authorId": "1694621", "name": "Michael I. Jordan"}]}}, {"intents": [], "isInfluential":
      false, "contexts": [], "citedPaper": {"paperId": "7669c8e2edb9409dec9911ff20a5ba2d53d48ac0",
      "externalIds": {"MAG": "2100564780", "DBLP": "journals/jmlr/TaskarLJ06", "DOI":
      "10.5555/1248547.1248607", "CorpusId": 14842644}, "corpusId": 14842644, "publicationVenue":
      {"id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780", "name": "Journal of machine learning
      research", "type": "journal", "alternate_names": ["Journal of Machine Learning
      Research", "J mach learn res", "J Mach Learn Res"], "issn": "1532-4435", "alternate_issns":
      ["1533-7928"], "url": "http://www.ai.mit.edu/projects/jmlr/", "alternate_urls":
      ["http://jmlr.csail.mit.edu/", "http://www.jmlr.org/", "http://portal.acm.org/affiliated/jmlr"]},
      "url": "https://www.semanticscholar.org/paper/7669c8e2edb9409dec9911ff20a5ba2d53d48ac0",
      "title": "Structured Prediction, Dual Extragradient and Bregman Projections",
      "abstract": "We present a simple and scalable algorithm for maximum-margin estimation
      of structured output models, including an important class of Markov networks
      and combinatorial models. We formulate the estimation problem as a convex-concave
      saddle-point problem that allows us to use simple projection methods based on
      the dual extragradient algorithm (Nesterov, 2003). The projection step can be
      solved using dynamic programming or combinatorial algorithms for min-cost convex
      flow, depending on the structure of the problem. We show that this approach
      provides a memory-efficient alternative to formulations based on reductions
      to a quadratic program (QP). We analyze the convergence of the method and present
      experiments on two very different structured prediction tasks: 3D image segmentation
      and word alignment, illustrating the favorable scaling properties of our algorithm.",
      "venue": "Journal of machine learning research", "year": 2006, "referenceCount":
      44, "citationCount": 129, "influentialCitationCount": 6, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
      "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2006-12-01", "journal":
      {"volume": "7", "pages": "1627-1653", "name": "J. Mach. Learn. Res."}, "authors":
      [{"authorId": "1685978", "name": "B. Taskar"}, {"authorId": "1388317459", "name":
      "Simon Lacoste-Julien"}, {"authorId": "1694621", "name": "Michael I. Jordan"}]}},
      {"intents": [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId":
      "ddddd9be714919db194a371070a1e13a6a0546b4", "externalIds": {"MAG": "2122244345",
      "DBLP": "conf/naacl/Lacoste-JulienTKJ06", "ACL": "N06-1015", "DOI": "10.3115/1220835.1220850",
      "CorpusId": 5409635}, "corpusId": 5409635, "publicationVenue": {"id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "type": "conference", "alternate_names": ["North Am Chapter Assoc Comput Linguistics",
      "NAACL"], "url": "https://www.aclweb.org/portal/naacl"}, "url": "https://www.semanticscholar.org/paper/ddddd9be714919db194a371070a1e13a6a0546b4",
      "title": "Word Alignment via Quadratic Assignment", "abstract": "Recently, discriminative
      word alignment methods have achieved state-of-the-art accuracies by extending
      the range of information sources that can be easily incorporated into aligners.
      The chief advantage of a discriminative framework is the ability to score alignments
      based on arbitrary features of the matching word tokens, including orthographic
      form, predictions of other models, lexical context and so on. However, the proposed
      bipartite matching model of Taskar et al. (2005), despite being tractable and
      effective, has two important limitations. First, it is limited by the restriction
      that words have fertility of at most one. More importantly, first order correlations
      between consecutive words cannot be directly captured by the model. In this
      work, we address these limitations by enriching the model form. We give estimation
      and inference algorithms for these enhancements. Our best model achieves a relative
      AER reduction of 25% over the basic matching formulation, outperforming intersected
      IBM Model 4 without using any overly compute-intensive features. By including
      predictions of other models as features, we achieve AER of 3.8 on the standard
      Hansards dataset.", "venue": "North American Chapter of the Association for
      Computational Linguistics", "year": 2006, "referenceCount": 11, "citationCount":
      100, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url":
      "https://dl.acm.org/doi/pdf/10.3115/1220835.1220850", "status": null}, "fieldsOfStudy":
      ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2006-06-04", "journal":
      {"volume": "", "pages": "112-119", "name": ""}, "authors": [{"authorId": "1388317459",
      "name": "Simon Lacoste-Julien"}, {"authorId": "1685978", "name": "B. Taskar"},
      {"authorId": "38666915", "name": "D. Klein"}, {"authorId": "1694621", "name":
      "Michael I. Jordan"}]}}, {"intents": [], "isInfluential": false, "contexts":
      [], "citedPaper": {"paperId": "fd5590d9696be6d9e0807c6660826f5351093790", "externalIds":
      {"MAG": "2161002641", "DBLP": "conf/nips/TaskarLJ05", "CorpusId": 2298202},
      "corpusId": 2298202, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/fd5590d9696be6d9e0807c6660826f5351093790",
      "title": "Structured Prediction via the Extragradient Method", "abstract": "We
      present a simple and scalable algorithm for large-margin estimation of structured
      models, including an important class of Markov networks and combinatorial models.
      We formulate the estimation problem as a convex-concave saddle-point problem
      and apply the extragradient method, yielding an algorithm with linear convergence
      using simple gradient and projection calculations. The projection step can be
      solved using combinatorial algorithms for min-cost quadratic flow. This makes
      the approach an efficient alternative to formulations based on reductions to
      a quadratic program (QP). We present experiments on two very different structured
      prediction tasks: 3D image segmentation and word alignment, illustrating the
      favorable scaling properties of our algorithm.", "venue": "Neural Information
      Processing Systems", "year": 2005, "referenceCount": 32, "citationCount": 64,
      "influentialCitationCount": 5, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category":
      "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2005-12-05", "journal": {"pages": "1345-1352"},
      "authors": [{"authorId": "1685978", "name": "B. Taskar"}, {"authorId": "1388317459",
      "name": "Simon Lacoste-Julien"}, {"authorId": "1694621", "name": "Michael I.
      Jordan"}]}}, {"intents": [], "isInfluential": false, "contexts": [], "citedPaper":
      {"paperId": "64a007a07cbeab1b6949f196e58fdbe93ef1a297", "externalIds": {"DBLP":
      "conf/naacl/TaskarLK05", "MAG": "2119224513", "ACL": "H05-1010", "DOI": "10.3115/1220575.1220585",
      "CorpusId": 2379886}, "corpusId": 2379886, "publicationVenue": {"id": "f8e3f8d0-0f40-48c0-b3c0-0c540237b859",
      "name": "Human Language Technology - The Baltic Perspectiv", "type": "conference",
      "alternate_names": ["Human Language Technology", "HLT", "Hum Lang Technol",
      "Hum Lang Technol  Balt Perspect"]}, "url": "https://www.semanticscholar.org/paper/64a007a07cbeab1b6949f196e58fdbe93ef1a297",
      "title": "A Discriminative Matching Approach to Word Alignment", "abstract":
      "We present a discriminative, large-margin approach to feature-based matching
      for word alignment. In this framework, pairs of word tokens receive a matching
      score, which is based on features of that pair, including measures of association
      between the words, distortion between their positions, similarity of the orthographic
      form, and so on. Even with only 100 labeled training examples and simple features
      which incorporate counts from a large unlabeled corpus, we achieve AER performance
      close to IBM Model 4, in much less time. Including Model 4 predictions as features,
      we achieve a relative AER reduction of 22% in over intersected Model 4 alignments.",
      "venue": "Human Language Technology - The Baltic Perspectiv", "year": 2005,
      "referenceCount": 18, "citationCount": 190, "influentialCitationCount": 16,
      "isOpenAccess": true, "openAccessPdf": {"url": "http://dl.acm.org/ft_gateway.cfm?id=1220585&type=pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2005-10-06", "journal": {"pages": "73-80"}, "authors": [{"authorId": "1685978",
      "name": "B. Taskar"}, {"authorId": "1388317459", "name": "Simon Lacoste-Julien"},
      {"authorId": "38666915", "name": "D. Klein"}]}}, {"intents": [], "isInfluential":
      false, "contexts": [], "citedPaper": {"paperId": "a70563b755ee309b11c49062af53020214046fcd",
      "externalIds": {"MAG": "2540599577", "DOI": "10.1109/CACSD.2004.1393852", "CorpusId":
      1899687}, "corpusId": 1899687, "publicationVenue": {"id": "3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
      "name": "IEEE International Conference on Robotics and Automation", "type":
      "conference", "alternate_names": ["International Conference on Robotics and
      Automation", "Int Conf Robot Autom", "ICRA", "IEEE Int Conf Robot Autom"], "issn":
      "2152-4092", "alternate_issns": ["2379-9544"], "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639",
      "alternate_urls": ["http://www.ncsu.edu/IEEE-RAS/"]}, "url": "https://www.semanticscholar.org/paper/a70563b755ee309b11c49062af53020214046fcd",
      "title": "Meta-modelling hybrid formalisms", "abstract": "This article demonstrates
      how meta-modelling can simplify the construction of domain-and formalism-specific
      modelling environments. Using AToM3 (a tool for multi-formalism and meta-modelling
      developed at McGill University), a model is constructed of a hybrid formalism,
      HS, that combines event scheduling constructs with ordinary differential equations.
      From this specification, an HS-specific visual modelling environment is synthesized.
      For the purpose of this demonstration, a simple hybrid model of a bouncing ball
      is modelled in this environment. It is envisioned that the future of modelling
      and simulation in general, and more specifically in hybrid dynamic systems design
      lies in domain-specific computer automated multi-paradigm modelling (CAMPaM)
      which combines multi-abstraction, multi-formalism, and meta-modelling. The small
      example presented in this article demonstrates the feasibility of this approach",
      "venue": "IEEE International Conference on Robotics and Automation", "year":
      2004, "referenceCount": 26, "citationCount": 13, "influentialCitationCount":
      0, "isOpenAccess": true, "openAccessPdf": {"url": "http://msdl.cs.mcgill.ca/people/mosterman/campam/cacsd04/index.html/lacoste-julien.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}],
      "publicationTypes": ["Conference"], "publicationDate": "2004-09-02", "journal":
      {"pages": "65-70", "name": "2004 IEEE International Conference on Robotics and
      Automation (IEEE Cat. No.04CH37508)"}, "authors": [{"authorId": "2239746115",
      "name": "Simon Lacoste-Julien"}, {"authorId": "1762640", "name": "H. Vangheluwe"},
      {"authorId": "2239747160", "name": "Juan de Lara"}, {"authorId": "1779212",
      "name": "P. Mosterman"}]}}, {"intents": ["background"], "isInfluential": false,
      "contexts": ["The task has received substantial attention from researchers in
      the computer vision community for its wide downstream applications, such as
      image reconstruction [5]."], "citedPaper": {"paperId": "7c850f4c56f184891f3ccfe2de6213881dc0e08c",
      "externalIds": {"DBLP": "journals/spm/ParkPK03", "MAG": "2067042811", "DOI":
      "10.1109/MSP.2003.1203207", "CorpusId": 12320918}, "corpusId": 12320918, "publicationVenue":
      {"id": "f62e5eab-173a-4e0a-a963-ed8de9835d22", "name": "IEEE Signal Processing
      Magazine", "type": "journal", "alternate_names": ["IEEE Signal Process Mag"],
      "issn": "1053-5888", "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=79",
      "alternate_urls": ["https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=79"]},
      "url": "https://www.semanticscholar.org/paper/7c850f4c56f184891f3ccfe2de6213881dc0e08c",
      "title": "Super-resolution image reconstruction: a technical overview", "abstract":
      "A new approach toward increasing spatial resolution is required to overcome
      the limitations of the sensors and optics manufacturing technology. One promising
      approach is to use signal processing techniques to obtain an high-resolution
      (HR) image (or sequence) from observed multiple low-resolution (LR) images.
      Such a resolution enhancement approach has been one of the most active research
      areas, and it is called super resolution (SR) (or HR) image reconstruction or
      simply resolution enhancement. In this article, we use the term \"SR image reconstruction\"
      to refer to a signal processing approach toward resolution enhancement because
      the term \"super\" in \"super resolution\" represents very well the characteristics
      of the technique overcoming the inherent resolution limitation of LR imaging
      systems. The major advantage of the signal processing approach is that it may
      cost less and the existing LR imaging systems can be still utilized. The SR
      image reconstruction is proved to be useful in many practical cases where multiple
      frames of the same scene can be obtained, including medical imaging, satellite
      imaging, and video applications. The goal of this article is to introduce the
      concept of SR algorithms to readers who are unfamiliar with this area and to
      provide a review for experts. To this purpose, we present the technical review
      of various existing SR methodologies which are often employed. Before presenting
      the review of existing SR algorithms, we first model the LR image acquisition
      process.", "venue": "IEEE Signal Processing Magazine", "year": 2003, "referenceCount":
      79, "citationCount": 3372, "influentialCitationCount": 158, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Engineering",
      "source": "s2-fos-model"}, {"category": "Physics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2003-06-11",
      "journal": {"volume": "20", "pages": "21-36", "name": "IEEE Signal Process.
      Mag."}, "authors": [{"authorId": "2115277167", "name": "S. Park"}, {"authorId":
      "2149269035", "name": "Min Kyu Park"}, {"authorId": "1839581", "name": "M. Kang"}]}},
      {"intents": ["background"], "isInfluential": false, "contexts": ["Neural networks
      have been shown to forget previous learned tasks [17, 35], and catastrophic
      forgetting was previously considered as a major cause for GAN training instability."],
      "citedPaper": {"paperId": "2722b9e5ab8da95f03e578bb65879c452c105385", "externalIds":
      {"MAG": "2554957561", "DOI": "10.1016/S1364-6613(99)01294-2", "CorpusId": 2691726,
      "PubMed": "10322466"}, "corpusId": 2691726, "publicationVenue": {"id": "20cb626a-518d-4016-826a-0157cf2f8fd6",
      "name": "Trends in Cognitive Sciences", "type": "journal", "alternate_names":
      ["Trends Cogn Sci"], "issn": "1364-6613", "url": "https://www.cell.com/trends/cognitive-sciences/home",
      "alternate_urls": ["http://www.cell.com/trends/cognitive-sciences/home", "https://www.journals.elsevier.com/trends-in-cognitive-sciences",
      "http://www.sciencedirect.com/science/journal/13646613"]}, "url": "https://www.semanticscholar.org/paper/2722b9e5ab8da95f03e578bb65879c452c105385",
      "title": "Catastrophic forgetting in connectionist networks", "abstract": null,
      "venue": "Trends in Cognitive Sciences", "year": 1999, "referenceCount": 68,
      "citationCount": 1812, "influentialCitationCount": 75, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Psychology", "Medicine"], "s2FieldsOfStudy":
      [{"category": "Psychology", "source": "external"}, {"category": "Medicine",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "1999-04-01",
      "journal": {"volume": "3", "pages": "128-135", "name": "Trends in Cognitive
      Sciences"}, "authors": [{"authorId": "2440747", "name": "R. French"}]}}, {"intents":
      ["background"], "isInfluential": false, "contexts": ["The first two scenarios
      can be solved via supervised learning but they will cost a lot of human effort
      to enrich the labeled data or to use active learning approaches [11]."], "citedPaper":
      {"paperId": "1150f9289c6151506e3f7cf0e6ebbcfd49f1dace", "externalIds": {"MAG":
      "1773652845", "DBLP": "conf/nips/CohnGJ94", "ArXiv": "cs/9603104", "DOI": "10.1613/jair.295",
      "CorpusId": 9242771}, "corpusId": 9242771, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"},
      "url": "https://www.semanticscholar.org/paper/1150f9289c6151506e3f7cf0e6ebbcfd49f1dace",
      "title": "Active Learning with Statistical Models", "abstract": "For many types
      of learners one can compute the statistically \"optimal\" way to select data.
      We review how these techniques have been used with feedforward neural networks
      [MacKay, 1992; Cohn, 1994]. We then show how the same principles may be used
      to select data for two alternative, statistically-based learning architectures:
      mixtures of Gaussians and locally weighted regression. While the techniques
      for neural networks are expensive and approximate, the techniques for mixtures
      of Gaussians and locally weighted regression are both efficient and accurate.",
      "venue": "Neural Information Processing Systems", "year": 1996, "referenceCount":
      31, "citationCount": 2256, "influentialCitationCount": 107, "isOpenAccess":
      true, "openAccessPdf": {"url": "https://www.jair.org/index.php/jair/article/download/10158/24082",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate":
      "1996-02-29", "journal": {"volume": "cs.AI/9603104", "name": "ArXiv"}, "authors":
      [{"authorId": "2976268", "name": "D. Cohn"}, {"authorId": "1744700", "name":
      "Zoubin Ghahramani"}, {"authorId": "1694621", "name": "Michael I. Jordan"}]}},
      {"intents": ["background"], "isInfluential": false, "contexts": ["That style
      is the way some semantics is delivered [11]."], "citedPaper": {"paperId": "16f1d467c7b855321ca1d8a643c30f4ddd7e3650",
      "externalIds": {"ACL": "E85-1027", "DBLP": "conf/eacl/McDonaldP85", "MAG": "1992258378",
      "DOI": "10.3115/976931.976958", "CorpusId": 15628268}, "corpusId": 15628268,
      "publicationVenue": {"id": "8de18c35-6785-4e54-99f2-21ee961302c6", "name": "Conference
      of the European Chapter of the Association for Computational Linguistics", "type":
      "conference", "alternate_names": ["Conf Eur Chapter Assoc Comput Linguistics",
      "EACL"], "url": "https://www.aclweb.org/anthology/venues/eacl/"}, "url": "https://www.semanticscholar.org/paper/16f1d467c7b855321ca1d8a643c30f4ddd7e3650",
      "title": "A Computational Theory of Prose Style for Natural Language Generation",
      "abstract": "In this paper we report on initial research we have conducted on
      a computational theory of prose style. Our theory speaks to the following major
      points:1. Where in the generation process style is taken into account.2. How
      a particular prose style is represented; what \"stylistic rules\" look like;3.
      What modifications to a generation algorithm are needed; what the decision is
      that evaluates stylistic alternatives;4. What elaborations to the normal description
      of surface structure are necessary to make it usable as a plan for the text
      and a reference for these decisions;5. What kinds of information decisions about
      style have access to.Our theory emerged out of design experiments we have made
      over the past year with our natural language generation system, the Zetalisp
      program MUMBLE. In the process we have extended MUMBLE through the addition
      of an additional process that now mediates between content planning and linguistic
      realization. This new process, which we call \"attachment\", provides the further
      significant benefit that text structure is no longer dictated by the structure
      of the message: the sequential order and dominance relationships of concepts
      in the message no longer force one form onto the words and phrases in the text.
      Instead, rhetorical and intentional directives can be interpreted flexibly in
      the context of the ongoing discourse and stylistic preferences. The text is
      built up through composition under the direction of linguistic organizing principles,
      rather than having to follow conceptual principles in lockstep.We will begin
      by describing what we mean by prose style and then introducing the generation
      task that lead us to this theory, the reproduction of short encyclopedia articles
      on African tribes. We will then use that task to outline the parts of our theory
      and the operations of the attachment process. Finally we will compare our techniques
      to the related work of Davey, McKeown and Derr, and Gabriel, and consider some
      of the possible psycholinguistic hypotheses that it may lead to.", "venue":
      "Conference of the European Chapter of the Association for Computational Linguistics",
      "year": 1985, "referenceCount": 13, "citationCount": 40, "influentialCitationCount":
      0, "isOpenAccess": true, "openAccessPdf": {"url": "https://dl.acm.org/doi/pdf/10.3115/976931.976958",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "1985-03-27",
      "journal": {"pages": "187-193"}, "authors": [{"authorId": "27119069", "name":
      "David D. McDonald"}, {"authorId": "1707726", "name": "J. Pustejovsky"}]}},
      {"intents": [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId":
      "69d76d8a89ab1ad8a988f320dc424ee6f9e67288", "externalIds": {"DBLP": "journals/corr/abs-2211-14666",
      "DOI": "10.48550/arXiv.2211.14666", "CorpusId": 254044391}, "corpusId": 254044391,
      "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org",
      "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"},
      "url": "https://www.semanticscholar.org/paper/69d76d8a89ab1ad8a988f320dc424ee6f9e67288",
      "title": "Synergies Between Disentanglement and Sparsity: a Multi-Task Learning
      Perspective", "abstract": "Although disentangled representations are often said
      to be beneficial for downstream tasks, current empirical and theoretical understanding
      is limited. In this work, we provide evidence that disentangled representations
      coupled with sparse base-predictors improve generalization. In the context of
      multi-task learning, we prove a new identifiability result that provides conditions
      under which maximally sparse base-predictors yield disentangled representations.
      Motivated by this theoretical result, we propose a practical approach to learn
      disentangled representations based on a sparsity-promoting bi-level optimization
      problem. Finally, we explore a meta-learning version of this algorithm based
      on group Lasso multiclass SVM base-predictors, for which we derive a tractable
      dual formulation. It obtains competitive results on standard few-shot classification
      benchmarks, while each task is using only a fraction of the learned representations.",
      "venue": "arXiv.org", "year": 2022, "referenceCount": 92, "citationCount": 4,
      "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url":
      "http://arxiv.org/pdf/2211.14666", "status": null}, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": null, "journal": {"volume": "abs/2211.14666",
      "name": "ArXiv"}, "authors": [{"authorId": "134730235", "name": "S\u00e9bastien
      Lachapelle"}, {"authorId": "7636193", "name": "T. Deleu"}, {"authorId": "133841722",
      "name": "Divyat Mahajan"}, {"authorId": "2065139188", "name": "Ioannis Mitliagkas"},
      {"authorId": "1865800402", "name": "Y. Bengio"}, {"authorId": "1388317459",
      "name": "Simon Lacoste-Julien"}, {"authorId": "14205549", "name": "Quentin Bertrand"}]}},
      {"intents": [], "isInfluential": false, "contexts": [], "citedPaper": {"paperId":
      null, "externalIds": null, "corpusId": null, "publicationVenue": null, "url":
      null, "title": "On the Convergence of Continuous Constrained Optimization for
      Bayesian Network Structure Learning, AISTATS, 2022", "abstract": null, "venue":
      "", "year": 2022, "referenceCount": null, "citationCount": null, "influentialCitationCount":
      null, "isOpenAccess": null, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
      null, "publicationTypes": null, "publicationDate": null, "journal": null, "authors":
      []}}, {"intents": [], "isInfluential": false, "contexts": [], "citedPaper":
      {"paperId": null, "externalIds": null, "corpusId": null, "publicationVenue":
      null, "url": null, "title": "D Scieur", "abstract": null, "venue": "Affine Invariant
      Analysis of Frank-Wolfe on Strongly Convex Sets, ICML,", "year": 2021, "referenceCount":
      null, "citationCount": null, "influentialCitationCount": null, "isOpenAccess":
      null, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": null,
      "publicationTypes": null, "publicationDate": null, "journal": null, "authors":
      []}}, {"intents": [], "isInfluential": false, "contexts": [], "citedPaper":
      {"paperId": null, "externalIds": null, "corpusId": null, "publicationVenue":
      null, "url": null, "title": "S Lacoste-Julien", "abstract": null, "venue": "Structured
      Convolutional Kernel Networks for Airline Crew Scheduling, ICML,", "year": 2021,
      "referenceCount": null, "citationCount": null, "influentialCitationCount": null,
      "isOpenAccess": null, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
      null, "publicationTypes": null, "publicationDate": null, "journal": null, "authors":
      []}}, {"intents": ["background"], "isInfluential": false, "contexts": ["Recently
      a more advanced styleGAN architecture has been proposed [32, 18, 33]."], "citedPaper":
      {"paperId": null, "externalIds": null, "corpusId": null, "publicationVenue":
      null, "url": null, "title": "Swagan: A style-based wavelet-driven generative
      model", "abstract": null, "venue": "ACM Trans. Graph.,", "year": 2021, "referenceCount":
      null, "citationCount": null, "influentialCitationCount": null, "isOpenAccess":
      null, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": null,
      "publicationTypes": null, "publicationDate": null, "journal": null, "authors":
      []}}, {"intents": ["background"], "isInfluential": false, "contexts": ["It was
      proved successful in many domains such as computer vision [14, 29, 25, 38],
      semantic segmentation [39, 27, 70, 24], time-series synthesis [9, 23], image
      editing [61, 36, 19, 3, 75], natural language processing [15, 28, 22], text-to-image
      generation [59, 58, 54], and many more."], "citedPaper": {"paperId": null, "externalIds":
      null, "corpusId": null, "publicationVenue": null, "url": null, "title": "Stylegan-nada:
      Clip-guided domain adaptation of image generators", "abstract": null, "venue":
      "", "year": 2021, "referenceCount": null, "citationCount": null, "influentialCitationCount":
      null, "isOpenAccess": null, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
      null, "publicationTypes": null, "publicationDate": null, "journal": null, "authors":
      []}}, {"intents": ["methodology"], "isInfluential": false, "contexts": ["This
      is just a partial list and other loss functions and regularization methods exist
      such as the least square GANs [41] or optimal transport models [55]."], "citedPaper":
      {"paperId": null, "externalIds": null, "corpusId": null, "publicationVenue":
      null, "url": null, "title": "Computational optimal transport: With applications
      to data science", "abstract": null, "venue": "Foundations and Trends\u00ae in
      Machine Learning,", "year": 2019, "referenceCount": null, "citationCount": null,
      "influentialCitationCount": null, "isOpenAccess": null, "openAccessPdf": null,
      "fieldsOfStudy": null, "s2FieldsOfStudy": null, "publicationTypes": null, "publicationDate":
      null, "journal": null, "authors": []}}, {"intents": ["methodology", "background"],
      "isInfluential": true, "contexts": ["The \ufb01rst GAN that was introduced by
      Goodfellow et al. [20] is depicted in Fig.", "We sample a minibatch of \ud835\udc5a
      noise samples { z ( 1 ) , ..., z ( \ud835\udc5a ) } from \ud835\udc5d z , and
      calculate the generator\u2019s gradients and update the generator weights, \ud835\udf03
      \ud835\udc3a , by descending this term.", "Due to the vanishing gradient problem,
      Goodfellow et al. [20] proposed a change to the original adversarial loss.",
      "The original GAN model and loss proposed by Goodfellow et al. [20] su\ufb00er
      from three inherent challenges: (1) Mode collapse; (2) Vanishing gradients;
      and (3) Image quality.", "The global optimality, stated in [20], is de\ufb01ned
      when \ud835\udc37 is optimized for any given ."], "citedPaper": {"paperId":
      "c68796f833a7151f0a63d1d1608dc902b4fdc9b6", "externalIds": {"CorpusId": 10319744},
      "corpusId": 10319744, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/c68796f833a7151f0a63d1d1608dc902b4fdc9b6",
      "title": "GENERATIVE ADVERSARIAL NETS", "abstract": "Estimating individualized
      treatment effects (ITE) is a challenging task due to the need for an individual\u2019s
      potential outcomes to be learned from biased data and without having access
      to the counterfactuals. We propose a novel method for inferring ITE based on
      the Generative Adversarial Nets (GANs) framework. Our method, termed Generative
      Adversarial Nets for inference of Individualized Treatment Effects (GANITE),
      is motivated by the possibility that we can capture the uncertainty in the counterfactual
      distributions by attempting to learn them using a GAN. We generate proxies of
      the counterfactual outcomes using a counterfactual generator, G, and then pass
      these proxies to an ITE generator, I, in order to train it. By modeling both
      of these using the GAN framework, we are able to infer based on the factual
      data, while still accounting for the unseen counterfactuals. We test our method
      on three real-world datasets (with both binary and multiple treatments) and
      show that GANITE outperforms state-of-the-art methods.", "venue": "", "year":
      2018, "referenceCount": 24, "citationCount": 7125, "influentialCitationCount":
      1138, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      null, "publicationDate": null, "journal": null, "authors": [{"authorId": "2262142027",
      "name": "Individualized Treat"}, {"authorId": "2262204742", "name": "Jinsung
      Yoon"}]}}, {"intents": [], "isInfluential": false, "contexts": [], "citedPaper":
      {"paperId": "0edc142b51581a358055d7eddada8a4d0f9d021b", "externalIds": {"DBLP":
      "journals/corr/abs-1802-10551", "CorpusId": 260497042}, "corpusId": 260497042,
      "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org",
      "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"},
      "url": "https://www.semanticscholar.org/paper/0edc142b51581a358055d7eddada8a4d0f9d021b",
      "title": "A Variational Inequality Perspective on Generative Adversarial Nets",
      "abstract": "Stability has been a recurrent issue in training generative adversarial
      networks (GANs). One common way to tackle this issue has been to propose new
      formulations of the GAN objective. Yet, surprisingly few studies have looked
      at optimization methods specifically designed for this adversarial training.
      In this work, we review the \u201cvariational inequality\u201d framework which
      contains most formulations of the GAN objective introduced so far. Taping into
      the mathematical programming literature, we counter some common misconceptions
      about the difficulties of saddle point optimization and propose to extend standard
      methods designed for variational inequalities to GANs training, such as a stochastic
      version of the extragradient method, and empirically investigate their behavior
      on GANs.", "venue": "arXiv.org", "year": 2018, "referenceCount": 38, "citationCount":
      18, "influentialCitationCount": 3, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate":
      null, "journal": {"volume": "abs/1802.10551", "name": "ArXiv"}, "authors": [{"authorId":
      "8150760", "name": "Gauthier Gidel"}, {"authorId": "40201329", "name": "Hugo
      Berard"}, {"authorId": "145467703", "name": "Pascal Vincent"}, {"authorId":
      "1388317459", "name": "Simon Lacoste-Julien"}]}}, {"intents": ["background"],
      "isInfluential": false, "contexts": ["Since their \ufb01rst introduction in
      2014, GANs have attracted a growing interest all over the academia and industry,
      thanks to many advantages over other generative models (mainly Variational Auto-encoders
      (VAEs) [34]): \u2022 Sharp images : GANs produce sharper images than other generative
      models."], "citedPaper": {"paperId": null, "externalIds": null, "corpusId":
      null, "publicationVenue": null, "url": null, "title": "and Max Welling", "abstract":
      null, "venue": "Proceedings of the National Academy of Sciences", "year": 2017,
      "referenceCount": null, "citationCount": null, "influentialCitationCount": null,
      "isOpenAccess": null, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
      null, "publicationTypes": null, "publicationDate": null, "journal": null, "authors":
      []}}, {"intents": [], "isInfluential": false, "contexts": [], "citedPaper":
      {"paperId": null, "externalIds": null, "corpusId": null, "publicationVenue":
      null, "url": null, "title": "ASAGA: AsynchronousParallel Saga", "abstract":
      null, "venue": "", "year": 2017, "referenceCount": null, "citationCount": null,
      "influentialCitationCount": null, "isOpenAccess": null, "openAccessPdf": null,
      "fieldsOfStudy": null, "s2FieldsOfStudy": null, "publicationTypes": null, "publicationDate":
      null, "journal": null, "authors": []}}, {"intents": ["background"], "isInfluential":
      false, "contexts": ["For example, a human\u2019s EEG is sensitive to different
      types of face images such as happy, angry, and sad faces [42]."], "citedPaper":
      {"paperId": "32610513986d27c9e3c3045e4f5e04f8b47c9082", "externalIds": {"MAG":
      "2181325373", "DBLP": "journals/neuroimage/MavratzakisHW16", "DOI": "10.1016/j.neuroimage.2015.09.065",
      "CorpusId": 3239285, "PubMed": "26453930"}, "corpusId": 3239285, "publicationVenue":
      {"id": "fd4c7628-c16e-4b50-8555-3ac3ad6da2d7", "name": "NeuroImage", "type":
      "journal", "issn": "1053-8119", "url": "http://www.elsevier.com/locate/ynimg",
      "alternate_urls": ["http://www.elsevier.com/wps/find/journaldescription.cws_home/622925/description#description",
      "https://www.journals.elsevier.com/neuroimage", "http://www.sciencedirect.com/science/journal/10538119",
      "http://www.idealibrary.com/"]}, "url": "https://www.semanticscholar.org/paper/32610513986d27c9e3c3045e4f5e04f8b47c9082",
      "title": "Emotional facial expressions evoke faster orienting responses, but
      weaker emotional responses at neural and behavioural levels compared to scenes:
      A simultaneous EEG and facial EMG study", "abstract": null, "venue": "NeuroImage",
      "year": 2016, "referenceCount": 123, "citationCount": 70, "influentialCitationCount":
      1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Psychology",
      "Medicine", "Computer Science"], "s2FieldsOfStudy": [{"category": "Psychology",
      "source": "external"}, {"category": "Medicine", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Psychology", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      null, "journal": {"volume": "124", "pages": "931-946", "name": "NeuroImage"},
      "authors": [{"authorId": "3485684", "name": "A. Mavratzakis"}, {"authorId":
      "145676117", "name": "C. Herbert"}, {"authorId": "49098412", "name": "P. Walla"}]}},
      {"intents": ["background"], "isInfluential": false, "contexts": ["\u2022 The
      discriminator architecture replaces all normal ReLU activations with Leaky-ReLU
      [40]; this activation multiplies also the negative kernel output by a small
      value (0 ."], "citedPaper": {"paperId": "367f2c63a6f6a10b3b64b8729d601e69337ee3cc",
      "externalIds": {"MAG": "2901349389", "CorpusId": 16489696}, "corpusId": 16489696,
      "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/367f2c63a6f6a10b3b64b8729d601e69337ee3cc",
      "title": "Rectifier Nonlinearities Improve Neural Network Acoustic Models",
      "abstract": "Deep neural network acoustic models produce substantial gains in
      large vocabulary continuous speech recognition systems. Emerging work with recti\ufb01ed
      linear (ReL) hidden units demonstrates additional gains in \ufb01nal system
      performance relative to more commonly used sigmoidal nonlinearities. In this
      work, we explore the use of deep recti\ufb01er networks as acoustic models for
      the 300 hour Switchboard conversational speech recognition task. Using simple
      training procedures without pretraining, networks with recti\ufb01er nonlinearities
      produce 2% absolute reductions in word error rates over their sigmoidal counterparts.
      We analyze hidden layer representations to quantify di\ufb00erences in how ReL
      units encode inputs as compared to sigmoidal units. Finally, we evaluate a variant
      of the ReL unit with a gradient more amenable to optimization in an attempt
      to further improve deep recti\ufb01er networks.", "venue": "", "year": 2013,
      "referenceCount": 14, "citationCount": 5857, "influentialCitationCount": 773,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      null, "publicationDate": null, "journal": {"volume": "", "name": ""}, "authors":
      [{"authorId": "34961461", "name": "Andrew L. Maas"}]}}, {"intents": ["methodology"],
      "isInfluential": true, "contexts": ["48% on the StreetView House Numbers dataset
      (SVHN) [47], whereas a purely supervised CNN with the same architecture achieved",
      "DCGAN yields a classi\ufb01cation error of 22.48% on the StreetView House Numbers
      dataset (SVHN) [47], whereas a purely supervised CNN with the same architecture
      achieved a signi\ufb01cantly higher 28.87% test error."], "citedPaper": {"paperId":
      "02227c94dd41fe0b439e050d377b0beb5d427cda", "externalIds": {"MAG": "2335728318",
      "CorpusId": 16852518}, "corpusId": 16852518, "publicationVenue": null, "url":
      "https://www.semanticscholar.org/paper/02227c94dd41fe0b439e050d377b0beb5d427cda",
      "title": "Reading Digits in Natural Images with Unsupervised Feature Learning",
      "abstract": "Detecting and reading text from natural images is a hard computer
      vision task that is central to a variety of emerging applications. Related problems
      like document character recognition have been widely studied by computer vision
      and machine learning researchers and are virtually solved for practical applications
      like reading handwritten digits. Reliably recognizing characters in more complex
      scenes like photographs, however, is far more difficult: the best existing methods
      lag well behind human performance on the same tasks. In this paper we attack
      the problem of recognizing digits in a real application using unsupervised feature
      learning methods: reading house numbers from street level photos. To this end,
      we introduce a new benchmark dataset for research use containing over 600,000
      labeled digits cropped from Street View images. We then demonstrate the difficulty
      of recognizing these digits when the problem is approached with hand-designed
      features. Finally, we employ variants of two recently proposed unsupervised
      feature learning methods and find that they are convincingly superior on our
      benchmarks.", "venue": "", "year": 2011, "referenceCount": 28, "citationCount":
      5494, "influentialCitationCount": 1918, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null,
      "journal": {"volume": "", "name": ""}, "authors": [{"authorId": "34180232",
      "name": "Yuval Netzer"}, {"authorId": "2156632012", "name": "Tao Wang"}, {"authorId":
      "144638694", "name": "Adam Coates"}, {"authorId": "1726358", "name": "A. Bissacco"},
      {"authorId": "144397975", "name": "Bo Wu"}, {"authorId": "34699434", "name":
      "A. Ng"}]}}, {"intents": [], "isInfluential": false, "contexts": [], "citedPaper":
      {"paperId": "7789ce16348a1a4d85f17b6b3b08cfb9cb9c5f0f", "externalIds": {"DBLP":
      "conf/shape/1999", "MAG": "1547531277", "DOI": "10.1007/3-540-46805-6", "CorpusId":
      29941054}, "corpusId": 29941054, "publicationVenue": {"id": "2f5d0e8a-faad-4f10-b323-2b2e3c439a78",
      "name": "Lecture Notes in Computer Science", "type": "journal", "alternate_names":
      ["LNCS", "Transactions on Computational Systems Biology", "Trans Comput Syst
      Biology", "Lect Note Comput Sci"], "issn": "0302-9743", "alternate_issns": ["1861-2059",
      "1861-2075", "1866-4733"], "url": "http://www.springer.com/lncs", "alternate_urls":
      ["http://www.springer.com/sgw/cda/frontpage/0,11855,1-164-2-73659-0,00.html",
      "https://link.springer.com/bookseries/558", "http://link.springer.com/search?query=\"Transactions+on+Computational+Systems+Biology\""]},
      "url": "https://www.semanticscholar.org/paper/7789ce16348a1a4d85f17b6b3b08cfb9cb9c5f0f",
      "title": "Shape, Contour and Grouping in Computer Vision", "abstract": null,
      "venue": "Lecture Notes in Computer Science", "year": 1999, "referenceCount":
      414, "citationCount": 355, "influentialCitationCount": 15, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
      "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}],
      "publicationTypes": ["Book", "Review"], "publicationDate": null, "journal":
      {"name": "Shape, Contour and Grouping in Computer Vision"}, "authors": [{"authorId":
      "144016256", "name": "D. Forsyth"}, {"authorId": "3453447", "name": "J. Mundy"},
      {"authorId": "46356795", "name": "V. di Ges\u00fa"}, {"authorId": "1745672",
      "name": "R. Cipolla"}]}}, {"intents": [], "isInfluential": false, "contexts":
      ["CNNs: Convolutional neural networks (CNNs) were proposed by LeCun et al. [37];
      These networks consist of trained spatial \ufb01lters applied on hidden activations
      throughout their architecture."], "citedPaper": {"paperId": "9a5ea367f0fb05805acaa84a402f5d036eea37dc",
      "externalIds": {"MAG": "1928278792", "DBLP": "conf/shape/CunHBB99", "DOI": "10.1007/3-540-46805-6_19",
      "CorpusId": 37392765}, "corpusId": 37392765, "publicationVenue": {"id": "c762bd16-49e0-4924-af92-220ea09c53e8",
      "name": "Shape, Contour and Grouping in Computer Vision", "type": "conference",
      "alternate_names": ["Shape Contour Group Comput Vis"]}, "url": "https://www.semanticscholar.org/paper/9a5ea367f0fb05805acaa84a402f5d036eea37dc",
      "title": "Object Recognition with Gradient-Based Learning", "abstract": null,
      "venue": "Shape, Contour and Grouping in Computer Vision", "year": 1999, "referenceCount":
      36, "citationCount": 911, "influentialCitationCount": 92, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": null, "journal": {"pages": "319-"}, "authors": [{"authorId":
      "1688882", "name": "Yann LeCun"}, {"authorId": "1721248", "name": "P. Haffner"},
      {"authorId": "52184096", "name": "L. Bottou"}, {"authorId": "1751762", "name":
      "Yoshua Bengio"}]}}]}

      '
    headers:
      Access-Control-Allow-Origin:
      - '*'
      Connection:
      - keep-alive
      Content-Length:
      - '432406'
      Content-Type:
      - application/json
      Date:
      - Fri, 01 Dec 2023 02:33:50 GMT
      Via:
      - 1.1 a5313514fbb2ef971d07fa7af0982586.cloudfront.net (CloudFront)
      X-Amz-Cf-Id:
      - NlkL6TepxtPcTuAfcC2jomtW0GfSVgONdLn63Vt_lWDKADd2q5ZL3A==
      X-Amz-Cf-Pop:
      - GRU3-P4
      X-Cache:
      - Miss from cloudfront
      x-amz-apigw-id:
      - PPfiLGOSPHcEHxg=
      x-amzn-Remapped-Connection:
      - keep-alive
      x-amzn-Remapped-Content-Length:
      - '432406'
      x-amzn-Remapped-Date:
      - Fri, 01 Dec 2023 02:33:50 GMT
      x-amzn-Remapped-Server:
      - gunicorn
      x-amzn-RequestId:
      - 5add7213-5de1-4a1a-8c40-9bc8d1ecf3e9
    http_version: HTTP/1.1
    status_code: 200
version: 1
