interactions:
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - api.semanticscholar.org
      user-agent:
      - python-httpx/0.24.1
    method: GET
    uri: https://api.semanticscholar.org/graph/v1/paper/CorpusID:1033682/references?&fields=contexts,intents,isInfluential,abstract,authors,citationCount,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=1000
  response:
    content: '{"offset": 0, "data": [{"contexts": [], "isInfluential": false, "intents":
      [], "citedPaper": {"paperId": "a4d513cfc9d4902ef1a80198582f29b8ba46ac28", "externalIds":
      {"DBLP": "journals/corr/abs-1802-07228", "ArXiv": "1802.07228", "MAG": "2787887017",
      "DOI": "10.17863/CAM.22520", "CorpusId": 3385567}, "corpusId": 3385567, "publicationVenue":
      {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
      ["ArXiv"], "issn": "2331-8422", "url": "http://bibpurl.oclc.org/web/7130"},
      "url": "https://www.semanticscholar.org/paper/a4d513cfc9d4902ef1a80198582f29b8ba46ac28",
      "title": "The Malicious Use of Artificial Intelligence: Forecasting, Prevention,
      and Mitigation", "abstract": "The following organisations are named on the report:
      Future of Humanity Institute, University of Oxford, Centre for the Study of
      Existential Risk, University of Cambridge, Center for a New American Security,
      Electronic Frontier Foundation, OpenAI. The Future of Life Institute is acknowledged
      as a funder.", "venue": "arXiv.org", "year": 2018, "referenceCount": 203, "citationCount":
      500, "influentialCitationCount": 23, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2018-02-20", "journal": {"volume": "abs/1802.07228", "name": "ArXiv"}, "authors":
      [{"authorId": "35167962", "name": "Miles Brundage"}, {"authorId": "49344883",
      "name": "S. Avin"}, {"authorId": "2115193883", "name": "Jack Clark"}, {"authorId":
      "48625835", "name": "H. Toner"}, {"authorId": "2654106", "name": "P. Eckersley"},
      {"authorId": "39928654", "name": "Ben Garfinkel"}, {"authorId": "3198576", "name":
      "A. Dafoe"}, {"authorId": "35681920", "name": "P. Scharre"}, {"authorId": "46225273",
      "name": "T. Zeitzoff"}, {"authorId": "7888676", "name": "Bobby Filar"}, {"authorId":
      "2639880", "name": "H. Anderson"}, {"authorId": "47075894", "name": "H. Roff"},
      {"authorId": "2059529715", "name": "Gregory C. Allen"}, {"authorId": "5164568",
      "name": "J. Steinhardt"}, {"authorId": "152629250", "name": "Carrick Flynn"},
      {"authorId": "35793299", "name": "Se\u00e1n \u00d3 h\u00c9igeartaigh"}, {"authorId":
      "38992229", "name": "S. Beard"}, {"authorId": "36729401", "name": "Haydn Belfield"},
      {"authorId": "33859827", "name": "Sebastian Farquhar"}, {"authorId": "39439114",
      "name": "Clare Lyle"}, {"authorId": "35431817", "name": "Rebecca Crootof"},
      {"authorId": "47107786", "name": "Owain Evans"}, {"authorId": "2054564415",
      "name": "Michael Page"}, {"authorId": "2055073817", "name": "Joanna J. Bryson"},
      {"authorId": "26336155", "name": "Roman V. Yampolskiy"}, {"authorId": "2698777",
      "name": "Dario Amodei"}]}}, {"contexts": ["GANs struggle to generate discrete
      data because the back-propagation algorithm needs to propagate gradients from
      the discriminator through the output of the generator, but this problem is being
      gradually resolved.(9) Like most generative models, GANs can be used to fill
      in gaps in missing data."], "isInfluential": false, "intents": ["background"],
      "citedPaper": {"paperId": "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d", "externalIds":
      {"MAG": "2951433039", "DBLP": "journals/corr/abs-1801-07736", "ArXiv": "1801.07736",
      "CorpusId": 3655946}, "corpusId": 3655946, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d",
      "title": "MaskGAN: Better Text Generation via Filling in the ______", "abstract":
      "Neural text generation models are often autoregressive language models or seq2seq
      models. These models generate text by sampling words sequentially, with each
      word conditioned on the previous word, and are state-of-the-art for several
      machine translation and summarization benchmarks. These benchmarks are often
      defined by validation perplexity even though this is not a direct measure of
      the quality of the generated text. Additionally, these models are typically
      trained via maxi- mum likelihood and teacher forcing. These methods are well-suited
      to optimizing perplexity but can result in poor sample quality since generating
      text requires conditioning on sequences of words that may have never been observed
      at training time. We propose to improve sample quality using Generative Adversarial
      Networks (GANs), which explicitly train the generator to produce high quality
      samples and have shown a lot of success in image generation. GANs were originally
      designed to output differentiable values, so discrete language generation is
      challenging for them. We claim that validation perplexity alone is not indicative
      of the quality of text generated by a model. We introduce an actor-critic conditional
      GAN that fills in missing text conditioned on the surrounding context. We show
      qualitatively and quantitatively, evidence that this produces more realistic
      conditional and unconditional text samples compared to a maximum likelihood
      trained model.", "venue": "International Conference on Learning Representations",
      "year": 2018, "referenceCount": 38, "citationCount": 430, "influentialCitationCount":
      29, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2018-01-23", "journal": {"volume": "abs/1801.07736", "name":
      "ArXiv"}, "authors": [{"authorId": "26958176", "name": "W. Fedus"}, {"authorId":
      "153440022", "name": "I. Goodfellow"}, {"authorId": "2555924", "name": "Andrew
      M. Dai"}]}}, {"contexts": ["Very many different specific formulations of these
      costs are possible and so far most popular formulations seem to perform roughly
      the same.(18) In the original version of GANs, J(D) was defined to be the negative
      log-likelihood that the discriminator assigns to the real-vs-fake labels given
      the input to the discriminator."], "isInfluential": false, "intents": ["background"],
      "citedPaper": {"paperId": "c88e8d85fd5160b0793598bda037f977366acf7a", "externalIds":
      {"MAG": "2963873275", "DBLP": "conf/nips/LucicKMGB18", "ArXiv": "1711.10337",
      "CorpusId": 4053393}, "corpusId": 4053393, "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
      "name": "Neural Information Processing Systems", "type": "conference", "alternate_names":
      ["Neural Inf Process Syst", "NeurIPS"], "url": "http://neurips.cc/"}, "url":
      "https://www.semanticscholar.org/paper/c88e8d85fd5160b0793598bda037f977366acf7a",
      "title": "Are GANs Created Equal? A Large-Scale Study", "abstract": "Generative
      adversarial networks (GAN) are a powerful subclass of generative models. Despite
      a very rich research activity leading to numerous interesting GAN algorithms,
      it is still very hard to assess which algorithm(s) perform better than others.
      We conduct a neutral, multi-faceted large-scale empirical study on state-of-the
      art models and evaluation measures. We find that most models can reach similar
      scores with enough hyperparameter optimization and random restarts. This suggests
      that improvements can arise from a higher computational budget and tuning more
      than fundamental algorithmic changes. To overcome some limitations of the current
      metrics, we also propose several data sets on which precision and recall can
      be computed. Our experimental results suggest that future GAN research should
      be based on more systematic and objective evaluation procedures. Finally, we
      did not find evidence that any of the tested algorithms consistently outperforms
      the non-saturating GAN introduced in \\cite{goodfellow2014generative}.", "venue":
      "Neural Information Processing Systems", "year": 2017, "referenceCount": 27,
      "citationCount": 859, "influentialCitationCount": 81, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
      "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2017-11-28", "journal": {"pages": "698-707"}, "authors": [{"authorId": "34302129",
      "name": "Mario Lucic"}, {"authorId": "2006889", "name": "Karol Kurach"}, {"authorId":
      "145605490", "name": "Marcin Michalski"}, {"authorId": "1802148", "name": "S.
      Gelly"}, {"authorId": "1698617", "name": "O. Bousquet"}]}}, {"contexts": [],
      "isInfluential": false, "intents": [], "citedPaper": {"paperId": "744fe47157477235032f7bb3777800f9f2f45e52",
      "externalIds": {"MAG": "2949124434", "DBLP": "conf/iclr/KarrasALL18", "ArXiv":
      "1710.10196", "CorpusId": 3568073}, "corpusId": 3568073, "publicationVenue":
      {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
      on Learning Representations", "type": "conference", "alternate_names": ["Int
      Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/744fe47157477235032f7bb3777800f9f2f45e52",
      "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
      "abstract": "We describe a new training methodology for generative adversarial
      networks. The key idea is to grow both the generator and discriminator progressively:
      starting from a low resolution, we add new layers that model increasingly fine
      details as training progresses. This both speeds the training up and greatly
      stabilizes it, allowing us to produce images of unprecedented quality, e.g.,
      CelebA images at 1024^2. We also propose a simple way to increase the variation
      in generated images, and achieve a record inception score of 8.80 in unsupervised
      CIFAR10. Additionally, we describe several implementation details that are important
      for discouraging unhealthy competition between the generator and discriminator.
      Finally, we suggest a new metric for evaluating GAN results, both in terms of
      image quality and variation. As an additional contribution, we construct a higher-quality
      version of the CelebA dataset.", "venue": "International Conference on Learning
      Representations", "year": 2017, "referenceCount": 67, "citationCount": 5518,
      "influentialCitationCount": 1086, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2017-10-27", "journal": {"volume": "abs/1710.10196",
      "name": "ArXiv"}, "authors": [{"authorId": "2976930", "name": "Tero Karras"},
      {"authorId": "1761103", "name": "Timo Aila"}, {"authorId": "36436218", "name":
      "S. Laine"}, {"authorId": "49244945", "name": "J. Lehtinen"}]}}, {"contexts":
      [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "471908e99d6965f0f6d249c9cd013485dc2b21df",
      "externalIds": {"DBLP": "conf/iclr/FedusRLDMG18", "ArXiv": "1710.08446", "MAG":
      "2953320313", "CorpusId": 3398677}, "corpusId": 3398677, "publicationVenue":
      {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference
      on Learning Representations", "type": "conference", "alternate_names": ["Int
      Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "url": "https://www.semanticscholar.org/paper/471908e99d6965f0f6d249c9cd013485dc2b21df",
      "title": "Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence
      At Every Step", "abstract": "Generative adversarial networks (GANs) are a family
      of generative models that do not minimize a single training criterion. Unlike
      other generative models, the data distribution is learned via a game between
      a generator (the generative model) and a discriminator (a teacher providing
      training signal) that each minimize their own cost. GANs are designed to reach
      a Nash equilibrium at which each player cannot reduce their cost without changing
      the other players'' parameters. One useful approach for the theory of GANs is
      to show that a divergence between the training distribution and the model distribution
      obtains its minimum value at equilibrium. Several recent research directions
      have been motivated by the idea that this divergence is the primary guide for
      the learning process and that every step of learning should decrease the divergence.
      We show that this view is overly restrictive. During GAN training, the discriminator
      provides learning signal in situations where the gradients of the divergences
      between distributions would not be useful. We provide empirical counterexamples
      to the view of GAN training as divergence minimization. Specifically, we demonstrate
      that GANs are able to learn distributions in situations where the divergence
      minimization point of view predicts they would fail. We also show that gradient
      penalties motivated from the divergence minimization perspective are equally
      helpful when applied in other contexts in which the divergence minimization
      perspective does not predict they would be helpful. This contributes to a growing
      body of evidence that GAN training may be more usefully viewed as approaching
      Nash equilibria via trajectories that do not necessarily minimize a specific
      divergence at each step.", "venue": "International Conference on Learning Representations",
      "year": 2017, "referenceCount": 25, "citationCount": 197, "influentialCitationCount":
      23, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
      "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2017-10-23", "journal": {"volume": "abs/1710.08446", "name":
      "ArXiv"}, "authors": [{"authorId": "26958176", "name": "W. Fedus"}, {"authorId":
      "35578586", "name": "Mihaela Rosca"}, {"authorId": "40627523", "name": "Balaji
      Lakshminarayanan"}, {"authorId": "2555924", "name": "Andrew M. Dai"}, {"authorId":
      "14594344", "name": "S. Mohamed"}, {"authorId": "153440022", "name": "I. Goodfellow"}]}},
      {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId":
      "9f605f8d5d6b143b8c34ccd498986a460c32e641", "externalIds": {"DBLP": "conf/iclr/UnterthinerNSKH18",
      "MAG": "2751565964", "ArXiv": "1708.08819", "CorpusId": 7559933}, "corpusId":
      7559933, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/9f605f8d5d6b143b8c34ccd498986a460c32e641",
      "title": "Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields",
      "abstract": "Generative adversarial networks (GANs) evolved into one of the
      most successful unsupervised techniques for generating realistic images. Even
      though it has recently been shown that GAN training converges, GAN models often
      end up in local Nash equilibria that are associated with mode collapse or otherwise
      fail to model the target distribution. We introduce Coulomb GANs, which pose
      the GAN learning problem as a potential field of charged particles, where generated
      samples are attracted to training set samples but repel each other. The discriminator
      learns a potential field while the generator decreases the energy by moving
      its samples along the vector (force) field determined by the gradient of the
      potential field. Through decreasing the energy, the GAN model learns to generate
      samples according to the whole target distribution and does not only cover some
      of its modes. We prove that Coulomb GANs possess only one Nash equilibrium which
      is optimal in the sense that the model distribution equals the target distribution.
      We show the efficacy of Coulomb GANs on a variety of image datasets. On LSUN
      and celebA, Coulomb GANs set a new state of the art and produce a previously
      unseen variety of different samples.", "venue": "International Conference on
      Learning Representations", "year": 2017, "referenceCount": 43, "citationCount":
      70, "influentialCitationCount": 12, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2017-08-29", "journal":
      {"volume": "abs/1708.08819", "name": "ArXiv"}, "authors": [{"authorId": "2465270",
      "name": "Thomas Unterthiner"}, {"authorId": "37082831", "name": "Bernhard Nessler"},
      {"authorId": "1994964", "name": "G. Klambauer"}, {"authorId": "2445103", "name":
      "M. Heusel"}, {"authorId": "19219270", "name": "Hubert Ramsauer"}, {"authorId":
      "3308557", "name": "S. Hochreiter"}]}}, {"contexts": [], "isInfluential": false,
      "intents": [], "citedPaper": {"paperId": "041ed3b277e5852a28acd23740b0772a7ce3c6ef",
      "externalIds": {"PubMedCentral": "7041894", "MAG": "2961396908", "DOI": "10.1161/CIRCOUTCOMES.118.005122",
      "CorpusId": 23808696, "PubMed": "31284738"}, "corpusId": 23808696, "publicationVenue":
      {"id": "027ffd21-ebb0-4af8-baf5-911124292fd0", "name": "bioRxiv", "type": "journal",
      "url": "http://biorxiv.org/"}, "url": "https://www.semanticscholar.org/paper/041ed3b277e5852a28acd23740b0772a7ce3c6ef",
      "title": "Privacy-Preserving Generative Deep Neural Networks Support Clinical
      Data Sharing", "abstract": "Background Data sharing accelerates scientific progress
      but sharing individual level data while preserving patient privacy presents
      a barrier. Methods and Results Using pairs of deep neural networks, we generated
      simulated, synthetic \u201cparticipants\u201d that closely resemble participants
      of the SPRINT trial. We showed that such paired networks can be trained with
      differential privacy, a formal privacy framework that limits the likelihood
      that queries of the synthetic participants\u2019 data could identify a real
      a participant in the trial. Machine-learning predictors built on the synthetic
      population generalize to the original dataset. This finding suggests that the
      synthetic data can be shared with others, enabling them to perform hypothesis-generating
      analyses as though they had the original trial data. Conclusions Deep neural
      networks that generate synthetic participants facilitate secondary analyses
      and reproducible investigation of clinical datasets by enhancing data sharing
      while preserving participant privacy.", "venue": "bioRxiv", "year": 2017, "referenceCount":
      47, "citationCount": 322, "influentialCitationCount": 19, "isOpenAccess": true,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Biology", "Medicine"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Biology", "source": "external"}, {"category": "Medicine", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2017-07-05", "journal": {"volume": "12",
      "pages": "e005122 - e005122", "name": "Circulation. Cardiovascular Quality and
      Outcomes"}, "authors": [{"authorId": "1402651503", "name": "B. Beaulieu-Jones"},
      {"authorId": "1768074", "name": "Zhiwei Steven Wu"}, {"authorId": "2110521722",
      "name": "Christopher J. Williams"}, {"authorId": "2110703328", "name": "Ran
      Lee"}, {"authorId": "10301085", "name": "S. Bhavnani"}, {"authorId": "6031233",
      "name": "James Brian Byrd"}, {"authorId": "2104940", "name": "C. Greene"}]}},
      {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId":
      "21b25b025898bd1cabe60234434b49cf14016981", "externalIds": {"MAG": "2951897565",
      "ArXiv": "1706.04156", "DBLP": "conf/nips/NagarajanK17", "CorpusId": 8424807},
      "corpusId": 8424807, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/21b25b025898bd1cabe60234434b49cf14016981",
      "title": "Gradient descent GAN optimization is locally stable", "abstract":
      "Despite the growing prominence of generative adversarial networks (GANs), optimization
      in GANs is still a poorly understood topic. In this paper, we analyze the \"gradient
      descent\" form of GAN optimization i.e., the natural setting where we simultaneously
      take small gradient steps in both generator and discriminator parameters. We
      show that even though GAN optimization does not correspond to a convex-concave
      game (even for simple parameterizations), under proper conditions, equilibrium
      points of this optimization procedure are still \\emph{locally asymptotically
      stable} for the traditional GAN formulation. On the other hand, we show that
      the recently proposed Wasserstein GAN can have non-convergent limit cycles near
      equilibrium. Motivated by this stability analysis, we propose an additional
      regularization term for gradient descent GAN updates, which \\emph{is} able
      to guarantee local stability for both the WGAN and the traditional GAN, and
      also shows practical promise in speeding up convergence and addressing mode
      collapse.", "venue": "NIPS", "year": 2017, "referenceCount": 25, "citationCount":
      312, "influentialCitationCount": 46, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-06-13",
      "journal": {"pages": "5585-5595"}, "authors": [{"authorId": "34602162", "name":
      "Vaishnavh Nagarajan"}, {"authorId": "145116464", "name": "J. Z. Kolter"}]}},
      {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId":
      "459fbc416eb9a55920645c741b1e4cce95f39786", "externalIds": {"ArXiv": "1705.10461",
      "DBLP": "conf/nips/MeschederNG17", "MAG": "2951567597", "CorpusId": 2349418},
      "corpusId": 2349418, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/459fbc416eb9a55920645c741b1e4cce95f39786",
      "title": "The Numerics of GANs", "abstract": "In this paper, we analyze the
      numerics of common algorithms for training Generative Adversarial Networks (GANs).
      Using the formalism of smooth two-player games we analyze the associated gradient
      vector field of GAN training objectives. Our findings suggest that the convergence
      of current algorithms suffers due to two factors: i) presence of eigenvalues
      of the Jacobian of the gradient vector field with zero real-part, and ii) eigenvalues
      with big imaginary part. Using these findings, we design a new algorithm that
      overcomes some of these limitations and has better convergence properties. Experimentally,
      we demonstrate its superiority on training common GAN architectures and show
      convergence on GAN architectures that are known to be notoriously hard to train.",
      "venue": "NIPS", "year": 2017, "referenceCount": 30, "citationCount": 408, "influentialCitationCount":
      75, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2017-05-30", "journal":
      {"volume": "abs/1705.10461", "name": "ArXiv"}, "authors": [{"authorId": "8226549",
      "name": "L. Mescheder"}, {"authorId": "2388416", "name": "S. Nowozin"}, {"authorId":
      "47237027", "name": "Andreas Geiger"}]}}, {"contexts": [], "isInfluential":
      false, "intents": [], "citedPaper": {"paperId": "bc995457cf5f4b2b5ef62106856571588d7d70f2",
      "externalIds": {"MAG": "2615429765", "DBLP": "journals/corr/DanihelkaLUWD17",
      "ArXiv": "1705.05263", "CorpusId": 1564507}, "corpusId": 1564507, "publicationVenue":
      {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
      ["ArXiv"], "issn": "2331-8422", "url": "http://bibpurl.oclc.org/web/7130"},
      "url": "https://www.semanticscholar.org/paper/bc995457cf5f4b2b5ef62106856571588d7d70f2",
      "title": "Comparison of Maximum Likelihood and GAN-based training of Real NVPs",
      "abstract": "We train a generator by maximum likelihood and we also train the
      same generator architecture by Wasserstein GAN. We then compare the generated
      samples, exact log-probability densities and approximate Wasserstein distances.
      We show that an independent critic trained to approximate Wasserstein distance
      between the validation set and the generator distribution helps detect overfitting.
      Finally, we use ideas from the one-shot learning literature to develop a novel
      fast learning critic.", "venue": "arXiv.org", "year": 2017, "referenceCount":
      19, "citationCount": 52, "influentialCitationCount": 6, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2017-05-15", "journal": {"volume": "abs/1705.05263", "name": "ArXiv"}, "authors":
      [{"authorId": "1841008", "name": "Ivo Danihelka"}, {"authorId": "40627523",
      "name": "Balaji Lakshminarayanan"}, {"authorId": "2825051", "name": "Benigno
      Uria"}, {"authorId": "1688276", "name": "Daan Wierstra"}, {"authorId": "1790646",
      "name": "P. Dayan"}]}}, {"contexts": ["GANs can be used for a variety of interactive
      digital media effects where the end goal is to produce compelling imagery.(35)
      GANs can even be used to solve variational inference problems used in other
      approaches to generative modeling.", "For example, after studying a collection
      of photos of zebras and a collection of photos of horses, GANs can turn a photo
      of a horse into a photo of a zebra.(35) GANs have been used in science to simulate
      experiments that would be costly to run even in traditional software simulators."],
      "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId":
      "c43d954cf8133e6254499f3d68e45218067e4941", "externalIds": {"MAG": "2951488027",
      "DBLP": "journals/corr/ZhuPIE17", "ArXiv": "1703.10593", "DOI": "10.1109/ICCV.2017.244",
      "CorpusId": 206770979}, "corpusId": 206770979, "publicationVenue": {"id": "7654260e-79f9-45c5-9663-d72027cf88f3",
      "name": "IEEE International Conference on Computer Vision", "type": "conference",
      "alternate_names": ["ICCV", "IEEE Int Conf Comput Vis", "ICCV Workshops", "ICCV
      Work"], "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"},
      "url": "https://www.semanticscholar.org/paper/c43d954cf8133e6254499f3d68e45218067e4941",
      "title": "Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial
      Networks", "abstract": "Image-to-image translation is a class of vision and
      graphics problems where the goal is to learn the mapping between an input image
      and an output image using a training set of aligned image pairs. However, for
      many tasks, paired training data will not be available. We present an approach
      for learning to translate an image from a source domain X to a target domain
      Y in the absence of paired examples. Our goal is to learn a mapping G : X \u2192
      Y such that the distribution of images from G(X) is indistinguishable from the
      distribution Y using an adversarial loss. Because this mapping is highly under-constrained,
      we couple it with an inverse mapping F : Y \u2192 X and introduce a cycle consistency
      loss to push F(G(X)) \u2248 X (and vice versa). Qualitative results are presented
      on several tasks where paired training data does not exist, including collection
      style transfer, object transfiguration, season transfer, photo enhancement,
      etc. Quantitative comparisons against several prior methods demonstrate the
      superiority of our approach.", "venue": "IEEE International Conference on Computer
      Vision", "year": 2017, "referenceCount": 69, "citationCount": 4182, "influentialCitationCount":
      626, "isOpenAccess": true, "openAccessPdf": {"url": "https://repositorio.unal.edu.co/bitstream/unal/82529/2/98562187.2022.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2017-03-30", "journal": {"pages": "2242-2251", "name": "2017
      IEEE International Conference on Computer Vision (ICCV)"}, "authors": [{"authorId":
      "2436356", "name": "Jun-Yan Zhu"}, {"authorId": "2071929129", "name": "Taesung
      Park"}, {"authorId": "2094770", "name": "Phillip Isola"}, {"authorId": "1763086",
      "name": "Alexei A. Efros"}]}}, {"contexts": [], "isInfluential": false, "intents":
      [], "citedPaper": {"paperId": "641165c959554d8f03314778bd6dfb581d9a469e", "externalIds":
      {"DBLP": "conf/icml/Arora0LMZ17", "ArXiv": "1703.00573", "MAG": "2952745707",
      "CorpusId": 13141061}, "corpusId": 13141061, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/641165c959554d8f03314778bd6dfb581d9a469e",
      "title": "Generalization and Equilibrium in Generative Adversarial Nets (GANs)",
      "abstract": "We show that training of generative adversarial network (GAN) may
      not have good generalization properties; e.g., training may appear successful
      but the trained distribution may be far from target distribution in standard
      metrics. However, generalization does occur for a weaker metric called neural
      net distance. It is also shown that an approximate pure equilibrium exists in
      the discriminator/generator game for a special class of generators with natural
      training objectives when generator capacity and training set sizes are moderate.
      \nThis existence of equilibrium inspires MIX+GAN protocol, which can be combined
      with any existing GAN training, and empirically shown to improve some of them.",
      "venue": "International Conference on Machine Learning", "year": 2017, "referenceCount":
      27, "citationCount": 610, "influentialCitationCount": 91, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2017-03-02", "journal": {"volume": "abs/1703.00573", "name":
      "ArXiv"}, "authors": [{"authorId": "145563459", "name": "Sanjeev Arora"}, {"authorId":
      "144804200", "name": "Rong Ge"}, {"authorId": "40609253", "name": "Yingyu Liang"},
      {"authorId": "1901958", "name": "Tengyu Ma"}, {"authorId": "2153912718", "name":
      "Yi Zhang"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper":
      {"paperId": "633f478bf07ee5fbee9388df84b07c87cc190ae9", "externalIds": {"DBLP":
      "journals/corr/SchawinskiZZFS17", "ArXiv": "1702.00403", "MAG": "2572438701",
      "DOI": "10.1093/mnrasl/slx008", "CorpusId": 7213940}, "corpusId": 7213940, "publicationVenue":
      {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
      ["ArXiv"], "issn": "2331-8422", "url": "http://bibpurl.oclc.org/web/7130"},
      "url": "https://www.semanticscholar.org/paper/633f478bf07ee5fbee9388df84b07c87cc190ae9",
      "title": "Generative Adversarial Networks recover features in astrophysical
      images of galaxies beyond the deconvolution limit", "abstract": "Observations
      of astrophysical objects such as galaxies are limited by various sources of
      random and systematic noise from the sky background, the optical system of the
      telescope and the detector used to record the data. Conventional deconvolution
      techniques are limited in their ability to recover features in imaging data
      by the Shannon-Nyquist sampling theorem. Here we train a generative adversarial
      network (GAN) on a sample of $4,550$ images of nearby galaxies at $0.01<z<0.02$
      from the Sloan Digital Sky Survey and conduct $10\\times$ cross validation to
      evaluate the results. We present a method using a GAN trained on galaxy images
      that can recover features from artificially degraded images with worse seeing
      and higher noise than the original with a performance which far exceeds simple
      deconvolution. The ability to better recover detailed features such as galaxy
      morphology from low-signal-to-noise and low angular resolution imaging data
      significantly increases our ability to study existing data sets of astrophysical
      objects as well as future observations with observatories such as the Large
      Synoptic Sky Telescope (LSST) and the Hubble and James Webb space telescopes.",
      "venue": "arXiv.org", "year": 2017, "referenceCount": 11, "citationCount": 160,
      "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url":
      "https://academic.oup.com/mnrasl/article-pdf/467/1/L110/10730451/slx008.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science", "Physics", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Physics", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Physics", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Review"], "publicationDate": "2017-02-01", "journal": {"volume":
      "abs/1702.00403", "name": "ArXiv"}, "authors": [{"authorId": "6552704", "name":
      "K. Schawinski"}, {"authorId": "1776014", "name": "Ce Zhang"}, {"authorId":
      "2016429687", "name": "Hantian Zhang"}, {"authorId": "2105910294", "name": "Lucas
      Fowler"}, {"authorId": "9540353", "name": "G. Santhanam"}]}}, {"contexts": ["GANs
      have been used in science to simulate experiments that would be costly to run
      even in traditional software simulators.(7) GANs can be used to create fake
      data to train other machine learning models, either when real data would be
      hard to acquire(30) or when there would be privacy concerns associated with
      real data."], "isInfluential": false, "intents": ["methodology"], "citedPaper":
      {"paperId": "7ecd7c88018d8fd27194b8ae7bf48b6a9dac9823", "externalIds": {"MAG":
      "3098389423", "ArXiv": "1701.05927", "DOI": "10.1007/s41781-017-0004-6", "CorpusId":
      88514467}, "corpusId": 88514467, "publicationVenue": {"id": "3a6a506a-7577-4f58-be4a-b4f786c2b892",
      "name": "Computing and Software for Big Science", "alternate_names": ["Comput
      Softw Big Sci"], "issn": "2510-2044", "url": "https://link.springer.com/journal/41781"},
      "url": "https://www.semanticscholar.org/paper/7ecd7c88018d8fd27194b8ae7bf48b6a9dac9823",
      "title": "Learning Particle Physics by Example: Location-Aware Generative Adversarial
      Networks for Physics Synthesis", "abstract": null, "venue": "Computing and Software
      for Big Science", "year": 2017, "referenceCount": 46, "citationCount": 253,
      "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url":
      "https://arxiv.org/pdf/1701.05927", "status": null}, "fieldsOfStudy": ["Physics",
      "Mathematics"], "s2FieldsOfStudy": [{"category": "Physics", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}, {"category": "Physics", "source": "s2-fos-model"}],
      "publicationTypes": null, "publicationDate": "2017-01-20", "journal": {"volume":
      "1", "name": "Computing and Software for Big Science"}, "authors": [{"authorId":
      "8972778", "name": "Luke de Oliveira"}, {"authorId": "35550664", "name": "Michela
      Paganini"}, {"authorId": "3085579", "name": "B. Nachman"}]}}, {"contexts": [],
      "isInfluential": false, "intents": [], "citedPaper": {"paperId": "29831b8830e278c8c28e45c8e9c41c619c89f86a",
      "externalIds": {"MAG": "2579277680", "DBLP": "conf/icml/MeschederNG17", "ArXiv":
      "1701.04722", "CorpusId": 605416}, "corpusId": 605416, "publicationVenue": {"id":
      "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference on
      Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int Conf
      Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/29831b8830e278c8c28e45c8e9c41c619c89f86a",
      "title": "Adversarial Variational Bayes: Unifying Variational Autoencoders and
      Generative Adversarial Networks", "abstract": "Variational Autoencoders (VAEs)
      are expressive latent variable models that can be used to learn complex probability
      distributions from training data. However, the quality of the resulting model
      crucially relies on the expressiveness of the inference model. We introduce
      Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders
      with arbitrarily expressive inference models. We achieve this by introducing
      an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem
      as a two-player game, hence establishing a principled connection between VAEs
      and Generative Adversarial Networks (GANs). We show that in the nonparametric
      limit our method yields an exact maximum-likelihood assignment for the parameters
      of the generative model, as well as the exact posterior distribution over the
      latent variables given an observation. Contrary to competing approaches which
      combine VAEs with GANs, our approach has a clear theoretical justification,
      retains most advantages of standard Variational Autoencoders and is easy to
      implement.", "venue": "International Conference on Machine Learning", "year":
      2017, "referenceCount": 45, "citationCount": 465, "influentialCitationCount":
      60, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
      "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2017-01-17", "journal": {"volume": "abs/1701.04722",
      "name": "ArXiv"}, "authors": [{"authorId": "8226549", "name": "L. Mescheder"},
      {"authorId": "2388416", "name": "S. Nowozin"}, {"authorId": "47237027", "name":
      "Andreas Geiger"}]}}, {"contexts": [], "isInfluential": false, "intents": [],
      "citedPaper": {"paperId": "68cb9fce1e6af2740377494350b650533c9a29e1", "externalIds":
      {"DBLP": "journals/corr/ShrivastavaPTSW16", "MAG": "2567101557", "ArXiv": "1612.07828",
      "DOI": "10.1109/CVPR.2017.241", "CorpusId": 8229065}, "corpusId": 8229065, "publicationVenue":
      {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf", "name": "Computer Vision and
      Pattern Recognition", "type": "conference", "alternate_names": ["CVPR", "Comput
      Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
      "url": "https://www.semanticscholar.org/paper/68cb9fce1e6af2740377494350b650533c9a29e1",
      "title": "Learning from Simulated and Unsupervised Images through Adversarial
      Training", "abstract": "With recent progress in graphics, it has become more
      tractable to train models on synthetic images, potentially avoiding the need
      for expensive annotations. However, learning from synthetic images may not achieve
      the desired performance due to a gap between synthetic and real image distributions.
      To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where
      the task is to learn a model to improve the realism of a simulators output using
      unlabeled real data, while preserving the annotation information from the simulator.
      We develop a method for S+U learning that uses an adversarial network similar
      to Generative Adversarial Networks (GANs), but with synthetic images as inputs
      instead of random vectors. We make several key modifications to the standard
      GAN algorithm to preserve annotations, avoid artifacts, and stabilize training:
      (i) a self-regularization term, (ii) a local adversarial loss, and (iii) updating
      the discriminator using a history of refined images. We show that this enables
      generation of highly realistic images, which we demonstrate both qualitatively
      and with a user study. We quantitatively evaluate the generated images by training
      models for gaze estimation and hand pose estimation. We show a significant improvement
      over using synthetic images, and achieve state-of-the-art results on the MPIIGaze
      dataset without any labeled real data.", "venue": "Computer Vision and Pattern
      Recognition", "year": 2016, "referenceCount": 55, "citationCount": 1649, "influentialCitationCount":
      135, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1612.07828",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2016-12-22", "journal": {"pages": "2242-2251", "name": "2017
      IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"}, "authors":
      [{"authorId": "1490900960", "name": "A. Shrivastava"}, {"authorId": "1945962",
      "name": "Tomas Pfister"}, {"authorId": "2577513", "name": "Oncel Tuzel"}, {"authorId":
      "49158771", "name": "J. Susskind"}, {"authorId": "2108465550", "name": "Wenda
      Wang"}, {"authorId": "51138986", "name": "Russ Webb"}]}}, {"contexts": [], "isInfluential":
      false, "intents": [], "citedPaper": {"paperId": "858dc7408c27702ec42778599fc8d11f73ef3f76",
      "externalIds": {"MAG": "2561050497", "ArXiv": "1611.04273", "DBLP": "journals/corr/WuBSG16",
      "CorpusId": 13583585}, "corpusId": 13583585, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/858dc7408c27702ec42778599fc8d11f73ef3f76",
      "title": "On the Quantitative Analysis of Decoder-Based Generative Models",
      "abstract": "The past several years have seen remarkable progress in generative
      models which produce convincing samples of images and other modalities. A shared
      component of many powerful generative models is a decoder network, a parametric
      deep neural net that defines a generative distribution. Examples include variational
      autoencoders, generative adversarial networks, and generative moment matching
      networks. Unfortunately, it can be difficult to quantify the performance of
      these models because of the intractability of log-likelihood estimation, and
      inspecting samples can be misleading. We propose to use Annealed Importance
      Sampling for evaluating log-likelihoods for decoder-based models and validate
      its accuracy using bidirectional Monte Carlo. The evaluation code is provided
      at this https URL. Using this technique, we analyze the performance of decoder-based
      models, the effectiveness of existing log-likelihood estimators, the degree
      of overfitting, and the degree to which these models miss important modes of
      the data distribution.", "venue": "International Conference on Learning Representations",
      "year": 2016, "referenceCount": 38, "citationCount": 210, "influentialCitationCount":
      29, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2016-11-04", "journal": {"volume": "abs/1611.04273",
      "name": "ArXiv"}, "authors": [{"authorId": "3374063", "name": "Yuhuai Wu"},
      {"authorId": "3080409", "name": "Yuri Burda"}, {"authorId": "145124475", "name":
      "R. Salakhutdinov"}, {"authorId": "1785346", "name": "R. Grosse"}]}}, {"contexts":
      [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "488bb25e0b1777847f04c943e6dbc4f84415b712",
      "externalIds": {"MAG": "2554314924", "ArXiv": "1611.02163", "DBLP": "journals/corr/MetzPPS16",
      "CorpusId": 6610705}, "corpusId": 6610705, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/488bb25e0b1777847f04c943e6dbc4f84415b712",
      "title": "Unrolled Generative Adversarial Networks", "abstract": "We introduce
      a method to stabilize Generative Adversarial Networks (GANs) by defining the
      generator objective with respect to an unrolled optimization of the discriminator.
      This allows training to be adjusted between using the optimal discriminator
      in the generator''s objective, which is ideal but infeasible in practice, and
      using the current value of the discriminator, which is often unstable and leads
      to poor solutions. We show how this technique solves the common problem of mode
      collapse, stabilizes training of GANs with complex recurrent generators, and
      increases diversity and coverage of the data distribution by the generator.",
      "venue": "International Conference on Learning Representations", "year": 2016,
      "referenceCount": 55, "citationCount": 881, "influentialCitationCount": 115,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science",
      "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2016-11-04", "journal": {"volume": "abs/1611.02163", "name":
      "ArXiv"}, "authors": [{"authorId": "2096458", "name": "Luke Metz"}, {"authorId":
      "16443937", "name": "Ben Poole"}, {"authorId": "144846367", "name": "David Pfau"},
      {"authorId": "1407546424", "name": "Jascha Narain Sohl-Dickstein"}]}}, {"contexts":
      [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "ecc0edd450ae7e52f65ddf61405b30ad6dbabdd7",
      "externalIds": {"ArXiv": "1610.09585", "MAG": "2950776302", "DBLP": "journals/corr/OdenaOS16",
      "CorpusId": 1099052}, "corpusId": 1099052, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/ecc0edd450ae7e52f65ddf61405b30ad6dbabdd7",
      "title": "Conditional Image Synthesis with Auxiliary Classifier GANs", "abstract":
      "In this paper we introduce new methods for the improved training of generative
      adversarial networks (GANs) for image synthesis. We construct a variant of GANs
      employing label conditioning that results in 128 x 128 resolution image samples
      exhibiting global coherence. We expand on previous work for image quality assessment
      to provide two new analyses for assessing the discriminability and diversity
      of samples from class-conditional image synthesis models. These analyses demonstrate
      that high resolution samples provide class information not present in low resolution
      samples. Across 1000 ImageNet classes, 128 x 128 samples are more than twice
      as discriminable as artificially resized 32 x 32 samples. In addition, 84.7%
      of the classes have samples exhibiting diversity comparable to real ImageNet
      data.", "venue": "International Conference on Machine Learning", "year": 2016,
      "referenceCount": 45, "citationCount": 2683, "influentialCitationCount": 439,
      "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science",
      "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source":
      "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2016-10-30", "journal": {"pages": "2642-2651"},
      "authors": [{"authorId": "2624088", "name": "Augustus Odena"}, {"authorId":
      "37232298", "name": "C. Olah"}, {"authorId": "1789737", "name": "Jonathon Shlens"}]}},
      {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId":
      "ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1", "externalIds": {"MAG": "2520707650",
      "DBLP": "journals/corr/VondrickPT16", "ArXiv": "1609.02612", "DOI": "10.13016/M26GIH-TNYZ",
      "CorpusId": 9933254}, "corpusId": 9933254, "publicationVenue": null, "url":
      "https://www.semanticscholar.org/paper/ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1",
      "title": "Generating Videos with Scene Dynamics", "abstract": "We capitalize
      on large amounts of unlabeled video in order to learn a model of scene dynamics
      for both video recognition tasks (e.g. action classification) and video generation
      tasks (e.g. future prediction). We propose a generative adversarial network
      for video with a spatio-temporal convolutional architecture that untangles the
      scene''s foreground from the background. Experiments suggest this model can
      generate tiny videos up to a second at full frame rate better than simple baselines,
      and we show its utility at predicting plausible futures of static images. Moreover,
      experiments and visualizations show the model internally learns useful features
      for recognizing actions with minimal supervision, suggesting scene dynamics
      are a promising signal for representation learning. We believe generative video
      models can impact many applications in video understanding and simulation.",
      "venue": "NIPS", "year": 2016, "referenceCount": 59, "citationCount": 1286,
      "influentialCitationCount": 145, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2016-09-08", "journal": {"pages": "613-621"}, "authors": [{"authorId": "1856025",
      "name": "Carl Vondrick"}, {"authorId": "2367683", "name": "H. Pirsiavash"},
      {"authorId": "143805211", "name": "A. Torralba"}]}}, {"contexts": [], "isInfluential":
      false, "intents": [], "citedPaper": {"paperId": "a1b3d8a94323122d63a1ec31c1d722d30c509cb4",
      "externalIds": {"DBLP": "journals/corr/YehCLHD16", "MAG": "2479644247", "CorpusId":
      15140030}, "corpusId": 15140030, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "http://bibpurl.oclc.org/web/7130"}, "url": "https://www.semanticscholar.org/paper/a1b3d8a94323122d63a1ec31c1d722d30c509cb4",
      "title": "Semantic Image Inpainting with Perceptual and Contextual Losses",
      "abstract": "In this paper, we propose a novel method for image inpainting based
      on a Deep Convolutional Generative Adversarial Network (DCGAN). We define a
      loss function consisting of two parts: (1) a contextual loss that preserves
      similarity between the input corrupted image and the recovered image, and (2)
      a perceptual loss that ensures a perceptually realistic output image. Given
      a corrupted image with missing values, we use back-propagation on this loss
      to map the corrupted image to a smaller latent space. The mapped vector is then
      passed through the generative model to predict the missing content. The proposed
      framework is evaluated on the CelebA and SVHN datasets for two challenging inpainting
      tasks with random 80% corruption and large blocky corruption. Experiments show
      that our method can successfully predict semantic information in the missing
      region and achieve pixel-level photorealism, which is impossible by almost all
      existing methods.", "venue": "arXiv.org", "year": 2016, "referenceCount": 26,
      "citationCount": 350, "influentialCitationCount": 37, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2016-07-26", "journal": {"volume": "abs/1607.07539", "name":
      "ArXiv"}, "authors": [{"authorId": "28919105", "name": "Raymond A. Yeh"}, {"authorId":
      null, "name": "Chen Chen"}, {"authorId": "33494814", "name": "Teck-Yian Lim"},
      {"authorId": "1399115926", "name": "M. Hasegawa-Johnson"}, {"authorId": "1834451",
      "name": "M. Do"}]}}, {"contexts": [], "isInfluential": false, "intents": [],
      "citedPaper": {"paperId": "372bc106c61e7eb004835e85bbfee997409f176a", "externalIds":
      {"DBLP": "journals/corr/0001T16", "MAG": "2471149695", "ArXiv": "1606.07536",
      "CorpusId": 10627900}, "corpusId": 10627900, "publicationVenue": null, "url":
      "https://www.semanticscholar.org/paper/372bc106c61e7eb004835e85bbfee997409f176a",
      "title": "Coupled Generative Adversarial Networks", "abstract": "We propose
      coupled generative adversarial network (CoGAN) for learning a joint distribution
      of multi-domain images. In contrast to the existing approaches, which require
      tuples of corresponding images in different domains in the training set, CoGAN
      can learn a joint distribution without any tuple of corresponding images. It
      can learn a joint distribution with just samples drawn from the marginal distributions.
      This is achieved by enforcing a weight-sharing constraint that limits the network
      capacity and favors a joint distribution solution over a product of marginal
      distributions one. We apply CoGAN to several joint distribution learning tasks,
      including learning a joint distribution of color and depth images, and learning
      a joint distribution of face images with different attributes. For each task
      it successfully learns the joint distribution without any tuple of corresponding
      images. We also demonstrate its applications to domain adaptation and image
      transformation.", "venue": "NIPS", "year": 2016, "referenceCount": 34, "citationCount":
      1456, "influentialCitationCount": 147, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2016-06-24", "journal": {"pages": "469-477"}, "authors":
      [{"authorId": "39793900", "name": "Ming-Yu Liu"}, {"authorId": "2577513", "name":
      "Oncel Tuzel"}]}}, {"contexts": ["34 GANs have proven very effective for learning
      to classify data using very few labeled training examples.(29) Evaluating the
      performance of generative models including GANs is a difficult research area
      in its own right."], "isInfluential": false, "intents": ["background"], "citedPaper":
      {"paperId": "571b0750085ae3d939525e62af510ee2cee9d5ea", "externalIds": {"MAG":
      "2963373786", "ArXiv": "1606.03498", "DBLP": "conf/nips/SalimansGZCRCC16", "CorpusId":
      1687220}, "corpusId": 1687220, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/571b0750085ae3d939525e62af510ee2cee9d5ea",
      "title": "Improved Techniques for Training GANs", "abstract": "We present a
      variety of new architectural features and training procedures that we apply
      to the generative adversarial networks (GANs) framework. We focus on two applications
      of GANs: semi-supervised learning, and the generation of images that humans
      find visually realistic. Unlike most work on generative models, our primary
      goal is not to train a model that assigns high likelihood to test data, nor
      do we require the model to be able to learn well without using any labels. Using
      our new techniques, we achieve state-of-the-art results in semi-supervised classification
      on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed
      by a visual Turing test: our model generates MNIST samples that humans cannot
      distinguish from real data, and CIFAR-10 samples that yield a human error rate
      of 21.3%. We also present ImageNet samples with unprecedented resolution and
      show that our methods enable the model to learn recognizable features of ImageNet
      classes.", "venue": "NIPS", "year": 2016, "referenceCount": 28, "citationCount":
      7079, "influentialCitationCount": 883, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2016-06-10",
      "journal": {"volume": "abs/1606.03498", "name": "ArXiv"}, "authors": [{"authorId":
      "2887364", "name": "Tim Salimans"}, {"authorId": "153440022", "name": "I. Goodfellow"},
      {"authorId": "2563432", "name": "Wojciech Zaremba"}, {"authorId": "34415167",
      "name": "Vicki Cheung"}, {"authorId": "38909097", "name": "Alec Radford"}, {"authorId":
      "41192764", "name": "Xi Chen"}]}}, {"contexts": ["GANs can learn useful embedding
      vectors and discover concepts like gender of human faces without supervision.(27)"],
      "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId":
      "8388f1be26329fa45e5807e968a641ce170ea078", "externalIds": {"MAG": "2963684088",
      "ArXiv": "1511.06434", "DBLP": "journals/corr/RadfordMC15", "CorpusId": 11758569},
      "corpusId": 11758569, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/8388f1be26329fa45e5807e968a641ce170ea078",
      "title": "Unsupervised Representation Learning with Deep Convolutional Generative
      Adversarial Networks", "abstract": "In recent years, supervised learning with
      convolutional networks (CNNs) has seen huge adoption in computer vision applications.
      Comparatively, unsupervised learning with CNNs has received less attention.
      In this work we hope to help bridge the gap between the success of CNNs for
      supervised learning and unsupervised learning. We introduce a class of CNNs
      called deep convolutional generative adversarial networks (DCGANs), that have
      certain architectural constraints, and demonstrate that they are a strong candidate
      for unsupervised learning. Training on various image datasets, we show convincing
      evidence that our deep convolutional adversarial pair learns a hierarchy of
      representations from object parts to scenes in both the generator and discriminator.
      Additionally, we use the learned features for novel tasks - demonstrating their
      applicability as general image representations.", "venue": "International Conference
      on Learning Representations", "year": 2015, "referenceCount": 45, "citationCount":
      12043, "influentialCitationCount": 1832, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy":
      [{"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2015-11-19", "journal":
      {"volume": "abs/1511.06434", "name": "CoRR"}, "authors": [{"authorId": "38909097",
      "name": "Alec Radford"}, {"authorId": "2096458", "name": "Luke Metz"}, {"authorId":
      "2127604", "name": "Soumith Chintala"}]}}, {"contexts": ["GANs are thus great
      for learning in situations where there are many possible correct answers, such
      as predicting the many possible futures that can happen in video generation.(19)
      GANs and GAN-like models can be used to learn to transform data from one domain
      into data from another domain, even without any labeled pairs of examples from
      those domains (e."], "isInfluential": false, "intents": ["background"], "citedPaper":
      {"paperId": "17fa1c2a24ba8f731c8b21f1244463bc4b465681", "externalIds": {"DBLP":
      "journals/corr/MathieuCL15", "MAG": "2949900324", "ArXiv": "1511.05440", "CorpusId":
      205514}, "corpusId": 205514, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/17fa1c2a24ba8f731c8b21f1244463bc4b465681",
      "title": "Deep multi-scale video prediction beyond mean square error", "abstract":
      "Learning to predict future images from a video sequence involves the construction
      of an internal representation that models the image evolution accurately, and
      therefore, to some degree, its content and dynamics. This is why pixel-space
      video prediction may be viewed as a promising avenue for unsupervised feature
      learning. In addition, while optical flow has been a very studied problem in
      computer vision for a long time, future frame prediction is rarely approached.
      Still, many vision applications could benefit from the knowledge of the next
      frames of videos, that does not require the complexity of tracking every pixel
      trajectories. In this work, we train a convolutional network to generate future
      frames given an input sequence. To deal with the inherently blurry predictions
      obtained from the standard Mean Squared Error (MSE) loss function, we propose
      three different and complementary feature learning strategies: a multi-scale
      architecture, an adversarial training method, and an image gradient difference
      loss function. We compare our predictions to different published results based
      on recurrent neural networks on the UCF101 dataset", "venue": "International
      Conference on Learning Representations", "year": 2015, "referenceCount": 33,
      "citationCount": 1703, "influentialCitationCount": 205, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2015-11-17", "journal": {"volume": "abs/1511.05440", "name": "CoRR"}, "authors":
      [{"authorId": "143949035", "name": "Micha\u00ebl Mathieu"}, {"authorId": "2341378",
      "name": "C. Couprie"}, {"authorId": "1688882", "name": "Yann LeCun"}]}}, {"contexts":
      [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "39e0c341351f8f4a39ac890b96217c7f4bde5369",
      "externalIds": {"MAG": "2099057450", "ArXiv": "1511.01844", "DBLP": "journals/corr/TheisOB15",
      "CorpusId": 2187805}, "corpusId": 2187805, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/39e0c341351f8f4a39ac890b96217c7f4bde5369",
      "title": "A note on the evaluation of generative models", "abstract": "Probabilistic
      generative models can be used for compression, denoising, inpainting, texture
      synthesis, semi-supervised learning, unsupervised feature learning, and other
      tasks. Given this wide range of applications, it is not surprising that a lot
      of heterogeneity exists in the way these models are formulated, trained, and
      evaluated. As a consequence, direct comparison between models is often difficult.
      This article reviews mostly known but often underappreciated properties relating
      to the evaluation and interpretation of generative models with a focus on image
      models. In particular, we show that three of the currently most commonly used
      criteria---average log-likelihood, Parzen window estimates, and visual fidelity
      of samples---are largely independent of each other when the data is high-dimensional.
      Good performance with respect to one criterion therefore need not imply good
      performance with respect to the other criteria. Our results show that extrapolation
      from one criterion to another is not warranted and generative models need to
      be evaluated directly with respect to the application(s) they were intended
      for. In addition, we provide examples demonstrating that Parzen window estimates
      should generally be avoided.", "venue": "International Conference on Learning
      Representations", "year": 2015, "referenceCount": 38, "citationCount": 971,
      "influentialCitationCount": 64, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category":
      "Mathematics", "source": "external"}, {"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Review"], "publicationDate": "2015-11-05", "journal": {"volume":
      "abs/1511.01844", "name": "CoRR"}, "authors": [{"authorId": "2073063", "name":
      "Lucas Theis"}, {"authorId": "3422336", "name": "A\u00e4ron van den Oord"},
      {"authorId": "1731199", "name": "M. Bethge"}]}}, {"contexts": [], "isInfluential":
      false, "intents": [], "citedPaper": {"paperId": "2904a9932f4cd0f0886121dc1f2d4aaac0455176",
      "externalIds": {"ArXiv": "1502.02761", "DBLP": "journals/corr/LiSZ15", "MAG":
      "2950292946", "CorpusId": 536962}, "corpusId": 536962, "publicationVenue": {"id":
      "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference on
      Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int Conf
      Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/2904a9932f4cd0f0886121dc1f2d4aaac0455176",
      "title": "Generative Moment Matching Networks", "abstract": "We consider the
      problem of learning deep generative models from data. We formulate a method
      that generates an independent sample via a single feedforward pass through a
      multilayer perceptron, as in the recently proposed generative adversarial networks
      (Goodfellow et al., 2014). Training a generative adversarial network, however,
      requires careful optimization of a difficult minimax program. Instead, we utilize
      a technique from statistical hypothesis testing known as maximum mean discrepancy
      (MMD), which leads to a simple objective that can be interpreted as matching
      all orders of statistics between a dataset and samples from the model, and can
      be trained by backpropagation. We further boost the performance of this approach
      by combining our generative network with an auto-encoder network, using MMD
      to learn to generate codes that can then be decoded to produce samples. We show
      that the combination of these techniques yields excellent generative models
      compared to baseline approaches as measured on MNIST and the Toronto Face Database.",
      "venue": "International Conference on Machine Learning", "year": 2015, "referenceCount":
      52, "citationCount": 755, "influentialCitationCount": 122, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2015-02-09", "journal": {"pages": "1718-1727"}, "authors":
      [{"authorId": "47002813", "name": "Yujia Li"}, {"authorId": "1754860", "name":
      "Kevin Swersky"}, {"authorId": "1804104", "name": "R. Zemel"}]}}, {"contexts":
      [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "353ecf7b66b3e9ff5e9f41145a147e899a2eea5c",
      "externalIds": {"MAG": "2125389028", "ArXiv": "1411.1784", "DBLP": "journals/corr/MirzaO14",
      "CorpusId": 12803511}, "corpusId": 12803511, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "http://bibpurl.oclc.org/web/7130"}, "url": "https://www.semanticscholar.org/paper/353ecf7b66b3e9ff5e9f41145a147e899a2eea5c",
      "title": "Conditional Generative Adversarial Nets", "abstract": "Generative
      Adversarial Nets [8] were recently introduced as a novel way to train generative
      models. In this work we introduce the conditional version of generative adversarial
      nets, which can be constructed by simply feeding the data, y, we wish to condition
      on to both the generator and discriminator. We show that this model can generate
      MNIST digits conditioned on class labels. We also illustrate how this model
      could be used to learn a multi-modal model, and provide preliminary examples
      of an application to image tagging in which we demonstrate how this approach
      can generate descriptive tags which are not part of training labels.", "venue":
      "arXiv.org", "year": 2014, "referenceCount": 19, "citationCount": 8142, "influentialCitationCount":
      1285, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2014-11-06", "journal": {"volume": "abs/1411.1784", "name":
      "ArXiv"}, "authors": [{"authorId": "153583218", "name": "Mehdi Mirza"}, {"authorId":
      "2217144", "name": "Simon Osindero"}]}}, {"contexts": ["3 GAN-like models called
      domain-adversarial networks can be used for domain adaptation.(12) GANs can
      be used for a variety of interactive digital media effects where the end goal
      is to produce compelling imagery."], "isInfluential": false, "intents": ["background"],
      "citedPaper": {"paperId": "2530cfc7764bda1330c48c0c8e2cd0e0c671d7e1", "externalIds":
      {"DBLP": "conf/icml/GaninL15", "MAG": "2951688345", "ArXiv": "1409.7495", "CorpusId":
      6755881}, "corpusId": 6755881, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/2530cfc7764bda1330c48c0c8e2cd0e0c671d7e1",
      "title": "Unsupervised Domain Adaptation by Backpropagation", "abstract": "Top-performing
      deep architectures are trained on massive amounts of labeled data. In the absence
      of labeled data for a certain task, domain adaptation often provides an attractive
      option given that labeled data of similar nature but from a different domain
      (e.g. synthetic images) are available. Here, we propose a new approach to domain
      adaptation in deep architectures that can be trained on large amount of labeled
      data from the source domain and large amount of unlabeled data from the target
      domain (no labeled target-domain data is necessary). \nAs the training progresses,
      the approach promotes the emergence of \"deep\" features that are (i) discriminative
      for the main learning task on the source domain and (ii) invariant with respect
      to the shift between the domains. We show that this adaptation behaviour can
      be achieved in almost any feed-forward model by augmenting it with few standard
      layers and a simple new gradient reversal layer. The resulting augmented architecture
      can be trained using standard backpropagation. \nOverall, the approach can be
      implemented with little effort using any of the deep-learning packages. The
      method performs very well in a series of image classification experiments, achieving
      adaptation effect in the presence of big domain shifts and outperforming previous
      state-of-the-art on Office datasets.", "venue": "International Conference on
      Machine Learning", "year": 2014, "referenceCount": 43, "citationCount": 4471,
      "influentialCitationCount": 1049, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category":
      "Mathematics", "source": "external"}, {"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle", "Conference"], "publicationDate": "2014-09-26", "journal":
      {"pages": "1180-1189"}, "authors": [{"authorId": "2825246", "name": "Yaroslav
      Ganin"}, {"authorId": "1740145", "name": "V. Lempitsky"}]}}, {"contexts": ["en
      used ina feedback loop. More recent examples of training a generative machine
      by back-propagating into it include recent work on auto-encoding variational
      Bayes [20] and stochastic backpropagation [24]. 3 Adversarial nets The adversarial
      modeling framework is most straightforward to apply when the models are both
      multilayer perceptrons. To learn the generator\u2019s distribution p g over
      data x, we de\ufb01n"], "isInfluential": false, "intents": ["methodology"],
      "citedPaper": {"paperId": "484ad17c926292fbe0d5211540832a8c8a8e958b", "externalIds":
      {"MAG": "2951275616", "DBLP": "conf/icml/RezendeMW14", "CorpusId": 16895865},
      "corpusId": 16895865, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/484ad17c926292fbe0d5211540832a8c8a8e958b",
      "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative
      Models", "abstract": "We marry ideas from deep neural networks and approximate
      Bayesian inference to derive a generalised class of deep, directed generative
      models, endowed with a new algorithm for scalable inference and learning. Our
      algorithm introduces a recognition model to represent approximate posterior
      distributions, and that acts as a stochastic encoder of the data. We develop
      stochastic back-propagation -- rules for back-propagation through stochastic
      variables -- and use this to develop an algorithm that allows for joint optimisation
      of the parameters of both the generative and recognition model. We demonstrate
      on several real-world data sets that the model generates realistic samples,
      provides accurate imputations of missing data and is a useful tool for high-dimensional
      data visualisation.", "venue": "International Conference on Machine Learning",
      "year": 2014, "referenceCount": 38, "citationCount": 4587, "influentialCitationCount":
      758, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2014-01-16", "journal": {"pages": "1278-1286"},
      "authors": [{"authorId": "1748523", "name": "Danilo Jimenez Rezende"}, {"authorId":
      "14594344", "name": "S. Mohamed"}, {"authorId": "1688276", "name": "Daan Wierstra"}]}},
      {"contexts": ["roblems with unbounded activation when used ina feedback loop.
      More recent examples of training a generative machine by back-propagating into
      it include recent work on auto-encoding variational Bayes [20] and stochastic
      backpropagation [24]. 3 Adversarial nets The adversarial modeling framework
      is most straightforward to apply when the models are both multilayer perceptrons.
      To learn the generator\u2019s d"], "isInfluential": false, "intents": ["methodology"],
      "citedPaper": {"paperId": "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "externalIds":
      {"ArXiv": "1312.6114", "DBLP": "journals/corr/KingmaW13", "MAG": "2951004968",
      "CorpusId": 216078090}, "corpusId": 216078090, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/5f5dc5b9a2ba710937e2c413b37b053cd673df02",
      "title": "Auto-Encoding Variational Bayes", "abstract": "Abstract: How can we
      perform efficient inference and learning in directed probabilistic models, in
      the presence of continuous latent variables with intractable posterior distributions,
      and large datasets? We introduce a stochastic variational inference and learning
      algorithm that scales to large datasets and, under some mild differentiability
      conditions, even works in the intractable case. Our contributions is two-fold.
      First, we show that a reparameterization of the variational lower bound yields
      a lower bound estimator that can be straightforwardly optimized using standard
      stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous
      latent variables per datapoint, posterior inference can be made especially efficient
      by fitting an approximate inference model (also called a recognition model)
      to the intractable posterior using the proposed lower bound estimator. Theoretical
      advantages are reflected in experimental results.", "venue": "International
      Conference on Learning Representations", "year": 2013, "referenceCount": 26,
      "citationCount": 21838, "influentialCitationCount": 4746, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2013-12-20", "journal": {"volume": "abs/1312.6114", "name": "CoRR"}, "authors":
      [{"authorId": "1726807", "name": "Diederik P. Kingma"}, {"authorId": "1678311",
      "name": "M. Welling"}]}}, {"contexts": [" subset of the indices of x by training
      a family of conditional models that share parameters. Essentially, one can use
      adversarial nets to implement a stochastic extension of the deterministic MP-DBM
      [11]. 4. Semi-supervised learning: features from the discriminator or inference
      net could improve performance of classi\ufb01ers when limited labeled data is
      available. 5. Ef\ufb01ciency improvements: training coul"], "isInfluential":
      false, "intents": ["background"], "citedPaper": {"paperId": "5656fa5aa6e1beeb98703fc53ec112ad227c49ca",
      "externalIds": {"DBLP": "conf/nips/GoodfellowMCB13", "MAG": "2098617596", "CorpusId":
      6442575}, "corpusId": 6442575, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/5656fa5aa6e1beeb98703fc53ec112ad227c49ca",
      "title": "Multi-Prediction Deep Boltzmann Machines", "abstract": "We introduce
      the multi-prediction deep Boltzmann machine (MP-DBM). The MP-DBM can be seen
      as a single probabilistic model trained to maximize a variational approximation
      to the generalized pseudolikelihood, or as a family of recurrent nets that share
      parameters and approximately solve different inference problems. Prior methods
      of training DBMs either do not perform well on classification tasks or require
      an initial learning pass that trains the DBM greedily, one layer at a time.
      The MP-DBM does not require greedy layerwise pretraining, and outperforms the
      standard DBM at classification, classification with missing inputs, and mean
      field prediction tasks.1", "venue": "NIPS", "year": 2013, "referenceCount":
      25, "citationCount": 134, "influentialCitationCount": 17, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2013-12-05", "journal": {"pages": "548-556"},
      "authors": [{"authorId": "153440022", "name": "I. Goodfellow"}, {"authorId":
      "153583218", "name": "Mehdi Mirza"}, {"authorId": "1760871", "name": "Aaron
      C. Courville"}, {"authorId": "1751762", "name": "Yoshua Bengio"}]}}, {"contexts":
      [], "isInfluential": false, "intents": [], "citedPaper": {"paperId": "857036a25401c19e484cc32d974c90cd9a46cd66",
      "externalIds": {"DBLP": "conf/allerton/RatliffBS13", "MAG": "2085963752", "DOI":
      "10.1109/Allerton.2013.6736623", "CorpusId": 857142}, "corpusId": 857142, "publicationVenue":
      {"id": "e3e363b2-60f3-46d7-9067-5deaddc3f3f2", "name": "Allerton Conference
      on Communication, Control, and Computing", "type": "conference", "alternate_names":
      ["Allerton", "T Conf Commun Control Comput"]}, "url": "https://www.semanticscholar.org/paper/857036a25401c19e484cc32d974c90cd9a46cd66",
      "title": "Characterization and computation of local Nash equilibria in continuous
      games", "abstract": "We present derivative-based necessary and sufficient conditions
      ensuring player strategies constitute local Nash equilibria in non-cooperative
      continuous games. Our results can be interpreted as generalizations of analogous
      second-order conditions for local optimality from nonlinear programming and
      optimal control theory. Drawing on this analogy, we propose an iterative steepest
      descent algorithm for numerical approximation of local Nash equilibria and provide
      a sufficient condition ensuring local convergence of the algorithm. We demonstrate
      our analytical and computational techniques by computing local Nash equilibria
      in games played on a finite-dimensional differentiable manifold or an infinite-dimensional
      Hilbert space.", "venue": "Allerton Conference on Communication, Control, and
      Computing", "year": 2013, "referenceCount": 26, "citationCount": 179, "influentialCitationCount":
      2, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Economics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2013-10-01", "journal": {"pages": "917-924",
      "name": "2013 51st Annual Allerton Conference on Communication, Control, and
      Computing (Allerton)"}, "authors": [{"authorId": "1785356", "name": "L. Ratliff"},
      {"authorId": "144949860", "name": "Samuel A. Burden"}, {"authorId": "144797536",
      "name": "S. Sastry"}, {"authorId": "2229166654", "name": "bullet Urbain ''s
      Strategy Space"}, {"authorId": "2229161038", "name": "bullet Urbain ''s Cost
      Function"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper":
      {"paperId": "836acf6fc99ebf81d219e2b67f7ab25efc29a6a4", "externalIds": {"MAG":
      "1872489089", "ArXiv": "1308.4214", "DBLP": "journals/corr/GoodfellowWLDMPBBB13",
      "CorpusId": 2172854}, "corpusId": 2172854, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "http://bibpurl.oclc.org/web/7130"}, "url": "https://www.semanticscholar.org/paper/836acf6fc99ebf81d219e2b67f7ab25efc29a6a4",
      "title": "Pylearn2: a machine learning research library", "abstract": "Pylearn2
      is a machine learning research library. This does not just mean that it is a
      collection of machine learning algorithms that share a common API; it means
      that it has been designed for flexibility and extensibility in order to facilitate
      research projects that involve new or unusual use cases. In this paper we give
      a brief history of the library, an overview of its basic philosophy, a summary
      of the library''s architecture, and a description of how the Pylearn2 community
      functions socially.", "venue": "arXiv.org", "year": 2013, "referenceCount":
      54, "citationCount": 305, "influentialCitationCount": 17, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
      "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"],
      "publicationDate": "2013-08-19", "journal": {"volume": "abs/1308.4214", "name":
      "ArXiv"}, "authors": [{"authorId": "153440022", "name": "I. Goodfellow"}, {"authorId":
      "1393680089", "name": "David Warde-Farley"}, {"authorId": "3087941", "name":
      "Pascal Lamblin"}, {"authorId": "3074927", "name": "Vincent Dumoulin"}, {"authorId":
      "153583218", "name": "Mehdi Mirza"}, {"authorId": "1996134", "name": "Razvan
      Pascanu"}, {"authorId": "32837403", "name": "J. Bergstra"}, {"authorId": "3227028",
      "name": "Fr\u00e9d\u00e9ric Bastien"}, {"authorId": "1751762", "name": "Yoshua
      Bengio"}]}}, {"contexts": ["ractable for all but the most trivial instances,
      although they can be estimated by Markov chain Monte Carlo (MCMC) methods. Mixing
      poses a signi\ufb01cant problem for learning algorithms that rely on MCMC [3,
      5]. Deep belief networks (DBNs) [16] are hybrid models containing a single undirected
      layer and several directed layers. While a fast approximate layer-wise training
      criterion exists, DBNs incur the com", "on. This approach has the advantage
      that such machines can be designed to be trained by back-propagation. Prominent
      recent work in this area includes the generative stochastic network (GSN) framework
      [5], which extends generalized denoising auto-encoders [4]: both can be seen
      as de\ufb01ning a parameterized Markov chain, i.e., one learns the parameters
      of a machine that performs one step of a generative M", "ans was obtained by
      cross validation on the validation set. This procedure was introduced in Breuleux
      et al. [8] and used for various generative models for which the exact likelihood
      is not tractable [25, 3, 5]. Results are reported in Table 1. This method of
      estimating the likelihood has somewhat high variance and does not perform well
      in high dimensional spaces but it is the best method available to our k"], "isInfluential":
      false, "intents": ["methodology", "background"], "citedPaper": {"paperId": "5ffa8bf1bf3e39227be28de4ff6915d3b21eb52d",
      "externalIds": {"ArXiv": "1306.1091", "MAG": "2951446714", "DBLP": "journals/corr/BengioT13",
      "CorpusId": 9494295}, "corpusId": 9494295, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/5ffa8bf1bf3e39227be28de4ff6915d3b21eb52d",
      "title": "Deep Generative Stochastic Networks Trainable by Backprop", "abstract":
      "We introduce a novel training principle for probabilistic models that is an
      alternative to maximum likelihood. The proposed Generative Stochastic Networks
      (GSN) framework is based on learning the transition operator of a Markov chain
      whose stationary distribution estimates the data distribution. The transition
      distribution of the Markov chain is conditional on the previous state, generally
      involving a small move, so this conditional distribution has fewer dominant
      modes, being unimodal in the limit of small moves. Thus, it is easier to learn
      because it is easier to approximate its partition function, more like learning
      to perform supervised function approximation, with gradients that can be obtained
      by backprop. We provide theorems that generalize recent work on the probabilistic
      interpretation of denoising autoencoders and obtain along the way an interesting
      justification for dependency networks and generalized pseudolikelihood, along
      with a definition of an appropriate joint distribution and sampling mechanism
      even when the conditionals are not consistent. GSNs can be used with missing
      inputs and can be used to sample subsets of variables given the rest. We validate
      these theoretical results with experiments on two image datasets using an architecture
      that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to
      proceed with simple backprop, without the need for layerwise pretraining.",
      "venue": "International Conference on Machine Learning", "year": 2013, "referenceCount":
      44, "citationCount": 384, "influentialCitationCount": 27, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
      "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2013-06-05", "journal": {"pages": "226-234"}, "authors":
      [{"authorId": "1751762", "name": "Yoshua Bengio"}, {"authorId": "1398746441",
      "name": "Eric Thibodeau-Laufer"}, {"authorId": "1815021", "name": "Guillaume
      Alain"}, {"authorId": "2965424", "name": "J. Yosinski"}]}}, {"contexts": ["can
      be designed to be trained by back-propagation. Prominent recent work in this
      area includes the generative stochastic network (GSN) framework [5], which extends
      generalized denoising auto-encoders [4]: both can be seen as de\ufb01ning a
      parameterized Markov chain, i.e., one learns the parameters of a machine that
      performs one step of a generative Markov chain. Compared to GSNs, the adversarial
      nets fra"], "isInfluential": false, "intents": ["background"], "citedPaper":
      {"paperId": "d9704f8119d6ba748230b4f2ad59f0e8c64fdfb0", "externalIds": {"MAG":
      "2953267151", "DBLP": "conf/nips/BengioYAV13", "ArXiv": "1305.6663", "CorpusId":
      5554756}, "corpusId": 5554756, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/d9704f8119d6ba748230b4f2ad59f0e8c64fdfb0",
      "title": "Generalized Denoising Auto-Encoders as Generative Models", "abstract":
      "Recent work has shown how denoising and contractive autoencoders implicitly
      capture the structure of the data-generating density, in the case where the
      corruption noise is Gaussian, the reconstruction error is the squared error,
      and the data is continuous-valued. This has led to various proposals for sampling
      from this implicitly learned density function, using Langevin and Metropolis-Hastings
      MCMC. However, it remained unclear how to connect the training procedure of
      regularized auto-encoders to the implicit estimation of the underlying data-generating
      distribution when the data are discrete, or using other forms of corruption
      process and reconstruction errors. Another issue is the mathematical justification
      which is only valid in the limit of small corruption noise. We propose here
      a different attack on the problem, which deals with all these issues: arbitrary
      (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood),
      handling both discrete and continuous-valued variables, and removing the bias
      due to non-infinitesimal corruption noise (or non-infinitesimal contractive
      penalty).", "venue": "NIPS", "year": 2013, "referenceCount": 19, "citationCount":
      468, "influentialCitationCount": 36, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2013-05-28",
      "journal": {"volume": "abs/1305.6663", "name": "ArXiv"}, "authors": [{"authorId":
      "1751762", "name": "Yoshua Bengio"}, {"authorId": "145095579", "name": "L. Yao"},
      {"authorId": "1815021", "name": "Guillaume Alain"}, {"authorId": "145467703",
      "name": "Pascal Vincent"}]}}, {"contexts": ["at map a high-dimensional, rich
      sensory input to a class label [14, 22]. These striking successes have primarily
      been based on the backpropagation and dropout algorithms, using piecewise linear
      units [19, 9, 10] which have a particularly well-behaved gradient . Deep generative
      models have had less of an impact, due to the dif\ufb01culty of approximating
      many intractable probabilistic computations that arise in ma", "23], the Toronto
      Face Database (TFD) [28], and CIFAR-10 [21]. The generator nets used a mixture
      of recti\ufb01er linear activations [19, 9] and sigmoid activations, while the
      discriminator net used maxout [10] activations. Dropout [17] was applied in
      training the discriminator net. While our theoretical framework permits the
      use of dropout and other noise at intermediate layers of the generator, we used
      no", "ersarial nets framework does not require a Markov chain for sampling.
      Because adversarial nets do not require feedback loops during generation, they
      are better able to leverage piecewise linear units [19, 9, 10], which improve
      the performance of backpropagation but have problems with unbounded activation
      when used ina feedback loop. More recent examples of training a generative machine
      by back-propagating in"], "isInfluential": false, "intents": ["methodology",
      "background"], "citedPaper": {"paperId": "b7b915d508987b73b61eccd2b237e7ed099a2d29",
      "externalIds": {"DBLP": "conf/icml/GoodfellowWMCB13", "MAG": "3037950864", "ArXiv":
      "1302.4389", "CorpusId": 10600578}, "corpusId": 10600578, "publicationVenue":
      {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
      on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
      Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/b7b915d508987b73b61eccd2b237e7ed099a2d29",
      "title": "Maxout Networks", "abstract": "We consider the problem of designing
      models to leverage a recently introduced approximate model averaging technique
      called dropout. We define a simple new model called maxout (so named because
      its output is the max of a set of inputs, and because it is a natural companion
      to dropout) designed to both facilitate optimization by dropout and improve
      the accuracy of dropout''s fast approximate model averaging technique. We empirically
      verify that the model successfully accomplishes both of these tasks. We use
      maxout and dropout to demonstrate state of the art classification performance
      on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.", "venue":
      "International Conference on Machine Learning", "year": 2013, "referenceCount":
      27, "citationCount": 2017, "influentialCitationCount": 207, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2013-02-18", "journal": {"pages": "1319-1327"}, "authors":
      [{"authorId": "153440022", "name": "I. Goodfellow"}, {"authorId": "1393680089",
      "name": "David Warde-Farley"}, {"authorId": "153583218", "name": "Mehdi Mirza"},
      {"authorId": "1760871", "name": "Aaron C. Courville"}, {"authorId": "1751762",
      "name": "Yoshua Bengio"}]}}, {"contexts": ["s in natural language corpora. So
      far, the most striking successes in deep learning have involved discriminative
      models, usually those that map a high-dimensional, rich sensory input to a class
      label [14, 22]. These striking successes have primarily been based on the backpropagation
      and dropout algorithms, using piecewise linear units [19, 9, 10] which have
      a particularly well-behaved gradient . Deep gene"], "isInfluential": false,
      "intents": ["background"], "citedPaper": {"paperId": "e33cbb25a8c7390aec6a398e36381f4f7770c283",
      "externalIds": {"MAG": "2184045248", "CorpusId": 7230302}, "corpusId": 7230302,
      "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/e33cbb25a8c7390aec6a398e36381f4f7770c283",
      "title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition",
      "abstract": "Most current speech recognition systems use hidden Markov models
      ( HMMs) to deal with the temporal variability of speech and Gaussian mixture
      models to determine how well each state of each HMM fits a frame or a short
      window of frames of coefficients that represents the acoustic input. An alternati
      ve way to evaluate the fit is to use a feedforward neural network that takes
      several frames of coefficients a s input and produces posterior probabilities
      over HMM states as output. Deep neural networks with many hidden layers, that
      are trained using new methods have been shown to outperform Gaussian mixture
      models on a variety of speech rec ognition benchmarks, sometimes by a large
      margin. This paper provides an overview of this progress and repres nts the
      shared views of four research groups who have had recent successes in using
      deep neural networks for a coustic modeling in speech recognition.", "venue":
      "", "year": 2012, "referenceCount": 81, "citationCount": 2399, "influentialCitationCount":
      167, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["Review"], "publicationDate": "2012-11-01", "journal": {"volume": "29", "pages":
      "82", "name": "IEEE Signal Processing Magazine"}, "authors": [{"authorId": "1695689",
      "name": "Geoffrey E. Hinton"}, {"authorId": "144718788", "name": "L. Deng"},
      {"authorId": "144580027", "name": "Dong Yu"}, {"authorId": "35188630", "name":
      "George E. Dahl"}, {"authorId": "40360972", "name": "Abdel-rahman Mohamed"},
      {"authorId": "3111912", "name": "N. Jaitly"}, {"authorId": "33666044", "name":
      "A. Senior"}, {"authorId": "2657155", "name": "Vincent Vanhoucke"}, {"authorId":
      "14902530", "name": "Patrick Nguyen"}, {"authorId": "1784851", "name": "Tara
      N. Sainath"}, {"authorId": "144707379", "name": "Brian Kingsbury"}]}}, {"contexts":
      ["ractable for all but the most trivial instances, although they can be estimated
      by Markov chain Monte Carlo (MCMC) methods. Mixing poses a signi\ufb01cant problem
      for learning algorithms that rely on MCMC [3, 5]. Deep belief networks (DBNs)
      [16] are hybrid models containing a single undirected layer and several directed
      layers. While a fast approximate layer-wise training criterion exists, DBNs
      incur the com", "ability of the test set data under p g by \ufb01tting a Gaussian
      Parzen window to the samples generated with Gand reporting the log-likelihood
      under this distribution. The \u02d9parameter 5 Model MNIST TFD DBN [3] 138 2
      1909 66 Stacked CAE [3] 121 1:6 2110 50 Deep GSN [6] 214 1:1 1890 29 Adversarial
      nets 225 2 2057 26 Table 1: Parzen window-based log-likelihood estimates. The
      reported numbers on MNIST are the ", "ans was obtained by cross validation on
      the validation set. This procedure was introduced in Breuleux et al. [8] and
      used for various generative models for which the exact likelihood is not tractable
      [25, 3, 5]. Results are reported in Table 1. This method of estimating the likelihood
      has somewhat high variance and does not perform well in high dimensional spaces
      but it is the best method available to our k"], "isInfluential": false, "intents":
      ["methodology", "background"], "citedPaper": {"paperId": "d0965d8f9842f2db960b36b528107ca362c00d1a",
      "externalIds": {"ArXiv": "1207.4404", "MAG": "1496559305", "DBLP": "journals/corr/abs-1207-4404",
      "CorpusId": 1334653}, "corpusId": 1334653, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/d0965d8f9842f2db960b36b528107ca362c00d1a",
      "title": "Better Mixing via Deep Representations", "abstract": "It has been
      hypothesized, and supported with experimental evidence, that deeper representations,
      when well trained, tend to do a better job at disentangling the underlying factors
      of variation. We study the following related conjecture: better representations,
      in the sense of better disentangling, can be exploited to produce Markov chains
      that mix faster between modes. Consequently, mixing between modes would be more
      efficient at higher levels of representation. To better understand this, we
      propose a secondary conjecture: the higher-level samples fill more uniformly
      the space they occupy and the high-density manifolds tend to unfold when represented
      at higher levels. The paper discusses these hypotheses and tests them experimentally
      through visualization and measurements of mixing between modes and interpolating
      between samples.", "venue": "International Conference on Machine Learning",
      "year": 2012, "referenceCount": 37, "citationCount": 305, "influentialCitationCount":
      18, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2012-07-18", "journal": {"pages": "552-560"},
      "authors": [{"authorId": "1751762", "name": "Yoshua Bengio"}, {"authorId": "1935910",
      "name": "Gr\u00e9goire Mesnil"}, {"authorId": "2921469", "name": "Yann Dauphin"},
      {"authorId": "2425018", "name": "Salah Rifai"}]}}, {"contexts": ["odel is also
      a multilayer perceptron. We refer to this special case as adversarial nets.
      In this case, we can train both models using only the highly successful backpropagation
      and dropout algorithms [17] and sample from the generative model using only
      forward propagation. No approximate inference or Markov chains are necessary.
      2 Related work An alternative to directed graphical models with latent va",
      "base (TFD) [28], and CIFAR-10 [21]. The generator nets used a mixture of recti\ufb01er
      linear activations [19, 9] and sigmoid activations, while the discriminator
      net used maxout [10] activations. Dropout [17] was applied in training the discriminator
      net. While our theoretical framework permits the use of dropout and other noise
      at intermediate layers of the generator, we used noise as the input to only
      t"], "isInfluential": false, "intents": ["methodology"], "citedPaper": {"paperId":
      "0060745e006c5f14ec326904119dca19c6545e51", "externalIds": {"DBLP": "journals/corr/abs-1207-0580",
      "MAG": "1904365287", "ArXiv": "1207.0580", "CorpusId": 14832074}, "corpusId":
      14832074, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "http://bibpurl.oclc.org/web/7130"}, "url": "https://www.semanticscholar.org/paper/0060745e006c5f14ec326904119dca19c6545e51",
      "title": "Improving neural networks by preventing co-adaptation of feature detectors",
      "abstract": "When a large feedforward neural network is trained on a small training
      set, it typically performs poorly on held-out test data. This \"overfitting\"
      is greatly reduced by randomly omitting half of the feature detectors on each
      training case. This prevents complex co-adaptations in which a feature detector
      is only helpful in the context of several other specific feature detectors.
      Instead, each neuron learns to detect a feature that is generally helpful for
      producing the correct answer given the combinatorially large variety of internal
      contexts in which it must operate. Random \"dropout\" gives big improvements
      on many benchmark tasks and sets new records for speech and object recognition.",
      "venue": "arXiv.org", "year": 2012, "referenceCount": 26, "citationCount": 7031,
      "influentialCitationCount": 559, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2012-07-02", "journal": {"volume": "abs/1207.0580", "name": "ArXiv"}, "authors":
      [{"authorId": "1695689", "name": "Geoffrey E. Hinton"}, {"authorId": "2897313",
      "name": "Nitish Srivastava"}, {"authorId": "2064160", "name": "A. Krizhevsky"},
      {"authorId": "1701686", "name": "Ilya Sutskever"}, {"authorId": "145124475",
      "name": "R. Salakhutdinov"}]}}, {"contexts": ["ans was obtained by cross validation
      on the validation set. This procedure was introduced in Breuleux et al. [8]
      and used for various generative models for which the exact likelihood is not
      tractable [25, 3, 5]. Results are reported in Table 1. This method of estimating
      the likelihood has somewhat high variance and does not perform well in high
      dimensional spaces but it is the best method available to our k"], "isInfluential":
      false, "intents": ["methodology"], "citedPaper": {"paperId": "aaaea06da21f22221d5fbfd61bb3a02439f0fe02",
      "externalIds": {"MAG": "2950320139", "CorpusId": 122643575}, "corpusId": 122643575,
      "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International
      Conference on Machine Learning", "type": "conference", "alternate_names": ["ICML",
      "Int Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/aaaea06da21f22221d5fbfd61bb3a02439f0fe02",
      "title": "A Generative Process for sampling Contractive Auto-Encoders", "abstract":
      "The contractive auto-encoder learns a representation of the input data that
      captures the local manifold structure around each data point, through the leading
      singular vectors of the Jacobian of the transformation from input to representation.
      The corresponding singular values specify how much local variation is plausible
      in directions associated with the corresponding singular vectors, while remaining
      in a high-density region of the input space. This paper proposes a procedure
      for generating samples that are consistent with the local structure captured
      by a contractive auto-encoder. The associated stochastic process defines a distribution
      from which one can sample, and which experimentally appears to converge quickly
      and mix well between modes, compared to Restricted Boltzmann Machines and Deep
      Belief Networks. The intuitions behind this procedure can also be used to train
      the second layer of contraction that pools lower-level features and learns to
      be invariant to the local directions of variation discovered in the first layer.
      We show that this can help learn and represent invariances present in the data
      and improve classification error.", "venue": "International Conference on Machine
      Learning", "year": 2012, "referenceCount": 18, "citationCount": 78, "influentialCitationCount":
      3, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
      "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
      "2012-06-26", "journal": {"volume": "", "pages": "1811-1818", "name": ""}, "authors":
      [{"authorId": "2425018", "name": "Salah Rifai"}, {"authorId": "1751762", "name":
      "Yoshua Bengio"}, {"authorId": "2921469", "name": "Yann Dauphin"}, {"authorId":
      "145467703", "name": "Pascal Vincent"}]}}, {"contexts": ["inst other models
      of the real-valued (rather than binary) version of dataset. of the Gaussians
      was obtained by cross validation on the validation set. This procedure was introduced
      in Breuleux et al. [8] and used for various generative models for which the
      exact likelihood is not tractable [25, 3, 5]. Results are reported in Table
      1. This method of estimating the likelihood has somewhat high variance"], "isInfluential":
      false, "intents": ["methodology"], "citedPaper": {"paperId": "d1c67346e46b4d0067b3c2e5d3b959a8bc24b28c",
      "externalIds": {"DBLP": "journals/neco/BreuleuxBV11", "MAG": "2106439909", "DOI":
      "10.1162/NECO_a_00158", "CorpusId": 907908}, "corpusId": 907908, "publicationVenue":
      {"id": "69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3", "name": "Neural Computation",
      "type": "journal", "alternate_names": ["Neural Comput"], "issn": "0899-7667",
      "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667", "alternate_urls":
      ["http://ieeexplore.ieee.org/servlet/opac?punumber=6720226", "http://www.mitpressjournals.org/loi/neco",
      "https://www.mitpressjournals.org/loi/neco"]}, "url": "https://www.semanticscholar.org/paper/d1c67346e46b4d0067b3c2e5d3b959a8bc24b28c",
      "title": "Quickly Generating Representative Samples from an RBM-Derived Process",
      "abstract": "Two recently proposed learning algorithms, herding and fast persistent
      contrastive divergence (FPCD), share the following interesting characteristic:
      they exploit changes in the model parameters while sampling in order to escape
      modes and mix better during the sampling process that is part of the learning
      algorithm. We justify such approaches as ways to escape modes while keeping
      approximately the same asymptotic distribution of the Markov chain. In that
      spirit, we extend FPCD using an idea borrowed from Herding in order to obtain
      a pure sampling algorithm, which we call the rates-FPCD sampler. Interestingly,
      this sampler can improve the model as we collect more samples, since it optimizes
      a lower bound on the log likelihood of the training data. We provide empirical
      evidence that this new algorithm displays substantially better and more robust
      mixing than Gibbs sampling.", "venue": "Neural Computation", "year": 2011, "referenceCount":
      13, "citationCount": 82, "influentialCitationCount": 7, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2011-08-01", "journal": {"volume": "23", "pages": "2058-2073", "name": "Neural
      Computation"}, "authors": [{"authorId": "1967465", "name": "Olivier Breuleux"},
      {"authorId": "1751762", "name": "Yoshua Bengio"}, {"authorId": "145467703",
      "name": "Pascal Vincent"}]}}, {"contexts": ["th undirected and directed models.
      Alternative criteria that do not approximate or bound the log-likelihood have
      also been proposed, such as score matching [18] and noise-contrastive estimation
      (NCE) [13]. Both of these require the learned probability density to be analytically
      speci\ufb01ed up to a normalization constant. Note that in many interesting
      generative models with several layers of latent variab"], "isInfluential": false,
      "intents": ["background"], "citedPaper": {"paperId": "e3ce36b9deb47aa6bb2aa19c4bfa71283b505025",
      "externalIds": {"DBLP": "journals/jmlr/GutmannH10", "MAG": "2152790380", "CorpusId":
      15816723}, "corpusId": 15816723, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
      "name": "International Conference on Artificial Intelligence and Statistics",
      "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
      Stat"]}, "url": "https://www.semanticscholar.org/paper/e3ce36b9deb47aa6bb2aa19c4bfa71283b505025",
      "title": "Noise-contrastive estimation: A new estimation principle for unnormalized
      statistical models", "abstract": "We present a new estimation principle for
      parameterized statistical models. The idea is to perform nonlinear logistic
      regression to discriminate between the observed data and some artificially generated
      noise, using the model log-density function in the regression nonlinearity.
      We show that this leads to a consistent (convergent) estimator of the parameters,
      and analyze the asymptotic variance. In particular, the method is shown to directly
      work for unnormalized models, i.e. models where the density function does not
      integrate to one. The normalization constant can be estimated just like any
      other parameter. For a tractable ICA model, we compare the method with other
      estimation methods that can be used to learn unnormalized models, including
      score matching, contrastive divergence, and maximum-likelihood where the normalization
      constant is estimated with importance sampling. Simulations show that noise-contrastive
      estimation offers the best trade-off between computational and statistical efficiency.
      The method is then applied to the modeling of natural images: We show that the
      method can successfully estimate a large-scale two-layer model and a Markov
      random field.", "venue": "International Conference on Artificial Intelligence
      and Statistics", "year": 2010, "referenceCount": 15, "citationCount": 1756,
      "influentialCitationCount": 290, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2010-03-31", "journal": {"pages": "297-304"},
      "authors": [{"authorId": "145992652", "name": "Michael U Gutmann"}, {"authorId":
      "1791548", "name": "Aapo Hyv\u00e4rinen"}]}}, {"contexts": [], "isInfluential":
      false, "intents": [], "citedPaper": {"paperId": "0d2336389dff3031910bd21dd1c44d1b4cd51725",
      "externalIds": {"MAG": "2606321545", "DBLP": "journals/jmlr/ErhanBCMVB10", "DOI":
      "10.5555/1756006.1756025", "CorpusId": 15796526}, "corpusId": 15796526, "publicationVenue":
      {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e", "name": "International Conference
      on Artificial Intelligence and Statistics", "type": "conference", "alternate_names":
      ["AISTATS", "Int Conf Artif Intell Stat"]}, "url": "https://www.semanticscholar.org/paper/0d2336389dff3031910bd21dd1c44d1b4cd51725",
      "title": "Why Does Unsupervised Pre-training Help Deep Learning?", "abstract":
      "Much recent research has been devoted to learning algorithms for deep architectures
      such as Deep Belief Networks and stacks of auto-encoder variants, with impressive
      results obtained in several areas, mostly on vision and language data sets.
      The best results obtained on supervised learning tasks involve an unsupervised
      learning component, usually in an unsupervised pre-training phase. Even though
      these new algorithms have enabled training deep models, many questions remain
      as to the nature of this difficult learning problem. The main question investigated
      here is the following: how does unsupervised pre-training work? Answering this
      questions is important if learning in deep architectures is to be further improved.
      We propose several explanatory hypotheses and test them through extensive simulations.
      We empirically show the influence of pre-training with respect to architecture
      depth, model capacity, and number of training examples. The experiments confirm
      and clarify the advantage of unsupervised pre-training. The results suggest
      that unsupervised pre-training guides the learning towards basins of attraction
      of minima that support better generalization from the training data set; the
      evidence from these results supports a regularization explanation for the effect
      of pre-training.", "venue": "International Conference on Artificial Intelligence
      and Statistics", "year": 2010, "referenceCount": 69, "citationCount": 2162,
      "influentialCitationCount": 84, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2010-03-01", "journal": {"pages": "201-208"}, "authors": [{"authorId": "1761978",
      "name": "D. Erhan"}, {"authorId": "1760871", "name": "Aaron C. Courville"},
      {"authorId": "1751762", "name": "Yoshua Bengio"}, {"authorId": "120247189",
      "name": "Pascal Vincent"}]}}, {"contexts": ["ments We trained adversarial nets
      an a range of datasets including MNIST[23], the Toronto Face Database (TFD)
      [28], and CIFAR-10 [21]. The generator nets used a mixture of recti\ufb01er
      linear activations [19, 9] and sigmoid activations, while the discriminator
      net used maxout [10] activations. Dropout [17] was applied in training the discriminator
      net. While our theoretical framework permits the use of dropo", "at map a high-dimensional,
      rich sensory input to a class label [14, 22]. These striking successes have
      primarily been based on the backpropagation and dropout algorithms, using piecewise
      linear units [19, 9, 10] which have a particularly well-behaved gradient . Deep
      generative models have had less of an impact, due to the dif\ufb01culty of approximating
      many intractable probabilistic computations that arise in ma", "ersarial nets
      framework does not require a Markov chain for sampling. Because adversarial
      nets do not require feedback loops during generation, they are better able to
      leverage piecewise linear units [19, 9, 10], which improve the performance of
      backpropagation but have problems with unbounded activation when used ina feedback
      loop. More recent examples of training a generative machine by back-propagating
      in"], "isInfluential": false, "intents": ["methodology", "background"], "citedPaper":
      {"paperId": "1f88427d7aa8225e47f946ac41a0667d7b69ac52", "externalIds": {"MAG":
      "2546302380", "DBLP": "conf/iccv/JarrettKRL09", "DOI": "10.1109/ICCV.2009.5459469",
      "CorpusId": 206769720}, "corpusId": 206769720, "publicationVenue": {"id": "7654260e-79f9-45c5-9663-d72027cf88f3",
      "name": "IEEE International Conference on Computer Vision", "type": "conference",
      "alternate_names": ["ICCV", "IEEE Int Conf Comput Vis", "ICCV Workshops", "ICCV
      Work"], "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"},
      "url": "https://www.semanticscholar.org/paper/1f88427d7aa8225e47f946ac41a0667d7b69ac52",
      "title": "What is the best multi-stage architecture for object recognition?",
      "abstract": "In many recent object recognition systems, feature extraction stages
      are generally composed of a filter bank, a non-linear transformation, and some
      sort of feature pooling layer. Most systems use only one stage of feature extraction
      in which the filters are hard-wired, or two stages where the filters in one
      or both stages are learned in supervised or unsupervised mode. This paper addresses
      three questions: 1. How does the non-linearities that follow the filter banks
      influence the recognition accuracy? 2. does learning the filter banks in an
      unsupervised or supervised manner improve the performance over random filters
      or hardwired filters? 3. Is there any advantage to using an architecture with
      two stages of feature extraction, rather than one? We show that using non-linearities
      that include rectification and local contrast normalization is the single most
      important ingredient for good accuracy on object recognition benchmarks. We
      show that two stages of feature extraction yield better accuracy than one. Most
      surprisingly, we show that a two-stage system with random filters can yield
      almost 63% recognition rate on Caltech-101, provided that the proper non-linearities
      and pooling layers are used. Finally, we show that with supervised refinement,
      the system achieves state-of-the-art performance on NORB dataset (5.6%) and
      unsupervised pre-training followed by supervised refinement produces good accuracy
      on Caltech-101 (\u226b 65%), and the lowest known error rate on the undistorted,
      unprocessed MNIST dataset (0.53%).", "venue": "IEEE International Conference
      on Computer Vision", "year": 2009, "referenceCount": 55, "citationCount": 2261,
      "influentialCitationCount": 146, "isOpenAccess": true, "openAccessPdf": {"url":
      "http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf", "status": null},
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2009-09-01", "journal": {"pages": "2146-2153", "name": "2009 IEEE 12th International
      Conference on Computer Vision"}, "authors": [{"authorId": "2077257730", "name":
      "Kevin Jarrett"}, {"authorId": "2645384", "name": "K. Kavukcuoglu"}, {"authorId":
      "1706809", "name": "Marc''Aurelio Ranzato"}, {"authorId": "1688882", "name":
      "Yann LeCun"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper":
      {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "externalIds": {"DBLP":
      "conf/cvpr/DengDSLL009", "MAG": "2108598243", "DOI": "10.1109/CVPR.2009.5206848",
      "CorpusId": 57246310}, "corpusId": 57246310, "publicationVenue": null, "url":
      "https://www.semanticscholar.org/paper/d2c733e34d48784a37d717fe43d9e93277a8c53e",
      "title": "ImageNet: A large-scale hierarchical image database", "abstract":
      "The explosion of image data on the Internet has the potential to foster more
      sophisticated and robust models and algorithms to index, retrieve, organize
      and interact with images and multimedia data. But exactly how such data can
      be harnessed and organized remains a critical problem. We introduce here a new
      database called \u201cImageNet\u201d, a large-scale ontology of images built
      upon the backbone of the WordNet structure. ImageNet aims to populate the majority
      of the 80,000 synsets of WordNet with an average of 500-1000 clean and full
      resolution images. This will result in tens of millions of annotated images
      organized by the semantic hierarchy of WordNet. This paper offers a detailed
      analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and
      3.2 million images in total. We show that ImageNet is much larger in scale and
      diversity and much more accurate than the current image datasets. Constructing
      such a large-scale database is a challenging task. We describe the data collection
      scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of
      ImageNet through three simple applications in object recognition, image classification
      and automatic object clustering. We hope that the scale, accuracy, diversity
      and hierarchical structure of ImageNet can offer unparalleled opportunities
      to researchers in the computer vision community and beyond.", "venue": "2009
      IEEE Conference on Computer Vision and Pattern Recognition", "year": 2009, "referenceCount":
      27, "citationCount": 46360, "influentialCitationCount": 9295, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2009-06-20", "journal": {"pages": "248-255",
      "name": "2009 IEEE Conference on Computer Vision and Pattern Recognition"},
      "authors": [{"authorId": "153302678", "name": "Jia Deng"}, {"authorId": "144847596",
      "name": "Wei Dong"}, {"authorId": "2166511", "name": "R. Socher"}, {"authorId":
      "2040091191", "name": "Li-Jia Li"}, {"authorId": "94451829", "name": "K. Li"},
      {"authorId": "48004138", "name": "Li Fei-Fei"}]}}, {"contexts": ["rnative to
      directed graphical models with latent variables are undirected graphical models
      with latent variables, such as restricted Boltzmann machines (RBMs) [27, 16],
      deep Boltzmann machines (DBMs) [26] and their numerous variants. The interactions
      within such models are represented as the product of unnormalized potential
      functions, normalized by a global summation/integration over all states of th"],
      "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId":
      "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "externalIds": {"DBLP": "journals/jmlr/SalakhutdinovH09",
      "MAG": "189596042", "CorpusId": 877639}, "corpusId": 877639, "publicationVenue":
      {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e", "name": "International Conference
      on Artificial Intelligence and Statistics", "type": "conference", "alternate_names":
      ["AISTATS", "Int Conf Artif Intell Stat"]}, "url": "https://www.semanticscholar.org/paper/85021c84383d18a7a4434d76dc8135fc6bdc0aa6",
      "title": "Deep Boltzmann Machines", "abstract": "We present a new learning algorithm
      for Boltzmann machines that contain many layers of hidden variables. Data-dependent
      expectations are estimated using a variational approximation that tends to focus
      on a single mode, and dataindependent expectations are approximated using persistent
      Markov chains. The use of two quite different techniques for estimating the
      two types of expectation that enter into the gradient of the log-likelihood
      makes it practical to learn Boltzmann machines with multiple hidden layers and
      millions of parameters. The learning can be made more efficient by using a layer-by-layer
      \u201cpre-training\u201d phase that allows variational inference to be initialized
      with a single bottomup pass. We present results on the MNIST and NORB datasets
      showing that deep Boltzmann machines learn good generative models and perform
      well on handwritten digit and visual object recognition tasks.", "venue": "International
      Conference on Artificial Intelligence and Statistics", "year": 2009, "referenceCount":
      22, "citationCount": 2202, "influentialCitationCount": 272, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2009-04-15", "journal": {"pages": "448-455"}, "authors":
      [{"authorId": "145124475", "name": "R. Salakhutdinov"}, {"authorId": "1695689",
      "name": "Geoffrey E. Hinton"}]}}, {"contexts": ["ive models with several layers
      of latent variables (such as DBNs and DBMs), it is not even possible to derive
      a tractable unnormalized probability density. Some models such as denoising
      auto-encoders [30] and contractive autoencoders have learning rules very similar
      to score matching applied to RBMs. In NCE, as in this work, a discriminative
      training criterion is employed to \ufb01t a generative model. How"], "isInfluential":
      false, "intents": ["methodology"], "citedPaper": {"paperId": "843959ffdccf31c6694d135fad07425924f785b1",
      "externalIds": {"DBLP": "conf/icml/VincentLBM08", "MAG": "2025768430", "DOI":
      "10.1145/1390156.1390294", "CorpusId": 207168299}, "corpusId": 207168299, "publicationVenue":
      {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
      on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
      Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/843959ffdccf31c6694d135fad07425924f785b1",
      "title": "Extracting and composing robust features with denoising autoencoders",
      "abstract": "Previous work has shown that the difficulties in learning deep
      generative or discriminative models can be overcome by an initial unsupervised
      learning step that maps inputs to useful intermediate representations. We introduce
      and motivate a new training principle for unsupervised learning of a representation
      based on the idea of making the learned representations robust to partial corruption
      of the input pattern. This approach can be used to train autoencoders, and these
      denoising autoencoders can be stacked to initialize deep architectures. The
      algorithm can be motivated from a manifold learning and information theoretic
      perspective or from a generative model perspective. Comparative experiments
      clearly show the surprising advantage of corrupting the input of autoencoders
      on a pattern classification benchmark suite.", "venue": "International Conference
      on Machine Learning", "year": 2008, "referenceCount": 29, "citationCount": 6510,
      "influentialCitationCount": 540, "isOpenAccess": true, "openAccessPdf": {"url":
      "http://www.iro.umontreal.ca/~vincentp/Publications/denoising_autoencoders_tr1316.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2008-07-05", "journal":
      {"pages": "1096-1103"}, "authors": [{"authorId": "120247189", "name": "Pascal
      Vincent"}, {"authorId": "1777528", "name": "H. Larochelle"}, {"authorId": "1751762",
      "name": "Yoshua Bengio"}, {"authorId": "1798462", "name": "Pierre-Antoine Manzagol"}]}},
      {"contexts": [" ksteps of optimizing Dand one step of optimizing G. This results
      in Dbeing maintained near its optimal solution, so long as Gchanges slowly enough.
      This strategy is analogous to the way that SML/PCD [31, 29] training maintains
      samples from a Markov chain from one learning step to the next in order to avoid
      burning in a Markov chain as part of the inner loop of learning. The procedure
      is formally presente"], "isInfluential": false, "intents": ["result"], "citedPaper":
      {"paperId": "73d6a26f407db77506959fdf3f7b853e44f3844a", "externalIds": {"MAG":
      "2116825644", "DBLP": "conf/icml/Tieleman08", "DOI": "10.1145/1390156.1390290",
      "CorpusId": 7330145}, "corpusId": 7330145, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/73d6a26f407db77506959fdf3f7b853e44f3844a",
      "title": "Training restricted Boltzmann machines using approximations to the
      likelihood gradient", "abstract": "A new algorithm for training Restricted Boltzmann
      Machines is introduced. The algorithm, named Persistent Contrastive Divergence,
      is different from the standard Contrastive Divergence algorithms in that it
      aims to draw samples from almost exactly the model distribution. It is compared
      to some standard Contrastive Divergence and Pseudo-Likelihood algorithms on
      the tasks of modeling and classifying various types of data. The Persistent
      Contrastive Divergence algorithm outperforms the other algorithms, and is equally
      fast and simple.", "venue": "International Conference on Machine Learning",
      "year": 2008, "referenceCount": 22, "citationCount": 989, "influentialCitationCount":
      138, "isOpenAccess": true, "openAccessPdf": {"url": "http://icml2008.cs.helsinki.fi/papers/638.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2008-07-05", "journal":
      {"pages": "1064-1071"}, "authors": [{"authorId": "2957517", "name": "T. Tieleman"}]}},
      {"contexts": ["ins are necessary. 2 Related work An alternative to directed
      graphical models with latent variables are undirected graphical models with
      latent variables, such as restricted Boltzmann machines (RBMs) [27, 16], deep
      Boltzmann machines (DBMs) [26] and their numerous variants. The interactions
      within such models are represented as the product of unnormalized potential
      functions, normalized by a global summat", "l instances, although they can be
      estimated by Markov chain Monte Carlo (MCMC) methods. Mixing poses a signi\ufb01cant
      problem for learning algorithms that rely on MCMC [3, 5]. Deep belief networks
      (DBNs) [16] are hybrid models containing a single undirected layer and several
      directed layers. While a fast approximate layer-wise training criterion exists,
      DBNs incur the computational dif\ufb01culties associated "], "isInfluential":
      false, "intents": ["background"], "citedPaper": {"paperId": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
      "externalIds": {"MAG": "2136922672", "DBLP": "journals/neco/HintonOT06", "DOI":
      "10.1162/neco.2006.18.7.1527", "CorpusId": 2309950, "PubMed": "16764513"}, "corpusId":
      2309950, "publicationVenue": {"id": "69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
      "name": "Neural Computation", "type": "journal", "alternate_names": ["Neural
      Comput"], "issn": "0899-7667", "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667",
      "alternate_urls": ["http://ieeexplore.ieee.org/servlet/opac?punumber=6720226",
      "http://www.mitpressjournals.org/loi/neco", "https://www.mitpressjournals.org/loi/neco"]},
      "url": "https://www.semanticscholar.org/paper/8978cf7574ceb35f4c3096be768c7547b28a35d0",
      "title": "A Fast Learning Algorithm for Deep Belief Nets", "abstract": "We show
      how to use complementary priors to eliminate the explaining-away effects that
      make inference difficult in densely connected belief nets that have many hidden
      layers. Using complementary priors, we derive a fast, greedy algorithm that
      can learn deep, directed belief networks one layer at a time, provided the top
      two layers form an undirected associative memory. The fast, greedy algorithm
      is used to initialize a slower learning procedure that fine-tunes the weights
      using a contrastive version of the wake-sleep algorithm. After fine-tuning,
      a network with three hidden layers forms a very good generative model of the
      joint distribution of handwritten digit images and their labels. This generative
      model gives better digit classification than the best discriminative learning
      algorithms. The low-dimensional manifolds on which the digits lie are modeled
      by long ravines in the free-energy landscape of the top-level associative memory,
      and it is easy to explore these ravines by using the directed connections to
      display what the associative memory has in mind.", "venue": "Neural Computation",
      "year": 2006, "referenceCount": 31, "citationCount": 14954, "influentialCitationCount":
      1320, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Medicine",
      "Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category": "Medicine",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2006-07-01", "journal": {"volume": "18", "pages": "1527-1554", "name": "Neural
      Computation"}, "authors": [{"authorId": "1695689", "name": "Geoffrey E. Hinton"},
      {"authorId": "2217144", "name": "Simon Osindero"}, {"authorId": "1725303", "name":
      "Y. Teh"}]}}, {"contexts": [], "isInfluential": false, "intents": [], "citedPaper":
      {"paperId": "90929a6aa901ba958eb4960aeeb594c752e08369", "externalIds": {"DBLP":
      "conf/nips/NgJ01", "MAG": "2163614729", "CorpusId": 296750}, "corpusId": 296750,
      "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/90929a6aa901ba958eb4960aeeb594c752e08369",
      "title": "On Discriminative vs. Generative Classifiers: A comparison of logistic
      regression and naive Bayes", "abstract": "We compare discriminative and generative
      learning as typified by logistic regression and naive Bayes. We show, contrary
      to a widely-held belief that discriminative classifiers are almost always to
      be preferred, that there can often be two distinct regimes of performance as
      the training set size is increased, one in which each algorithm does better.
      This stems from the observation\u2014which is borne out in repeated experiments\u2014that
      while discriminative learning has lower asymptotic error, a generative classifier
      may also approach its (higher) asymptotic error much faster.", "venue": "NIPS",
      "year": 2001, "referenceCount": 7, "citationCount": 2380, "influentialCitationCount":
      94, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2001-01-03", "journal": {"pages": "841-848"},
      "authors": [{"authorId": "34699434", "name": "A. Ng"}, {"authorId": "1694621",
      "name": "Michael I. Jordan"}]}}, {"contexts": [" ksteps of optimizing Dand one
      step of optimizing G. This results in Dbeing maintained near its optimal solution,
      so long as Gchanges slowly enough. This strategy is analogous to the way that
      SML/PCD [31, 29] training maintains samples from a Markov chain from one learning
      step to the next in order to avoid burning in a Markov chain as part of the
      inner loop of learning. The procedure is formally presente"], "isInfluential":
      false, "intents": ["result"], "citedPaper": {"paperId": "ca9b21e84ffc7e193d1b3bb45fb7c4e48226b59e",
      "externalIds": {"MAG": "1990838964", "DOI": "10.1080/17442509908834179", "CorpusId":
      15419929}, "corpusId": 15419929, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/ca9b21e84ffc7e193d1b3bb45fb7c4e48226b59e",
      "title": "On the convergence of markovian stochastic algorithms with rapidly
      decreasing ergodicity rates", "abstract": "We analyse the convergence of stochastic
      algorithms with Markovian noise when the ergodicity of the Markov chain governing
      the noise rapidly decreases as the control parameter tends to infinity. In such
      a case, there may be a positive probability of divergence of the algorithm in
      the classic Robbins-Monro form. We provide sufficient condition which ensure
      convergence. Moreover, we analyse the asymptotic behaviour of these algorithms
      and state a diffusion approximation theorem", "venue": "", "year": 1999, "referenceCount":
      24, "citationCount": 163, "influentialCitationCount": 24, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Mathematics"], "s2FieldsOfStudy":
      [{"category": "Mathematics", "source": "external"}, {"category": "Mathematics",
      "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": null, "publicationDate": "1999-02-01", "journal": {"volume":
      "65", "pages": "177-228", "name": "Stochastics and Stochastics Reports"}, "authors":
      [{"authorId": "1721284", "name": "L. Younes"}]}}, {"contexts": [], "isInfluential":
      false, "intents": [], "citedPaper": {"paperId": "629cc74dcaf655feea40f64cd74617ac884ed0f8",
      "externalIds": {"MAG": "2150218618", "DOI": "10.7551/mitpress/3348.001.0001",
      "CorpusId": 62488180}, "corpusId": 62488180, "publicationVenue": null, "url":
      "https://www.semanticscholar.org/paper/629cc74dcaf655feea40f64cd74617ac884ed0f8",
      "title": "Graphical Models for Machine Learning and Digital Communication",
      "abstract": "Probabilistic inference in graphical models pattern classification
      unsupervised learning data compression channel coding future research directions.",
      "venue": "", "year": 1998, "referenceCount": 6, "citationCount": 633, "influentialCitationCount":
      43, "isOpenAccess": true, "openAccessPdf": {"url": "http://www.gaussianprocess.org/gpml/chapters/RW.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "1998-06-26",
      "journal": {"volume": "", "name": ""}, "authors": [{"authorId": "1749650", "name":
      "B. Frey"}]}}, {"contexts": ["input to both Gand D. 2. Learned approximate inference
      can be performed by training an auxiliary network to predict z given x. This
      is similar to the inference net trained by the wake-sleep algorithm [15] but
      with the advantage that the inference net may be trained for a \ufb01xed generator
      net after the generator net has \ufb01nished training. 7 3.One can approximately
      model all conditionals p(x S jx 6S) where"], "isInfluential": false, "intents":
      ["background"], "citedPaper": {"paperId": "6dd01cd9c17d1491ead8c9f97597fbc61dead8ea",
      "externalIds": {"MAG": "1993845689", "DOI": "10.1126/SCIENCE.7761831", "CorpusId":
      871473, "PubMed": "7761831"}, "corpusId": 871473, "publicationVenue": {"id":
      "f59506a8-d8bb-4101-b3d4-c4ac3ed03dad", "name": "Science", "type": "journal",
      "issn": "0193-4511", "alternate_issns": ["0036-8075"], "url": "https://www.jstor.org/journal/science",
      "alternate_urls": ["https://www.sciencemag.org/", "http://www.sciencemag.org/",
      "http://www.jstor.org/journals/00368075.html", "http://www.sciencemag.org/archive/"]},
      "url": "https://www.semanticscholar.org/paper/6dd01cd9c17d1491ead8c9f97597fbc61dead8ea",
      "title": "The \"wake-sleep\" algorithm for unsupervised neural networks.", "abstract":
      "An unsupervised learning algorithm for a multilayer network of stochastic neurons
      is described. Bottom-up \"recognition\" connections convert the input into representations
      in successive hidden layers, and top-down \"generative\" connections reconstruct
      the representation in one layer from the representation in the layer above.
      In the \"wake\" phase, neurons are driven by recognition connections, and generative
      connections are adapted to increase the probability that they would reconstruct
      the correct activity vector in the layer below. In the \"sleep\" phase, neurons
      are driven by generative connections, and recognition connections are adapted
      to increase the probability that they would produce the correct activity vector
      in the layer above.", "venue": "Science", "year": 1995, "referenceCount": 23,
      "citationCount": 1072, "influentialCitationCount": 49, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Medicine",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "1995-05-26", "journal":
      {"volume": "268 5214", "pages": "\n          1158-61\n        ", "name": "Science"},
      "authors": [{"authorId": "1695689", "name": "Geoffrey E. Hinton"}, {"authorId":
      "1790646", "name": "P. Dayan"}, {"authorId": "1749650", "name": "B. Frey"},
      {"authorId": "145572884", "name": "R. Neal"}]}}, {"contexts": ["ins are necessary.
      2 Related work An alternative to directed graphical models with latent variables
      are undirected graphical models with latent variables, such as restricted Boltzmann
      machines (RBMs) [27, 16], deep Boltzmann machines (DBMs) [26] and their numerous
      variants. The interactions within such models are represented as the product
      of unnormalized potential functions, normalized by a global summat"], "isInfluential":
      false, "intents": ["background"], "citedPaper": {"paperId": "4f7476037408ac3d993f5088544aab427bc319c1",
      "externalIds": {"MAG": "1820494964", "CorpusId": 533055}, "corpusId": 533055,
      "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/4f7476037408ac3d993f5088544aab427bc319c1",
      "title": "Information processing in dynamical systems: foundations of harmony
      theory", "abstract": "Abstract : At this early stage in the development of cognitive
      science, methodological issues are both open and central. There may have been
      times when developments in neuroscience, artificial intelligence, or cognitive
      psychology seduced researchers into believing that their discipline was on the
      verge of discovering the secret of intelligence. But a humbling history of hopes
      disappointed has produced the realization that understanding the mind will challenge
      the power of all these methodologies combined. The work reported in this chapter
      rests on the conviction that a methodology that has a crucial role to play in
      the development of cognitive science is mathematical analysis. The success of
      cognitive science, like that of many other sciences, will, I believe, depend
      upon the construction of a solid body of theoretical results: results that express
      in a mathematical language the conceptual insights of the field; results that
      squeeze all possible implications out of those insights by exploiting powerful
      mathematical techniques. This body of results, which I will call the theory
      of information processing, exists because information is a concept that lends
      itself to mathematical formalization. One part of the theory of information
      processing is already well-developed. The classical theory of computation provides
      powerful and elegant results about the notion of effective procedure, including
      languages for precisely expressing them and theoretical machines for realizing
      them.", "venue": "", "year": 1986, "referenceCount": 18, "citationCount": 2085,
      "influentialCitationCount": 208, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category":
      "Mathematics", "source": "external"}, {"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      null, "publicationDate": "1986-01-03", "journal": {"volume": "", "pages": "194-281",
      "name": ""}, "authors": [{"authorId": "1748557", "name": "P. Smolensky"}]}},
      {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId":
      "c68796f833a7151f0a63d1d1608dc902b4fdc9b6", "externalIds": {"CorpusId": 10319744},
      "corpusId": 10319744, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/c68796f833a7151f0a63d1d1608dc902b4fdc9b6",
      "title": "GENERATIVE ADVERSARIAL NETS", "abstract": "Estimating individualized
      treatment effects (ITE) is a challenging task due to the need for an individual\u2019s
      potential outcomes to be learned from biased data and without having access
      to the counterfactuals. We propose a novel method for inferring ITE based on
      the Generative Adversarial Nets (GANs) framework. Our method, termed Generative
      Adversarial Nets for inference of Individualized Treatment Effects (GANITE),
      is motivated by the possibility that we can capture the uncertainty in the counterfactual
      distributions by attempting to learn them using a GAN. We generate proxies of
      the counterfactual outcomes using a counterfactual generator, G, and then pass
      these proxies to an ITE generator, I, in order to train it. By modeling both
      of these using the GAN framework, we are able to infer based on the factual
      data, while still accounting for the unseen counterfactuals. We test our method
      on three real-world datasets (with both binary and multiple treatments) and
      show that GANITE outperforms state-of-the-art methods.", "venue": "", "year":
      2018, "referenceCount": 24, "citationCount": 267, "influentialCitationCount":
      56, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      null, "publicationDate": null, "journal": null, "authors": [{"authorId": "2085962273",
      "name": "Individualized Treat"}, {"authorId": "2144029", "name": "Jinsung Yoon"}]}},
      {"contexts": [], "isInfluential": false, "intents": [], "citedPaper": {"paperId":
      null, "externalIds": null, "corpusId": null, "publicationVenue": null, "url":
      null, "title": "Fast high-fidelity speech synthesis", "abstract": null, "venue":
      "arXiv preprint arXiv:1711.10433", "year": 2017, "referenceCount": null, "citationCount":
      null, "influentialCitationCount": null, "isOpenAccess": null, "openAccessPdf":
      null, "fieldsOfStudy": null, "s2FieldsOfStudy": null, "publicationTypes": null,
      "publicationDate": null, "journal": null, "authors": []}}, {"contexts": [],
      "isInfluential": false, "intents": [], "citedPaper": {"paperId": "d6cff906315e29b61afcf14bf7e6fe40f4d74ea5",
      "externalIds": {"MAG": "2978753875", "CorpusId": 208098497}, "corpusId": 208098497,
      "publicationVenue": {"id": "768b87bb-8a18-4d9c-a161-4d483c776bcf", "name": "Computer
      Vision and Pattern Recognition", "type": "conference", "alternate_names": ["CVPR",
      "Comput Vis Pattern Recognit"], "issn": "1063-6919", "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
      "alternate_urls": ["https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"]},
      "url": "https://www.semanticscholar.org/paper/d6cff906315e29b61afcf14bf7e6fe40f4d74ea5",
      "title": "A large-scale hierarchical image database", "abstract": null, "venue":
      "Computer Vision and Pattern Recognition", "year": 2009, "referenceCount": 0,
      "citationCount": 910, "influentialCitationCount": 71, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
      null, "journal": {"volume": "", "name": ""}, "authors": [{"authorId": "153302678",
      "name": "Jia Deng"}]}}, {"contexts": [], "isInfluential": false, "intents":
      [], "citedPaper": {"paperId": "f8b6999fa84023803f9666faf0940ffd0219d4ce", "externalIds":
      {"CorpusId": 6338543}, "corpusId": 6338543, "publicationVenue": null, "url":
      "https://www.semanticscholar.org/paper/f8b6999fa84023803f9666faf0940ffd0219d4ce",
      "title": "Advances in Neural Information Processing Systems I", "abstract":
      "We address the problem of learning structure in nonlinear Ma rkov networks
      with continuous variables. This can be viewed as non-Gaussi n multidimensional
      density estimation exploiting certain conditio nal independencies in the variables.
      Markov networks are a graphical way of desc ribing conditional independencies
      well suited to model relationship which do not exhibit a natural causal ordering.
      We use neural network struc tu es to model the quantitative relationships between
      variables. The mai n focus in this paper will be on learning the structure for
      the purpose of gaini ng insight into the underlying process. Using two data
      sets we show that inte resting structures can be found using our approach. Inference
      will be brie fly addressed.", "venue": "", "year": 2007, "referenceCount": 6,
      "citationCount": 1729, "influentialCitationCount": 156, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
      null, "journal": null, "authors": [{"authorId": "31579914", "name": "R. Hofmann"},
      {"authorId": "1700754", "name": "Volker Tresp"}]}}, {"contexts": ["monstrate
      the potential of the framework through qualitative and quantitative evaluation
      of the generated samples. 1 Introduction The promise of deep learning is to
      discover rich, hierarchical models [2] that represent probability distributions
      over the kinds of data encountered in arti\ufb01cial intelligence applications,
      such as natural images, audio waveforms containing speech, and symbols in natural
      l"], "isInfluential": false, "intents": ["background"], "citedPaper": {"paperId":
      "d04d6db5f0df11d0cff57ec7e15134990ac07a4f", "externalIds": {"DBLP": "journals/ftml/Bengio09",
      "MAG": "2072128103", "DOI": "10.1561/2200000006", "CorpusId": 207178999}, "corpusId":
      207178999, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/d04d6db5f0df11d0cff57ec7e15134990ac07a4f",
      "title": "Learning Deep Architectures for AI", "abstract": "Theoretical results
      strongly suggest that in order to learn the kind of complicated functions that
      can represent high-level abstractions (e.g. in vision, language, and other AI-level
      tasks), one needs deep architectures. Deep architectures are composed of multiple
      levels of non-linear operations, such as in neural nets with many hidden layers
      or in complicated propositional formulae re-using many sub-formulae. Searching
      the parameter space of deep architectures is a difficult optimization task,
      but learning algorithms such as those for Deep Belief Networks have recently
      been proposed to tackle this problem with notable success, beating the state-of-the-art
      in certain areas. This paper discusses the motivations and principles regarding
      learning algorithms for deep architectures, in particular those exploiting as
      building blocks unsupervised learning of single-layer models such as Restricted
      Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.",
      "venue": "Found. Trends Mach. Learn.", "year": 2007, "referenceCount": 247,
      "citationCount": 8236, "influentialCitationCount": 530, "isOpenAccess": true,
      "openAccessPdf": {"url": "http://www.iro.umontreal.ca/~bengioy/papers/ftml.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      null, "journal": {"volume": "2", "pages": "1-127", "name": "Found. Trends Mach.
      Learn."}, "authors": [{"authorId": "1751762", "name": "Yoshua Bengio"}]}}, {"contexts":
      [" perceptrons in practice suggests that they are a reasonable model to use
      despite their lack of theoretical guarantees. 5 Experiments We trained adversarial
      nets an a range of datasets including MNIST[23], the Toronto Face Database (TFD)
      [28], and CIFAR-10 [21]. The generator nets used a mixture of recti\ufb01er
      linear activations [19, 9] and sigmoid activations, while the discriminator
      net used maxout [10"], "isInfluential": false, "intents": ["methodology"], "citedPaper":
      {"paperId": "162d958ff885f1462aeda91cd72582323fd6a1f4", "externalIds": {"MAG":
      "2112796928", "DBLP": "journals/pieee/LeCunBBH98", "DOI": "10.1109/5.726791",
      "CorpusId": 14542261}, "corpusId": 14542261, "publicationVenue": {"id": "6faaccca-1cc4-45a9-aeb6-96a4901d2606",
      "name": "Proceedings of the IEEE", "type": "journal", "alternate_names": ["Proc
      IEEE"], "issn": "0018-9219", "alternate_issns": ["1558-2256"], "url": "http://www.ieee.org/portal/pages/pubs/proceedings/",
      "alternate_urls": ["http://www.ieee.org/products/onlinepubs/pub/about_conference.html",
      "https://ieeexplore.ieee.org/servlet/opac?punumber=5", "http://proceedingsoftheieee.ieee.org/"]},
      "url": "https://www.semanticscholar.org/paper/162d958ff885f1462aeda91cd72582323fd6a1f4",
      "title": "Gradient-based learning applied to document recognition", "abstract":
      "Multilayer neural networks trained with the back-propagation algorithm constitute
      the best example of a successful gradient based learning technique. Given an
      appropriate network architecture, gradient-based learning algorithms can be
      used to synthesize a complex decision surface that can classify high-dimensional
      patterns, such as handwritten characters, with minimal preprocessing. This paper
      reviews various methods applied to handwritten character recognition and compares
      them on a standard handwritten digit recognition task. Convolutional neural
      networks, which are specifically designed to deal with the variability of 2D
      shapes, are shown to outperform all other techniques. Real-life document recognition
      systems are composed of multiple modules including field extraction, segmentation
      recognition, and language modeling. A new learning paradigm, called graph transformer
      networks (GTN), allows such multimodule systems to be trained globally using
      gradient-based methods so as to minimize an overall performance measure. Two
      systems for online handwriting recognition are described. Experiments demonstrate
      the advantage of global training, and the flexibility of graph transformer networks.
      A graph transformer network for reading a bank cheque is also described. It
      uses convolutional neural network character recognizers combined with global
      training techniques to provide record accuracy on business and personal cheques.
      It is deployed commercially and reads several million cheques per day.", "venue":
      "Proceedings of the IEEE", "year": 1998, "referenceCount": 140, "citationCount":
      43380, "influentialCitationCount": 6955, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"],
      "publicationDate": null, "journal": {"volume": "86", "pages": "2278-2324", "name":
      "Proc. IEEE"}, "authors": [{"authorId": "1688882", "name": "Yann LeCun"}, {"authorId":
      "52184096", "name": "L. Bottou"}, {"authorId": "1751762", "name": "Yoshua Bengio"},
      {"authorId": "1721248", "name": "P. Haffner"}]}}]}

      '
    headers:
      Access-Control-Allow-Origin:
      - '*'
      Connection:
      - keep-alive
      Content-Length:
      - '156091'
      Content-Type:
      - application/json
      Date:
      - Wed, 13 Sep 2023 21:48:15 GMT
      Via:
      - 1.1 3aa2aa1b7b816f70e94675c9a63f98d0.cloudfront.net (CloudFront)
      X-Amz-Cf-Id:
      - RheR0wOapAffZLrbnptxRkN1DU0OnsYNi4j35RO5E7D8nZOCRQDmPg==
      X-Amz-Cf-Pop:
      - EWR50-C1
      X-Cache:
      - Miss from cloudfront
      x-amz-apigw-id:
      - LNwk8EyXvHcFZVQ=
      x-amzn-Remapped-Connection:
      - keep-alive
      x-amzn-Remapped-Content-Length:
      - '156091'
      x-amzn-Remapped-Date:
      - Wed, 13 Sep 2023 21:48:15 GMT
      x-amzn-Remapped-Server:
      - gunicorn
      x-amzn-RequestId:
      - 5b09957a-28b9-41f1-b7b2-dc56c8728d76
    http_version: HTTP/1.1
    status_code: 200
version: 1
