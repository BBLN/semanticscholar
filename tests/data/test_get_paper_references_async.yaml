interactions:
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - api.semanticscholar.org
      user-agent:
      - python-httpx/0.24.1
    method: GET
    uri: https://api.semanticscholar.org/graph/v1/paper/CorpusID:1033682/references?&fields=contexts,intents,isInfluential,abstract,authors,citationCount,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=1000
  response:
    content: '{"offset": 0, "data": [{"intents": [], "contexts": [], "isInfluential":
      false, "citedPaper": {"paperId": "0d2336389dff3031910bd21dd1c44d1b4cd51725",
      "externalIds": {"MAG": "2606321545", "DBLP": "journals/jmlr/ErhanBCMVB10", "DOI":
      "10.5555/1756006.1756025", "CorpusId": 15796526}, "corpusId": 15796526, "publicationVenue":
      {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e", "name": "International Conference
      on Artificial Intelligence and Statistics", "type": "conference", "alternate_names":
      ["AISTATS", "Int Conf Artif Intell Stat"]}, "url": "https://www.semanticscholar.org/paper/0d2336389dff3031910bd21dd1c44d1b4cd51725",
      "title": "Why Does Unsupervised Pre-training Help Deep Learning?", "abstract":
      "Much recent research has been devoted to learning algorithms for deep architectures
      such as Deep Belief Networks and stacks of auto-encoder variants, with impressive
      results obtained in several areas, mostly on vision and language data sets.
      The best results obtained on supervised learning tasks involve an unsupervised
      learning component, usually in an unsupervised pre-training phase. Even though
      these new algorithms have enabled training deep models, many questions remain
      as to the nature of this difficult learning problem. The main question investigated
      here is the following: how does unsupervised pre-training work? Answering this
      questions is important if learning in deep architectures is to be further improved.
      We propose several explanatory hypotheses and test them through extensive simulations.
      We empirically show the influence of pre-training with respect to architecture
      depth, model capacity, and number of training examples. The experiments confirm
      and clarify the advantage of unsupervised pre-training. The results suggest
      that unsupervised pre-training guides the learning towards basins of attraction
      of minima that support better generalization from the training data set; the
      evidence from these results supports a regularization explanation for the effect
      of pre-training.", "venue": "International Conference on Artificial Intelligence
      and Statistics", "year": 2010, "referenceCount": 69, "citationCount": 2159,
      "influentialCitationCount": 82, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2010-03-01", "journal": {"pages": "201-208"}, "authors": [{"authorId": "1761978",
      "name": "D. Erhan"}, {"authorId": "1760871", "name": "Aaron C. Courville"},
      {"authorId": "1751762", "name": "Yoshua Bengio"}, {"authorId": "120247189",
      "name": "Pascal Vincent"}]}}]}

      '
    headers:
      Access-Control-Allow-Origin:
      - '*'
      Connection:
      - keep-alive
      Content-Length:
      - '2723'
      Content-Type:
      - application/json
      Date:
      - Fri, 08 Sep 2023 19:40:56 GMT
      Via:
      - 1.1 d50f0ffd76e03cff5d1f6328069e44e0.cloudfront.net (CloudFront)
      X-Amz-Cf-Id:
      - meFZ-XANIT6766l0_2Nkwcs3A84Eng8WOQN1WzARid8-hgJK4SAl5A==
      X-Amz-Cf-Pop:
      - EWR50-C1
      X-Cache:
      - Miss from cloudfront
      x-amz-apigw-id:
      - K8_PUEsePHcFRvQ=
      x-amzn-Remapped-Connection:
      - keep-alive
      x-amzn-Remapped-Content-Length:
      - '2723'
      x-amzn-Remapped-Date:
      - Fri, 08 Sep 2023 19:40:56 GMT
      x-amzn-Remapped-Server:
      - gunicorn
      x-amzn-RequestId:
      - 2882152c-4217-4dbb-9f33-013adbefe5d1
    http_version: HTTP/1.1
    status_code: 200
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - partner.semanticscholar.org
      user-agent:
      - python-httpx/0.24.1
    method: GET
    uri: https://partner.semanticscholar.org/graph/v1/paper/CorpusID:1033682/references?&fields=contexts,intents,isInfluential,abstract,authors,citationCount,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=1000
  response:
    content: '{"offset": 0, "data": [{"contexts": [], "isInfluential": false, "intents":
      [], "citedPaper": {"paperId": "0d2336389dff3031910bd21dd1c44d1b4cd51725", "externalIds":
      {"MAG": "2606321545", "DBLP": "journals/jmlr/ErhanBCMVB10", "DOI": "10.5555/1756006.1756025",
      "CorpusId": 15796526}, "corpusId": 15796526, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
      "name": "International Conference on Artificial Intelligence and Statistics",
      "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
      Stat"]}, "url": "https://www.semanticscholar.org/paper/0d2336389dff3031910bd21dd1c44d1b4cd51725",
      "title": "Why Does Unsupervised Pre-training Help Deep Learning?", "abstract":
      "Much recent research has been devoted to learning algorithms for deep architectures
      such as Deep Belief Networks and stacks of auto-encoder variants, with impressive
      results obtained in several areas, mostly on vision and language data sets.
      The best results obtained on supervised learning tasks involve an unsupervised
      learning component, usually in an unsupervised pre-training phase. Even though
      these new algorithms have enabled training deep models, many questions remain
      as to the nature of this difficult learning problem. The main question investigated
      here is the following: how does unsupervised pre-training work? Answering this
      questions is important if learning in deep architectures is to be further improved.
      We propose several explanatory hypotheses and test them through extensive simulations.
      We empirically show the influence of pre-training with respect to architecture
      depth, model capacity, and number of training examples. The experiments confirm
      and clarify the advantage of unsupervised pre-training. The results suggest
      that unsupervised pre-training guides the learning towards basins of attraction
      of minima that support better generalization from the training data set; the
      evidence from these results supports a regularization explanation for the effect
      of pre-training.", "venue": "International Conference on Artificial Intelligence
      and Statistics", "year": 2010, "referenceCount": 69, "citationCount": 2159,
      "influentialCitationCount": 82, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2010-03-01", "journal": {"pages": "201-208"}, "authors": [{"authorId": "1761978",
      "name": "D. Erhan"}, {"authorId": "1760871", "name": "Aaron C. Courville"},
      {"authorId": "1751762", "name": "Yoshua Bengio"}, {"authorId": "120247189",
      "name": "Pascal Vincent"}]}}]}

      '
    headers:
      Access-Control-Allow-Origin:
      - '*'
      Connection:
      - keep-alive
      Content-Length:
      - '2723'
      Content-Type:
      - application/json
      Date:
      - Fri, 08 Sep 2023 20:26:05 GMT
      Via:
      - 1.1 2ead2a81ff8cd9f180f8ec7fa0607b6e.cloudfront.net (CloudFront)
      X-Amz-Cf-Id:
      - L6aQasTFs3q6pNsPDuF31WMMpWOMbnMv7Jet7Fx1f9c8REA31y1cHA==
      X-Amz-Cf-Pop:
      - EWR53-C1
      X-Cache:
      - Miss from cloudfront
      x-amz-apigw-id:
      - K9F10GN6PHcFsxw=
      x-amzn-Remapped-Connection:
      - keep-alive
      x-amzn-Remapped-Content-Length:
      - '2723'
      x-amzn-Remapped-Date:
      - Fri, 08 Sep 2023 20:26:05 GMT
      x-amzn-Remapped-Server:
      - gunicorn
      x-amzn-RequestId:
      - 1ca265db-5a00-4143-94ee-3b759223230b
    http_version: HTTP/1.1
    status_code: 200
version: 1
