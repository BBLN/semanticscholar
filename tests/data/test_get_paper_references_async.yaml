interactions:
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - api.semanticscholar.org
      user-agent:
      - python-httpx/0.24.1
    method: GET
    uri: https://api.semanticscholar.org/graph/v1/paper/CorpusID:1033682/references?&fields=contexts,intents,isInfluential,abstract,authors,citationCount,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=1000
  response:
    content: '{"offset": 0, "data": [{"isInfluential": false, "intents": [], "contexts":
      ["en used ina feedback loop. More recent examples of training a generative machine
      by back-propagating into it include recent work on auto-encoding variational
      Bayes [20] and stochastic backpropagation [24]. 3 Adversarial nets The adversarial
      modeling framework is most straightforward to apply when the models are both
      multilayer perceptrons. To learn the generator\u2019s distribution p g over
      data x, we de\ufb01n"], "citedPaper": {"paperId": "484ad17c926292fbe0d5211540832a8c8a8e958b",
      "externalIds": {"MAG": "2951275616", "DBLP": "conf/icml/RezendeMW14", "CorpusId":
      16895865}, "corpusId": 16895865, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/484ad17c926292fbe0d5211540832a8c8a8e958b",
      "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative
      Models", "abstract": "We marry ideas from deep neural networks and approximate
      Bayesian inference to derive a generalised class of deep, directed generative
      models, endowed with a new algorithm for scalable inference and learning. Our
      algorithm introduces a recognition model to represent approximate posterior
      distributions, and that acts as a stochastic encoder of the data. We develop
      stochastic back-propagation -- rules for back-propagation through stochastic
      variables -- and use this to develop an algorithm that allows for joint optimisation
      of the parameters of both the generative and recognition model. We demonstrate
      on several real-world data sets that the model generates realistic samples,
      provides accurate imputations of missing data and is a useful tool for high-dimensional
      data visualisation.", "venue": "International Conference on Machine Learning",
      "year": 2014, "referenceCount": 38, "citationCount": 4582, "influentialCitationCount":
      757, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2014-01-16", "journal": {"pages": "1278-1286"},
      "authors": [{"authorId": "1748523", "name": "Danilo Jimenez Rezende"}, {"authorId":
      "14594344", "name": "S. Mohamed"}, {"authorId": "1688276", "name": "Daan Wierstra"}]}},
      {"isInfluential": false, "intents": [], "contexts": ["roblems with unbounded
      activation when used ina feedback loop. More recent examples of training a generative
      machine by back-propagating into it include recent work on auto-encoding variational
      Bayes [20] and stochastic backpropagation [24]. 3 Adversarial nets The adversarial
      modeling framework is most straightforward to apply when the models are both
      multilayer perceptrons. To learn the generator\u2019s d"], "citedPaper": {"paperId":
      "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "externalIds": {"DBLP": "journals/corr/KingmaW13",
      "ArXiv": "1312.6114", "MAG": "2951004968", "CorpusId": 216078090}, "corpusId":
      216078090, "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
      "name": "International Conference on Learning Representations", "type": "conference",
      "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"},
      "url": "https://www.semanticscholar.org/paper/5f5dc5b9a2ba710937e2c413b37b053cd673df02",
      "title": "Auto-Encoding Variational Bayes", "abstract": "Abstract: How can we
      perform efficient inference and learning in directed probabilistic models, in
      the presence of continuous latent variables with intractable posterior distributions,
      and large datasets? We introduce a stochastic variational inference and learning
      algorithm that scales to large datasets and, under some mild differentiability
      conditions, even works in the intractable case. Our contributions is two-fold.
      First, we show that a reparameterization of the variational lower bound yields
      a lower bound estimator that can be straightforwardly optimized using standard
      stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous
      latent variables per datapoint, posterior inference can be made especially efficient
      by fitting an approximate inference model (also called a recognition model)
      to the intractable posterior using the proposed lower bound estimator. Theoretical
      advantages are reflected in experimental results.", "venue": "International
      Conference on Learning Representations", "year": 2013, "referenceCount": 26,
      "citationCount": 21760, "influentialCitationCount": 4733, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2013-12-20", "journal": {"volume": "abs/1312.6114", "name": "CoRR"}, "authors":
      [{"authorId": "1726807", "name": "Diederik P. Kingma"}, {"authorId": "1678311",
      "name": "M. Welling"}]}}, {"isInfluential": false, "intents": [], "contexts":
      [" subset of the indices of x by training a family of conditional models that
      share parameters. Essentially, one can use adversarial nets to implement a stochastic
      extension of the deterministic MP-DBM [11]. 4. Semi-supervised learning: features
      from the discriminator or inference net could improve performance of classi\ufb01ers
      when limited labeled data is available. 5. Ef\ufb01ciency improvements: training
      coul"], "citedPaper": {"paperId": "5656fa5aa6e1beeb98703fc53ec112ad227c49ca",
      "externalIds": {"DBLP": "conf/nips/GoodfellowMCB13", "MAG": "2098617596", "CorpusId":
      6442575}, "corpusId": 6442575, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/5656fa5aa6e1beeb98703fc53ec112ad227c49ca",
      "title": "Multi-Prediction Deep Boltzmann Machines", "abstract": "We introduce
      the multi-prediction deep Boltzmann machine (MP-DBM). The MP-DBM can be seen
      as a single probabilistic model trained to maximize a variational approximation
      to the generalized pseudolikelihood, or as a family of recurrent nets that share
      parameters and approximately solve different inference problems. Prior methods
      of training DBMs either do not perform well on classification tasks or require
      an initial learning pass that trains the DBM greedily, one layer at a time.
      The MP-DBM does not require greedy layerwise pretraining, and outperforms the
      standard DBM at classification, classification with missing inputs, and mean
      field prediction tasks.1", "venue": "NIPS", "year": 2013, "referenceCount":
      25, "citationCount": 134, "influentialCitationCount": 17, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2013-12-05", "journal": {"pages": "548-556"},
      "authors": [{"authorId": "153440022", "name": "I. Goodfellow"}, {"authorId":
      "153583218", "name": "Mehdi Mirza"}, {"authorId": "1760871", "name": "Aaron
      C. Courville"}, {"authorId": "1751762", "name": "Yoshua Bengio"}]}}, {"isInfluential":
      false, "intents": [], "contexts": [], "citedPaper": {"paperId": "836acf6fc99ebf81d219e2b67f7ab25efc29a6a4",
      "externalIds": {"MAG": "1872489089", "ArXiv": "1308.4214", "DBLP": "journals/corr/GoodfellowWLDMPBBB13",
      "CorpusId": 2172854}, "corpusId": 2172854, "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
      "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url":
      "http://bibpurl.oclc.org/web/7130"}, "url": "https://www.semanticscholar.org/paper/836acf6fc99ebf81d219e2b67f7ab25efc29a6a4",
      "title": "Pylearn2: a machine learning research library", "abstract": "Pylearn2
      is a machine learning research library. This does not just mean that it is a
      collection of machine learning algorithms that share a common API; it means
      that it has been designed for flexibility and extensibility in order to facilitate
      research projects that involve new or unusual use cases. In this paper we give
      a brief history of the library, an overview of its basic philosophy, a summary
      of the library''s architecture, and a description of how the Pylearn2 community
      functions socially.", "venue": "arXiv.org", "year": 2013, "referenceCount":
      54, "citationCount": 305, "influentialCitationCount": 17, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
      "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"],
      "publicationDate": "2013-08-19", "journal": {"volume": "abs/1308.4214", "name":
      "ArXiv"}, "authors": [{"authorId": "153440022", "name": "I. Goodfellow"}, {"authorId":
      "1393680089", "name": "David Warde-Farley"}, {"authorId": "3087941", "name":
      "Pascal Lamblin"}, {"authorId": "3074927", "name": "Vincent Dumoulin"}, {"authorId":
      "153583218", "name": "Mehdi Mirza"}, {"authorId": "1996134", "name": "Razvan
      Pascanu"}, {"authorId": "32837403", "name": "J. Bergstra"}, {"authorId": "3227028",
      "name": "Fr\u00e9d\u00e9ric Bastien"}, {"authorId": "1751762", "name": "Yoshua
      Bengio"}]}}, {"isInfluential": false, "intents": [], "contexts": ["ractable
      for all but the most trivial instances, although they can be estimated by Markov
      chain Monte Carlo (MCMC) methods. Mixing poses a signi\ufb01cant problem for
      learning algorithms that rely on MCMC [3, 5]. Deep belief networks (DBNs) [16]
      are hybrid models containing a single undirected layer and several directed
      layers. While a fast approximate layer-wise training criterion exists, DBNs
      incur the com", "on. This approach has the advantage that such machines can
      be designed to be trained by back-propagation. Prominent recent work in this
      area includes the generative stochastic network (GSN) framework [5], which extends
      generalized denoising auto-encoders [4]: both can be seen as de\ufb01ning a
      parameterized Markov chain, i.e., one learns the parameters of a machine that
      performs one step of a generative M", "ans was obtained by cross validation
      on the validation set. This procedure was introduced in Breuleux et al. [8]
      and used for various generative models for which the exact likelihood is not
      tractable [25, 3, 5]. Results are reported in Table 1. This method of estimating
      the likelihood has somewhat high variance and does not perform well in high
      dimensional spaces but it is the best method available to our k"], "citedPaper":
      {"paperId": "5ffa8bf1bf3e39227be28de4ff6915d3b21eb52d", "externalIds": {"ArXiv":
      "1306.1091", "MAG": "2951446714", "DBLP": "journals/corr/BengioT13", "CorpusId":
      9494295}, "corpusId": 9494295, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/5ffa8bf1bf3e39227be28de4ff6915d3b21eb52d",
      "title": "Deep Generative Stochastic Networks Trainable by Backprop", "abstract":
      "We introduce a novel training principle for probabilistic models that is an
      alternative to maximum likelihood. The proposed Generative Stochastic Networks
      (GSN) framework is based on learning the transition operator of a Markov chain
      whose stationary distribution estimates the data distribution. The transition
      distribution of the Markov chain is conditional on the previous state, generally
      involving a small move, so this conditional distribution has fewer dominant
      modes, being unimodal in the limit of small moves. Thus, it is easier to learn
      because it is easier to approximate its partition function, more like learning
      to perform supervised function approximation, with gradients that can be obtained
      by backprop. We provide theorems that generalize recent work on the probabilistic
      interpretation of denoising autoencoders and obtain along the way an interesting
      justification for dependency networks and generalized pseudolikelihood, along
      with a definition of an appropriate joint distribution and sampling mechanism
      even when the conditionals are not consistent. GSNs can be used with missing
      inputs and can be used to sample subsets of variables given the rest. We validate
      these theoretical results with experiments on two image datasets using an architecture
      that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to
      proceed with simple backprop, without the need for layerwise pretraining.",
      "venue": "International Conference on Machine Learning", "year": 2013, "referenceCount":
      44, "citationCount": 384, "influentialCitationCount": 27, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Mathematics", "Computer Science"],
      "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2013-06-05", "journal": {"pages": "226-234"}, "authors":
      [{"authorId": "1751762", "name": "Yoshua Bengio"}, {"authorId": "1398746441",
      "name": "Eric Thibodeau-Laufer"}, {"authorId": "1815021", "name": "Guillaume
      Alain"}, {"authorId": "2965424", "name": "J. Yosinski"}]}}, {"isInfluential":
      false, "intents": [], "contexts": ["can be designed to be trained by back-propagation.
      Prominent recent work in this area includes the generative stochastic network
      (GSN) framework [5], which extends generalized denoising auto-encoders [4]:
      both can be seen as de\ufb01ning a parameterized Markov chain, i.e., one learns
      the parameters of a machine that performs one step of a generative Markov chain.
      Compared to GSNs, the adversarial nets fra"], "citedPaper": {"paperId": "d9704f8119d6ba748230b4f2ad59f0e8c64fdfb0",
      "externalIds": {"MAG": "2953267151", "DBLP": "conf/nips/BengioYAV13", "ArXiv":
      "1305.6663", "CorpusId": 5554756}, "corpusId": 5554756, "publicationVenue":
      null, "url": "https://www.semanticscholar.org/paper/d9704f8119d6ba748230b4f2ad59f0e8c64fdfb0",
      "title": "Generalized Denoising Auto-Encoders as Generative Models", "abstract":
      "Recent work has shown how denoising and contractive autoencoders implicitly
      capture the structure of the data-generating density, in the case where the
      corruption noise is Gaussian, the reconstruction error is the squared error,
      and the data is continuous-valued. This has led to various proposals for sampling
      from this implicitly learned density function, using Langevin and Metropolis-Hastings
      MCMC. However, it remained unclear how to connect the training procedure of
      regularized auto-encoders to the implicit estimation of the underlying data-generating
      distribution when the data are discrete, or using other forms of corruption
      process and reconstruction errors. Another issue is the mathematical justification
      which is only valid in the limit of small corruption noise. We propose here
      a different attack on the problem, which deals with all these issues: arbitrary
      (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood),
      handling both discrete and continuous-valued variables, and removing the bias
      due to non-infinitesimal corruption noise (or non-infinitesimal contractive
      penalty).", "venue": "NIPS", "year": 2013, "referenceCount": 19, "citationCount":
      468, "influentialCitationCount": 36, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2013-05-28",
      "journal": {"volume": "abs/1305.6663", "name": "ArXiv"}, "authors": [{"authorId":
      "1751762", "name": "Yoshua Bengio"}, {"authorId": "145095579", "name": "L. Yao"},
      {"authorId": "1815021", "name": "Guillaume Alain"}, {"authorId": "145467703",
      "name": "Pascal Vincent"}]}}, {"isInfluential": false, "intents": [], "contexts":
      ["at map a high-dimensional, rich sensory input to a class label [14, 22]. These
      striking successes have primarily been based on the backpropagation and dropout
      algorithms, using piecewise linear units [19, 9, 10] which have a particularly
      well-behaved gradient . Deep generative models have had less of an impact, due
      to the dif\ufb01culty of approximating many intractable probabilistic computations
      that arise in ma", "23], the Toronto Face Database (TFD) [28], and CIFAR-10
      [21]. The generator nets used a mixture of recti\ufb01er linear activations
      [19, 9] and sigmoid activations, while the discriminator net used maxout [10]
      activations. Dropout [17] was applied in training the discriminator net. While
      our theoretical framework permits the use of dropout and other noise at intermediate
      layers of the generator, we used no", "ersarial nets framework does not require
      a Markov chain for sampling. Because adversarial nets do not require feedback
      loops during generation, they are better able to leverage piecewise linear units
      [19, 9, 10], which improve the performance of backpropagation but have problems
      with unbounded activation when used ina feedback loop. More recent examples
      of training a generative machine by back-propagating in"], "citedPaper": {"paperId":
      "b7b915d508987b73b61eccd2b237e7ed099a2d29", "externalIds": {"DBLP": "conf/icml/GoodfellowWMCB13",
      "MAG": "3037950864", "ArXiv": "1302.4389", "CorpusId": 10600578}, "corpusId":
      10600578, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/b7b915d508987b73b61eccd2b237e7ed099a2d29",
      "title": "Maxout Networks", "abstract": "We consider the problem of designing
      models to leverage a recently introduced approximate model averaging technique
      called dropout. We define a simple new model called maxout (so named because
      its output is the max of a set of inputs, and because it is a natural companion
      to dropout) designed to both facilitate optimization by dropout and improve
      the accuracy of dropout''s fast approximate model averaging technique. We empirically
      verify that the model successfully accomplishes both of these tasks. We use
      maxout and dropout to demonstrate state of the art classification performance
      on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.", "venue":
      "International Conference on Machine Learning", "year": 2013, "referenceCount":
      27, "citationCount": 2017, "influentialCitationCount": 207, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2013-02-18", "journal": {"pages": "1319-1327"}, "authors":
      [{"authorId": "153440022", "name": "I. Goodfellow"}, {"authorId": "1393680089",
      "name": "David Warde-Farley"}, {"authorId": "153583218", "name": "Mehdi Mirza"},
      {"authorId": "1760871", "name": "Aaron C. Courville"}, {"authorId": "1751762",
      "name": "Yoshua Bengio"}]}}, {"isInfluential": false, "intents": [], "contexts":
      ["s in natural language corpora. So far, the most striking successes in deep
      learning have involved discriminative models, usually those that map a high-dimensional,
      rich sensory input to a class label [14, 22]. These striking successes have
      primarily been based on the backpropagation and dropout algorithms, using piecewise
      linear units [19, 9, 10] which have a particularly well-behaved gradient . Deep
      gene"], "citedPaper": {"paperId": "e33cbb25a8c7390aec6a398e36381f4f7770c283",
      "externalIds": {"MAG": "2184045248", "CorpusId": 7230302}, "corpusId": 7230302,
      "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/e33cbb25a8c7390aec6a398e36381f4f7770c283",
      "title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition",
      "abstract": "Most current speech recognition systems use hidden Markov models
      ( HMMs) to deal with the temporal variability of speech and Gaussian mixture
      models to determine how well each state of each HMM fits a frame or a short
      window of frames of coefficients that represents the acoustic input. An alternati
      ve way to evaluate the fit is to use a feedforward neural network that takes
      several frames of coefficients a s input and produces posterior probabilities
      over HMM states as output. Deep neural networks with many hidden layers, that
      are trained using new methods have been shown to outperform Gaussian mixture
      models on a variety of speech rec ognition benchmarks, sometimes by a large
      margin. This paper provides an overview of this progress and repres nts the
      shared views of four research groups who have had recent successes in using
      deep neural networks for a coustic modeling in speech recognition.", "venue":
      "", "year": 2012, "referenceCount": 81, "citationCount": 2398, "influentialCitationCount":
      167, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["Review"], "publicationDate": "2012-11-01", "journal": {"volume": "29", "pages":
      "82", "name": "IEEE Signal Processing Magazine"}, "authors": [{"authorId": "1695689",
      "name": "Geoffrey E. Hinton"}, {"authorId": "144718788", "name": "L. Deng"},
      {"authorId": "144580027", "name": "Dong Yu"}, {"authorId": "35188630", "name":
      "George E. Dahl"}, {"authorId": "40360972", "name": "Abdel-rahman Mohamed"},
      {"authorId": "3111912", "name": "N. Jaitly"}, {"authorId": "33666044", "name":
      "A. Senior"}, {"authorId": "2657155", "name": "Vincent Vanhoucke"}, {"authorId":
      "14902530", "name": "Patrick Nguyen"}, {"authorId": "1784851", "name": "Tara
      N. Sainath"}, {"authorId": "144707379", "name": "Brian Kingsbury"}]}}, {"isInfluential":
      false, "intents": [], "contexts": ["ractable for all but the most trivial instances,
      although they can be estimated by Markov chain Monte Carlo (MCMC) methods. Mixing
      poses a signi\ufb01cant problem for learning algorithms that rely on MCMC [3,
      5]. Deep belief networks (DBNs) [16] are hybrid models containing a single undirected
      layer and several directed layers. While a fast approximate layer-wise training
      criterion exists, DBNs incur the com", "ability of the test set data under p
      g by \ufb01tting a Gaussian Parzen window to the samples generated with Gand
      reporting the log-likelihood under this distribution. The \u02d9parameter 5
      Model MNIST TFD DBN [3] 138 2 1909 66 Stacked CAE [3] 121 1:6 2110 50 Deep GSN
      [6] 214 1:1 1890 29 Adversarial nets 225 2 2057 26 Table 1: Parzen window-based
      log-likelihood estimates. The reported numbers on MNIST are the ", "ans was
      obtained by cross validation on the validation set. This procedure was introduced
      in Breuleux et al. [8] and used for various generative models for which the
      exact likelihood is not tractable [25, 3, 5]. Results are reported in Table
      1. This method of estimating the likelihood has somewhat high variance and does
      not perform well in high dimensional spaces but it is the best method available
      to our k"], "citedPaper": {"paperId": "d0965d8f9842f2db960b36b528107ca362c00d1a",
      "externalIds": {"ArXiv": "1207.4404", "MAG": "1496559305", "DBLP": "journals/corr/abs-1207-4404",
      "CorpusId": 1334653}, "corpusId": 1334653, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/d0965d8f9842f2db960b36b528107ca362c00d1a",
      "title": "Better Mixing via Deep Representations", "abstract": "It has been
      hypothesized, and supported with experimental evidence, that deeper representations,
      when well trained, tend to do a better job at disentangling the underlying factors
      of variation. We study the following related conjecture: better representations,
      in the sense of better disentangling, can be exploited to produce Markov chains
      that mix faster between modes. Consequently, mixing between modes would be more
      efficient at higher levels of representation. To better understand this, we
      propose a secondary conjecture: the higher-level samples fill more uniformly
      the space they occupy and the high-density manifolds tend to unfold when represented
      at higher levels. The paper discusses these hypotheses and tests them experimentally
      through visualization and measurements of mixing between modes and interpolating
      between samples.", "venue": "International Conference on Machine Learning",
      "year": 2012, "referenceCount": 37, "citationCount": 304, "influentialCitationCount":
      18, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer
      Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle",
      "Conference"], "publicationDate": "2012-07-18", "journal": {"pages": "552-560"},
      "authors": [{"authorId": "1751762", "name": "Yoshua Bengio"}, {"authorId": "1935910",
      "name": "Gr\u00e9goire Mesnil"}, {"authorId": "2921469", "name": "Yann Dauphin"},
      {"authorId": "2425018", "name": "Salah Rifai"}]}}, {"isInfluential": false,
      "intents": [], "contexts": ["odel is also a multilayer perceptron. We refer
      to this special case as adversarial nets. In this case, we can train both models
      using only the highly successful backpropagation and dropout algorithms [17]
      and sample from the generative model using only forward propagation. No approximate
      inference or Markov chains are necessary. 2 Related work An alternative to directed
      graphical models with latent va", "base (TFD) [28], and CIFAR-10 [21]. The generator
      nets used a mixture of recti\ufb01er linear activations [19, 9] and sigmoid
      activations, while the discriminator net used maxout [10] activations. Dropout
      [17] was applied in training the discriminator net. While our theoretical framework
      permits the use of dropout and other noise at intermediate layers of the generator,
      we used noise as the input to only t"], "citedPaper": {"paperId": "0060745e006c5f14ec326904119dca19c6545e51",
      "externalIds": {"DBLP": "journals/corr/abs-1207-0580", "MAG": "1904365287",
      "ArXiv": "1207.0580", "CorpusId": 14832074}, "corpusId": 14832074, "publicationVenue":
      {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names":
      ["ArXiv"], "issn": "2331-8422", "url": "http://bibpurl.oclc.org/web/7130"},
      "url": "https://www.semanticscholar.org/paper/0060745e006c5f14ec326904119dca19c6545e51",
      "title": "Improving neural networks by preventing co-adaptation of feature detectors",
      "abstract": "When a large feedforward neural network is trained on a small training
      set, it typically performs poorly on held-out test data. This \"overfitting\"
      is greatly reduced by randomly omitting half of the feature detectors on each
      training case. This prevents complex co-adaptations in which a feature detector
      is only helpful in the context of several other specific feature detectors.
      Instead, each neuron learns to detect a feature that is generally helpful for
      producing the correct answer given the combinatorially large variety of internal
      contexts in which it must operate. Random \"dropout\" gives big improvements
      on many benchmark tasks and sets new records for speech and object recognition.",
      "venue": "arXiv.org", "year": 2012, "referenceCount": 26, "citationCount": 7025,
      "influentialCitationCount": 559, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2012-07-02", "journal": {"volume": "abs/1207.0580", "name": "ArXiv"}, "authors":
      [{"authorId": "1695689", "name": "Geoffrey E. Hinton"}, {"authorId": "2897313",
      "name": "Nitish Srivastava"}, {"authorId": "2064160", "name": "A. Krizhevsky"},
      {"authorId": "1701686", "name": "Ilya Sutskever"}, {"authorId": "145124475",
      "name": "R. Salakhutdinov"}]}}, {"isInfluential": false, "intents": [], "contexts":
      ["ans was obtained by cross validation on the validation set. This procedure
      was introduced in Breuleux et al. [8] and used for various generative models
      for which the exact likelihood is not tractable [25, 3, 5]. Results are reported
      in Table 1. This method of estimating the likelihood has somewhat high variance
      and does not perform well in high dimensional spaces but it is the best method
      available to our k"], "citedPaper": {"paperId": "aaaea06da21f22221d5fbfd61bb3a02439f0fe02",
      "externalIds": {"MAG": "2950320139", "CorpusId": 122643575}, "corpusId": 122643575,
      "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International
      Conference on Machine Learning", "type": "conference", "alternate_names": ["ICML",
      "Int Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/aaaea06da21f22221d5fbfd61bb3a02439f0fe02",
      "title": "A Generative Process for sampling Contractive Auto-Encoders", "abstract":
      "The contractive auto-encoder learns a representation of the input data that
      captures the local manifold structure around each data point, through the leading
      singular vectors of the Jacobian of the transformation from input to representation.
      The corresponding singular values specify how much local variation is plausible
      in directions associated with the corresponding singular vectors, while remaining
      in a high-density region of the input space. This paper proposes a procedure
      for generating samples that are consistent with the local structure captured
      by a contractive auto-encoder. The associated stochastic process defines a distribution
      from which one can sample, and which experimentally appears to converge quickly
      and mix well between modes, compared to Restricted Boltzmann Machines and Deep
      Belief Networks. The intuitions behind this procedure can also be used to train
      the second layer of contraction that pools lower-level features and learns to
      be invariant to the local directions of variation discovered in the first layer.
      We show that this can help learn and represent invariances present in the data
      and improve classification error.", "venue": "International Conference on Machine
      Learning", "year": 2012, "referenceCount": 18, "citationCount": 78, "influentialCitationCount":
      3, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Mathematics",
      "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "external"}, {"category":
      "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate":
      "2012-06-26", "journal": {"volume": "", "pages": "1811-1818", "name": ""}, "authors":
      [{"authorId": "2425018", "name": "Salah Rifai"}, {"authorId": "1751762", "name":
      "Yoshua Bengio"}, {"authorId": "2921469", "name": "Yann Dauphin"}, {"authorId":
      "145467703", "name": "Pascal Vincent"}]}}, {"isInfluential": false, "intents":
      [], "contexts": ["inst other models of the real-valued (rather than binary)
      version of dataset. of the Gaussians was obtained by cross validation on the
      validation set. This procedure was introduced in Breuleux et al. [8] and used
      for various generative models for which the exact likelihood is not tractable
      [25, 3, 5]. Results are reported in Table 1. This method of estimating the likelihood
      has somewhat high variance"], "citedPaper": {"paperId": "d1c67346e46b4d0067b3c2e5d3b959a8bc24b28c",
      "externalIds": {"DBLP": "journals/neco/BreuleuxBV11", "MAG": "2106439909", "DOI":
      "10.1162/NECO_a_00158", "CorpusId": 907908}, "corpusId": 907908, "publicationVenue":
      {"id": "69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3", "name": "Neural Computation",
      "type": "journal", "alternate_names": ["Neural Comput"], "issn": "0899-7667",
      "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667", "alternate_urls":
      ["http://ieeexplore.ieee.org/servlet/opac?punumber=6720226", "http://www.mitpressjournals.org/loi/neco",
      "https://www.mitpressjournals.org/loi/neco"]}, "url": "https://www.semanticscholar.org/paper/d1c67346e46b4d0067b3c2e5d3b959a8bc24b28c",
      "title": "Quickly Generating Representative Samples from an RBM-Derived Process",
      "abstract": "Two recently proposed learning algorithms, herding and fast persistent
      contrastive divergence (FPCD), share the following interesting characteristic:
      they exploit changes in the model parameters while sampling in order to escape
      modes and mix better during the sampling process that is part of the learning
      algorithm. We justify such approaches as ways to escape modes while keeping
      approximately the same asymptotic distribution of the Markov chain. In that
      spirit, we extend FPCD using an idea borrowed from Herding in order to obtain
      a pure sampling algorithm, which we call the rates-FPCD sampler. Interestingly,
      this sampler can improve the model as we collect more samples, since it optimizes
      a lower bound on the log likelihood of the training data. We provide empirical
      evidence that this new algorithm displays substantially better and more robust
      mixing than Gibbs sampling.", "venue": "Neural Computation", "year": 2011, "referenceCount":
      13, "citationCount": 82, "influentialCitationCount": 7, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Mathematics"],
      "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"},
      {"category": "Mathematics", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2011-08-01", "journal": {"volume": "23", "pages": "2058-2073", "name": "Neural
      Computation"}, "authors": [{"authorId": "1967465", "name": "Olivier Breuleux"},
      {"authorId": "1751762", "name": "Yoshua Bengio"}, {"authorId": "145467703",
      "name": "Pascal Vincent"}]}}, {"isInfluential": false, "intents": [], "contexts":
      ["th undirected and directed models. Alternative criteria that do not approximate
      or bound the log-likelihood have also been proposed, such as score matching
      [18] and noise-contrastive estimation (NCE) [13]. Both of these require the
      learned probability density to be analytically speci\ufb01ed up to a normalization
      constant. Note that in many interesting generative models with several layers
      of latent variab"], "citedPaper": {"paperId": "e3ce36b9deb47aa6bb2aa19c4bfa71283b505025",
      "externalIds": {"DBLP": "journals/jmlr/GutmannH10", "MAG": "2152790380", "CorpusId":
      15816723}, "corpusId": 15816723, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
      "name": "International Conference on Artificial Intelligence and Statistics",
      "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
      Stat"]}, "url": "https://www.semanticscholar.org/paper/e3ce36b9deb47aa6bb2aa19c4bfa71283b505025",
      "title": "Noise-contrastive estimation: A new estimation principle for unnormalized
      statistical models", "abstract": "We present a new estimation principle for
      parameterized statistical models. The idea is to perform nonlinear logistic
      regression to discriminate between the observed data and some artificially generated
      noise, using the model log-density function in the regression nonlinearity.
      We show that this leads to a consistent (convergent) estimator of the parameters,
      and analyze the asymptotic variance. In particular, the method is shown to directly
      work for unnormalized models, i.e. models where the density function does not
      integrate to one. The normalization constant can be estimated just like any
      other parameter. For a tractable ICA model, we compare the method with other
      estimation methods that can be used to learn unnormalized models, including
      score matching, contrastive divergence, and maximum-likelihood where the normalization
      constant is estimated with importance sampling. Simulations show that noise-contrastive
      estimation offers the best trade-off between computational and statistical efficiency.
      The method is then applied to the modeling of natural images: We show that the
      method can successfully estimate a large-scale two-layer model and a Markov
      random field.", "venue": "International Conference on Artificial Intelligence
      and Statistics", "year": 2010, "referenceCount": 15, "citationCount": 1749,
      "influentialCitationCount": 290, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Mathematics", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      ["JournalArticle"], "publicationDate": "2010-03-31", "journal": {"pages": "297-304"},
      "authors": [{"authorId": "145992652", "name": "Michael U Gutmann"}, {"authorId":
      "1791548", "name": "Aapo Hyv\u00e4rinen"}]}}, {"isInfluential": false, "intents":
      [], "contexts": ["ments We trained adversarial nets an a range of datasets including
      MNIST[23], the Toronto Face Database (TFD) [28], and CIFAR-10 [21]. The generator
      nets used a mixture of recti\ufb01er linear activations [19, 9] and sigmoid
      activations, while the discriminator net used maxout [10] activations. Dropout
      [17] was applied in training the discriminator net. While our theoretical framework
      permits the use of dropo", "at map a high-dimensional, rich sensory input to
      a class label [14, 22]. These striking successes have primarily been based on
      the backpropagation and dropout algorithms, using piecewise linear units [19,
      9, 10] which have a particularly well-behaved gradient . Deep generative models
      have had less of an impact, due to the dif\ufb01culty of approximating many
      intractable probabilistic computations that arise in ma", "ersarial nets framework
      does not require a Markov chain for sampling. Because adversarial nets do not
      require feedback loops during generation, they are better able to leverage piecewise
      linear units [19, 9, 10], which improve the performance of backpropagation but
      have problems with unbounded activation when used ina feedback loop. More recent
      examples of training a generative machine by back-propagating in"], "citedPaper":
      {"paperId": "1f88427d7aa8225e47f946ac41a0667d7b69ac52", "externalIds": {"MAG":
      "2546302380", "DBLP": "conf/iccv/JarrettKRL09", "DOI": "10.1109/ICCV.2009.5459469",
      "CorpusId": 206769720}, "corpusId": 206769720, "publicationVenue": {"id": "7654260e-79f9-45c5-9663-d72027cf88f3",
      "name": "IEEE International Conference on Computer Vision", "type": "conference",
      "alternate_names": ["ICCV", "IEEE Int Conf Comput Vis", "ICCV Workshops", "ICCV
      Work"], "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"},
      "url": "https://www.semanticscholar.org/paper/1f88427d7aa8225e47f946ac41a0667d7b69ac52",
      "title": "What is the best multi-stage architecture for object recognition?",
      "abstract": "In many recent object recognition systems, feature extraction stages
      are generally composed of a filter bank, a non-linear transformation, and some
      sort of feature pooling layer. Most systems use only one stage of feature extraction
      in which the filters are hard-wired, or two stages where the filters in one
      or both stages are learned in supervised or unsupervised mode. This paper addresses
      three questions: 1. How does the non-linearities that follow the filter banks
      influence the recognition accuracy? 2. does learning the filter banks in an
      unsupervised or supervised manner improve the performance over random filters
      or hardwired filters? 3. Is there any advantage to using an architecture with
      two stages of feature extraction, rather than one? We show that using non-linearities
      that include rectification and local contrast normalization is the single most
      important ingredient for good accuracy on object recognition benchmarks. We
      show that two stages of feature extraction yield better accuracy than one. Most
      surprisingly, we show that a two-stage system with random filters can yield
      almost 63% recognition rate on Caltech-101, provided that the proper non-linearities
      and pooling layers are used. Finally, we show that with supervised refinement,
      the system achieves state-of-the-art performance on NORB dataset (5.6%) and
      unsupervised pre-training followed by supervised refinement produces good accuracy
      on Caltech-101 (\u226b 65%), and the lowest known error rate on the undistorted,
      unprocessed MNIST dataset (0.53%).", "venue": "IEEE International Conference
      on Computer Vision", "year": 2009, "referenceCount": 55, "citationCount": 2259,
      "influentialCitationCount": 145, "isOpenAccess": true, "openAccessPdf": {"url":
      "http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf", "status": null},
      "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer
      Science", "source": "external"}, {"category": "Computer Science", "source":
      "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate":
      "2009-09-01", "journal": {"pages": "2146-2153", "name": "2009 IEEE 12th International
      Conference on Computer Vision"}, "authors": [{"authorId": "2077257730", "name":
      "Kevin Jarrett"}, {"authorId": "2645384", "name": "K. Kavukcuoglu"}, {"authorId":
      "1706809", "name": "Marc''Aurelio Ranzato"}, {"authorId": "1688882", "name":
      "Yann LeCun"}]}}, {"isInfluential": false, "intents": [], "contexts": ["rnative
      to directed graphical models with latent variables are undirected graphical
      models with latent variables, such as restricted Boltzmann machines (RBMs) [27,
      16], deep Boltzmann machines (DBMs) [26] and their numerous variants. The interactions
      within such models are represented as the product of unnormalized potential
      functions, normalized by a global summation/integration over all states of th"],
      "citedPaper": {"paperId": "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "externalIds":
      {"DBLP": "journals/jmlr/SalakhutdinovH09", "MAG": "189596042", "CorpusId": 877639},
      "corpusId": 877639, "publicationVenue": {"id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
      "name": "International Conference on Artificial Intelligence and Statistics",
      "type": "conference", "alternate_names": ["AISTATS", "Int Conf Artif Intell
      Stat"]}, "url": "https://www.semanticscholar.org/paper/85021c84383d18a7a4434d76dc8135fc6bdc0aa6",
      "title": "Deep Boltzmann Machines", "abstract": "We present a new learning algorithm
      for Boltzmann machines that contain many layers of hidden variables. Data-dependent
      expectations are estimated using a variational approximation that tends to focus
      on a single mode, and dataindependent expectations are approximated using persistent
      Markov chains. The use of two quite different techniques for estimating the
      two types of expectation that enter into the gradient of the log-likelihood
      makes it practical to learn Boltzmann machines with multiple hidden layers and
      millions of parameters. The learning can be made more efficient by using a layer-by-layer
      \u201cpre-training\u201d phase that allows variational inference to be initialized
      with a single bottomup pass. We present results on the MNIST and NORB datasets
      showing that deep Boltzmann machines learn good generative models and perform
      well on handwritten digit and visual object recognition tasks.", "venue": "International
      Conference on Artificial Intelligence and Statistics", "year": 2009, "referenceCount":
      22, "citationCount": 2202, "influentialCitationCount": 272, "isOpenAccess":
      false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Computer
      Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"],
      "publicationDate": "2009-04-15", "journal": {"pages": "448-455"}, "authors":
      [{"authorId": "145124475", "name": "R. Salakhutdinov"}, {"authorId": "1695689",
      "name": "Geoffrey E. Hinton"}]}}, {"isInfluential": false, "intents": [], "contexts":
      ["ive models with several layers of latent variables (such as DBNs and DBMs),
      it is not even possible to derive a tractable unnormalized probability density.
      Some models such as denoising auto-encoders [30] and contractive autoencoders
      have learning rules very similar to score matching applied to RBMs. In NCE,
      as in this work, a discriminative training criterion is employed to \ufb01t
      a generative model. How"], "citedPaper": {"paperId": "843959ffdccf31c6694d135fad07425924f785b1",
      "externalIds": {"DBLP": "conf/icml/VincentLBM08", "MAG": "2025768430", "DOI":
      "10.1145/1390156.1390294", "CorpusId": 207168299}, "corpusId": 207168299, "publicationVenue":
      {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference
      on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int
      Conf Mach Learn"], "url": "https://icml.cc/"}, "url": "https://www.semanticscholar.org/paper/843959ffdccf31c6694d135fad07425924f785b1",
      "title": "Extracting and composing robust features with denoising autoencoders",
      "abstract": "Previous work has shown that the difficulties in learning deep
      generative or discriminative models can be overcome by an initial unsupervised
      learning step that maps inputs to useful intermediate representations. We introduce
      and motivate a new training principle for unsupervised learning of a representation
      based on the idea of making the learned representations robust to partial corruption
      of the input pattern. This approach can be used to train autoencoders, and these
      denoising autoencoders can be stacked to initialize deep architectures. The
      algorithm can be motivated from a manifold learning and information theoretic
      perspective or from a generative model perspective. Comparative experiments
      clearly show the surprising advantage of corrupting the input of autoencoders
      on a pattern classification benchmark suite.", "venue": "International Conference
      on Machine Learning", "year": 2008, "referenceCount": 28, "citationCount": 6504,
      "influentialCitationCount": 539, "isOpenAccess": true, "openAccessPdf": {"url":
      "http://www.iro.umontreal.ca/~vincentp/Publications/denoising_autoencoders_tr1316.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2008-07-05", "journal":
      {"pages": "1096-1103"}, "authors": [{"authorId": "120247189", "name": "Pascal
      Vincent"}, {"authorId": "1777528", "name": "H. Larochelle"}, {"authorId": "1751762",
      "name": "Yoshua Bengio"}, {"authorId": "1798462", "name": "Pierre-Antoine Manzagol"}]}},
      {"isInfluential": false, "intents": [], "contexts": [" ksteps of optimizing
      Dand one step of optimizing G. This results in Dbeing maintained near its optimal
      solution, so long as Gchanges slowly enough. This strategy is analogous to the
      way that SML/PCD [31, 29] training maintains samples from a Markov chain from
      one learning step to the next in order to avoid burning in a Markov chain as
      part of the inner loop of learning. The procedure is formally presente"], "citedPaper":
      {"paperId": "73d6a26f407db77506959fdf3f7b853e44f3844a", "externalIds": {"MAG":
      "2116825644", "DBLP": "conf/icml/Tieleman08", "DOI": "10.1145/1390156.1390290",
      "CorpusId": 7330145}, "corpusId": 7330145, "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
      "name": "International Conference on Machine Learning", "type": "conference",
      "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"},
      "url": "https://www.semanticscholar.org/paper/73d6a26f407db77506959fdf3f7b853e44f3844a",
      "title": "Training restricted Boltzmann machines using approximations to the
      likelihood gradient", "abstract": "A new algorithm for training Restricted Boltzmann
      Machines is introduced. The algorithm, named Persistent Contrastive Divergence,
      is different from the standard Contrastive Divergence algorithms in that it
      aims to draw samples from almost exactly the model distribution. It is compared
      to some standard Contrastive Divergence and Pseudo-Likelihood algorithms on
      the tasks of modeling and classifying various types of data. The Persistent
      Contrastive Divergence algorithm outperforms the other algorithms, and is equally
      fast and simple.", "venue": "International Conference on Machine Learning",
      "year": 2008, "referenceCount": 22, "citationCount": 986, "influentialCitationCount":
      138, "isOpenAccess": true, "openAccessPdf": {"url": "http://icml2008.cs.helsinki.fi/papers/638.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "2008-07-05", "journal":
      {"pages": "1064-1071"}, "authors": [{"authorId": "2957517", "name": "T. Tieleman"}]}},
      {"isInfluential": false, "intents": [], "contexts": ["ins are necessary. 2 Related
      work An alternative to directed graphical models with latent variables are undirected
      graphical models with latent variables, such as restricted Boltzmann machines
      (RBMs) [27, 16], deep Boltzmann machines (DBMs) [26] and their numerous variants.
      The interactions within such models are represented as the product of unnormalized
      potential functions, normalized by a global summat", "l instances, although
      they can be estimated by Markov chain Monte Carlo (MCMC) methods. Mixing poses
      a signi\ufb01cant problem for learning algorithms that rely on MCMC [3, 5].
      Deep belief networks (DBNs) [16] are hybrid models containing a single undirected
      layer and several directed layers. While a fast approximate layer-wise training
      criterion exists, DBNs incur the computational dif\ufb01culties associated "],
      "citedPaper": {"paperId": "8978cf7574ceb35f4c3096be768c7547b28a35d0", "externalIds":
      {"MAG": "2136922672", "DBLP": "journals/neco/HintonOT06", "DOI": "10.1162/neco.2006.18.7.1527",
      "CorpusId": 2309950, "PubMed": "16764513"}, "corpusId": 2309950, "publicationVenue":
      {"id": "69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3", "name": "Neural Computation",
      "type": "journal", "alternate_names": ["Neural Comput"], "issn": "0899-7667",
      "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667", "alternate_urls":
      ["http://ieeexplore.ieee.org/servlet/opac?punumber=6720226", "http://www.mitpressjournals.org/loi/neco",
      "https://www.mitpressjournals.org/loi/neco"]}, "url": "https://www.semanticscholar.org/paper/8978cf7574ceb35f4c3096be768c7547b28a35d0",
      "title": "A Fast Learning Algorithm for Deep Belief Nets", "abstract": "We show
      how to use complementary priors to eliminate the explaining-away effects that
      make inference difficult in densely connected belief nets that have many hidden
      layers. Using complementary priors, we derive a fast, greedy algorithm that
      can learn deep, directed belief networks one layer at a time, provided the top
      two layers form an undirected associative memory. The fast, greedy algorithm
      is used to initialize a slower learning procedure that fine-tunes the weights
      using a contrastive version of the wake-sleep algorithm. After fine-tuning,
      a network with three hidden layers forms a very good generative model of the
      joint distribution of handwritten digit images and their labels. This generative
      model gives better digit classification than the best discriminative learning
      algorithms. The low-dimensional manifolds on which the digits lie are modeled
      by long ravines in the free-energy landscape of the top-level associative memory,
      and it is easy to explore these ravines by using the directed connections to
      display what the associative memory has in mind.", "venue": "Neural Computation",
      "year": 2006, "referenceCount": 31, "citationCount": 14946, "influentialCitationCount":
      1319, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Medicine",
      "Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category": "Medicine",
      "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      "2006-07-01", "journal": {"volume": "18", "pages": "1527-1554", "name": "Neural
      Computation"}, "authors": [{"authorId": "1695689", "name": "Geoffrey E. Hinton"},
      {"authorId": "2217144", "name": "Simon Osindero"}, {"authorId": "1725303", "name":
      "Y. Teh"}]}}, {"isInfluential": false, "intents": [], "contexts": [" ksteps
      of optimizing Dand one step of optimizing G. This results in Dbeing maintained
      near its optimal solution, so long as Gchanges slowly enough. This strategy
      is analogous to the way that SML/PCD [31, 29] training maintains samples from
      a Markov chain from one learning step to the next in order to avoid burning
      in a Markov chain as part of the inner loop of learning. The procedure is formally
      presente"], "citedPaper": {"paperId": "ca9b21e84ffc7e193d1b3bb45fb7c4e48226b59e",
      "externalIds": {"MAG": "1990838964", "DOI": "10.1080/17442509908834179", "CorpusId":
      15419929}, "corpusId": 15419929, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/ca9b21e84ffc7e193d1b3bb45fb7c4e48226b59e",
      "title": "On the convergence of markovian stochastic algorithms with rapidly
      decreasing ergodicity rates", "abstract": "We analyse the convergence of stochastic
      algorithms with Markovian noise when the ergodicity of the Markov chain governing
      the noise rapidly decreases as the control parameter tends to infinity. In such
      a case, there may be a positive probability of divergence of the algorithm in
      the classic Robbins-Monro form. We provide sufficient condition which ensure
      convergence. Moreover, we analyse the asymptotic behaviour of these algorithms
      and state a diffusion approximation theorem", "venue": "", "year": 1999, "referenceCount":
      24, "citationCount": 163, "influentialCitationCount": 24, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Mathematics"], "s2FieldsOfStudy":
      [{"category": "Mathematics", "source": "external"}, {"category": "Mathematics",
      "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": null, "publicationDate": "1999-02-01", "journal": {"volume":
      "65", "pages": "177-228", "name": "Stochastics and Stochastics Reports"}, "authors":
      [{"authorId": "1721284", "name": "L. Younes"}]}}, {"isInfluential": false, "intents":
      [], "contexts": ["input to both Gand D. 2. Learned approximate inference can
      be performed by training an auxiliary network to predict z given x. This is
      similar to the inference net trained by the wake-sleep algorithm [15] but with
      the advantage that the inference net may be trained for a \ufb01xed generator
      net after the generator net has \ufb01nished training. 7 3.One can approximately
      model all conditionals p(x S jx 6S) where"], "citedPaper": {"paperId": "6dd01cd9c17d1491ead8c9f97597fbc61dead8ea",
      "externalIds": {"MAG": "1993845689", "DOI": "10.1126/SCIENCE.7761831", "CorpusId":
      871473, "PubMed": "7761831"}, "corpusId": 871473, "publicationVenue": {"id":
      "f59506a8-d8bb-4101-b3d4-c4ac3ed03dad", "name": "Science", "type": "journal",
      "issn": "0193-4511", "alternate_issns": ["0036-8075"], "url": "https://www.jstor.org/journal/science",
      "alternate_urls": ["https://www.sciencemag.org/", "http://www.sciencemag.org/",
      "http://www.jstor.org/journals/00368075.html", "http://www.sciencemag.org/archive/"]},
      "url": "https://www.semanticscholar.org/paper/6dd01cd9c17d1491ead8c9f97597fbc61dead8ea",
      "title": "The \"wake-sleep\" algorithm for unsupervised neural networks.", "abstract":
      "An unsupervised learning algorithm for a multilayer network of stochastic neurons
      is described. Bottom-up \"recognition\" connections convert the input into representations
      in successive hidden layers, and top-down \"generative\" connections reconstruct
      the representation in one layer from the representation in the layer above.
      In the \"wake\" phase, neurons are driven by recognition connections, and generative
      connections are adapted to increase the probability that they would reconstruct
      the correct activity vector in the layer below. In the \"sleep\" phase, neurons
      are driven by generative connections, and recognition connections are adapted
      to increase the probability that they would produce the correct activity vector
      in the layer above.", "venue": "Science", "year": 1995, "referenceCount": 23,
      "citationCount": 1071, "influentialCitationCount": 49, "isOpenAccess": false,
      "openAccessPdf": null, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy":
      [{"category": "Computer Science", "source": "external"}, {"category": "Medicine",
      "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}],
      "publicationTypes": ["JournalArticle"], "publicationDate": "1995-05-26", "journal":
      {"volume": "268 5214", "pages": "\n          1158-61\n        ", "name": "Science"},
      "authors": [{"authorId": "1695689", "name": "Geoffrey E. Hinton"}, {"authorId":
      "1790646", "name": "P. Dayan"}, {"authorId": "1749650", "name": "B. Frey"},
      {"authorId": "145572884", "name": "R. Neal"}]}}, {"isInfluential": false, "intents":
      [], "contexts": ["ins are necessary. 2 Related work An alternative to directed
      graphical models with latent variables are undirected graphical models with
      latent variables, such as restricted Boltzmann machines (RBMs) [27, 16], deep
      Boltzmann machines (DBMs) [26] and their numerous variants. The interactions
      within such models are represented as the product of unnormalized potential
      functions, normalized by a global summat"], "citedPaper": {"paperId": "4f7476037408ac3d993f5088544aab427bc319c1",
      "externalIds": {"MAG": "1820494964", "CorpusId": 533055}, "corpusId": 533055,
      "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/4f7476037408ac3d993f5088544aab427bc319c1",
      "title": "Information processing in dynamical systems: foundations of harmony
      theory", "abstract": "Abstract : At this early stage in the development of cognitive
      science, methodological issues are both open and central. There may have been
      times when developments in neuroscience, artificial intelligence, or cognitive
      psychology seduced researchers into believing that their discipline was on the
      verge of discovering the secret of intelligence. But a humbling history of hopes
      disappointed has produced the realization that understanding the mind will challenge
      the power of all these methodologies combined. The work reported in this chapter
      rests on the conviction that a methodology that has a crucial role to play in
      the development of cognitive science is mathematical analysis. The success of
      cognitive science, like that of many other sciences, will, I believe, depend
      upon the construction of a solid body of theoretical results: results that express
      in a mathematical language the conceptual insights of the field; results that
      squeeze all possible implications out of those insights by exploiting powerful
      mathematical techniques. This body of results, which I will call the theory
      of information processing, exists because information is a concept that lends
      itself to mathematical formalization. One part of the theory of information
      processing is already well-developed. The classical theory of computation provides
      powerful and elegant results about the notion of effective procedure, including
      languages for precisely expressing them and theoretical machines for realizing
      them.", "venue": "", "year": 1986, "referenceCount": 18, "citationCount": 2084,
      "influentialCitationCount": 209, "isOpenAccess": false, "openAccessPdf": null,
      "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category":
      "Mathematics", "source": "external"}, {"category": "Computer Science", "source":
      "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes":
      null, "publicationDate": "1986-01-03", "journal": {"volume": "", "pages": "194-281",
      "name": ""}, "authors": [{"authorId": "1748557", "name": "P. Smolensky"}]}},
      {"isInfluential": false, "intents": [], "contexts": [], "citedPaper": {"paperId":
      null, "externalIds": null, "corpusId": null, "publicationVenue": null, "url":
      null, "title": "Generative adversarial nets, NIPS (2014)", "abstract": null,
      "venue": "NIPS 2016 Tutorial: Generative Adversarial Networks,", "year": 2016,
      "referenceCount": null, "citationCount": null, "influentialCitationCount": null,
      "isOpenAccess": null, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy":
      null, "publicationTypes": null, "publicationDate": null, "journal": null, "authors":
      []}}, {"isInfluential": false, "intents": [], "contexts": ["monstrate the potential
      of the framework through qualitative and quantitative evaluation of the generated
      samples. 1 Introduction The promise of deep learning is to discover rich, hierarchical
      models [2] that represent probability distributions over the kinds of data encountered
      in arti\ufb01cial intelligence applications, such as natural images, audio waveforms
      containing speech, and symbols in natural l"], "citedPaper": {"paperId": "d04d6db5f0df11d0cff57ec7e15134990ac07a4f",
      "externalIds": {"DBLP": "journals/ftml/Bengio09", "MAG": "2072128103", "DOI":
      "10.1561/2200000006", "CorpusId": 207178999}, "corpusId": 207178999, "publicationVenue":
      null, "url": "https://www.semanticscholar.org/paper/d04d6db5f0df11d0cff57ec7e15134990ac07a4f",
      "title": "Learning Deep Architectures for AI", "abstract": "Theoretical results
      strongly suggest that in order to learn the kind of complicated functions that
      can represent high-level abstractions (e.g. in vision, language, and other AI-level
      tasks), one needs deep architectures. Deep architectures are composed of multiple
      levels of non-linear operations, such as in neural nets with many hidden layers
      or in complicated propositional formulae re-using many sub-formulae. Searching
      the parameter space of deep architectures is a difficult optimization task,
      but learning algorithms such as those for Deep Belief Networks have recently
      been proposed to tackle this problem with notable success, beating the state-of-the-art
      in certain areas. This paper discusses the motivations and principles regarding
      learning algorithms for deep architectures, in particular those exploiting as
      building blocks unsupervised learning of single-layer models such as Restricted
      Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.",
      "venue": "Found. Trends Mach. Learn.", "year": 2007, "referenceCount": 246,
      "citationCount": 8232, "influentialCitationCount": 530, "isOpenAccess": true,
      "openAccessPdf": {"url": "http://www.iro.umontreal.ca/~bengioy/papers/ftml.pdf",
      "status": null}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate":
      null, "journal": {"volume": "2", "pages": "1-127", "name": "Found. Trends Mach.
      Learn."}, "authors": [{"authorId": "1751762", "name": "Yoshua Bengio"}]}}, {"isInfluential":
      false, "intents": [], "contexts": [" perceptrons in practice suggests that they
      are a reasonable model to use despite their lack of theoretical guarantees.
      5 Experiments We trained adversarial nets an a range of datasets including MNIST[23],
      the Toronto Face Database (TFD) [28], and CIFAR-10 [21]. The generator nets
      used a mixture of recti\ufb01er linear activations [19, 9] and sigmoid activations,
      while the discriminator net used maxout [10"], "citedPaper": {"paperId": "162d958ff885f1462aeda91cd72582323fd6a1f4",
      "externalIds": {"MAG": "2112796928", "DBLP": "journals/pieee/LeCunBBH98", "DOI":
      "10.1109/5.726791", "CorpusId": 14542261}, "corpusId": 14542261, "publicationVenue":
      {"id": "6faaccca-1cc4-45a9-aeb6-96a4901d2606", "name": "Proceedings of the IEEE",
      "type": "journal", "alternate_names": ["Proc IEEE"], "issn": "0018-9219", "alternate_issns":
      ["1558-2256"], "url": "http://www.ieee.org/portal/pages/pubs/proceedings/",
      "alternate_urls": ["http://www.ieee.org/products/onlinepubs/pub/about_conference.html",
      "https://ieeexplore.ieee.org/servlet/opac?punumber=5", "http://proceedingsoftheieee.ieee.org/"]},
      "url": "https://www.semanticscholar.org/paper/162d958ff885f1462aeda91cd72582323fd6a1f4",
      "title": "Gradient-based learning applied to document recognition", "abstract":
      "Multilayer neural networks trained with the back-propagation algorithm constitute
      the best example of a successful gradient based learning technique. Given an
      appropriate network architecture, gradient-based learning algorithms can be
      used to synthesize a complex decision surface that can classify high-dimensional
      patterns, such as handwritten characters, with minimal preprocessing. This paper
      reviews various methods applied to handwritten character recognition and compares
      them on a standard handwritten digit recognition task. Convolutional neural
      networks, which are specifically designed to deal with the variability of 2D
      shapes, are shown to outperform all other techniques. Real-life document recognition
      systems are composed of multiple modules including field extraction, segmentation
      recognition, and language modeling. A new learning paradigm, called graph transformer
      networks (GTN), allows such multimodule systems to be trained globally using
      gradient-based methods so as to minimize an overall performance measure. Two
      systems for online handwriting recognition are described. Experiments demonstrate
      the advantage of global training, and the flexibility of graph transformer networks.
      A graph transformer network for reading a bank cheque is also described. It
      uses convolutional neural network character recognizers combined with global
      training techniques to provide record accuracy on business and personal cheques.
      It is deployed commercially and reads several million cheques per day.", "venue":
      "Proceedings of the IEEE", "year": 1998, "referenceCount": 140, "citationCount":
      43300, "influentialCitationCount": 6941, "isOpenAccess": false, "openAccessPdf":
      null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category":
      "Computer Science", "source": "external"}, {"category": "Computer Science",
      "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"],
      "publicationDate": null, "journal": {"volume": "86", "pages": "2278-2324", "name":
      "Proc. IEEE"}, "authors": [{"authorId": "1688882", "name": "Yann LeCun"}, {"authorId":
      "52184096", "name": "L. Bottou"}, {"authorId": "1751762", "name": "Yoshua Bengio"},
      {"authorId": "1721248", "name": "P. Haffner"}]}}]}

      '
    headers:
      Access-Control-Allow-Origin:
      - '*'
      Connection:
      - keep-alive
      Content-Length:
      - '67798'
      Content-Type:
      - application/json
      Date:
      - Thu, 07 Sep 2023 20:23:51 GMT
      Via:
      - 1.1 9c1465c390ec70cc0036cf15c3a531d8.cloudfront.net (CloudFront)
      X-Amz-Cf-Id:
      - Ugtusrst8riPbLtZx0wacX_UsUelSvnpTOdkePxDDsc3SrqXXIdJSw==
      X-Amz-Cf-Pop:
      - EWR50-C1
      X-Cache:
      - Miss from cloudfront
      x-amz-apigw-id:
      - K5ylqF7gvHcFaXw=
      x-amzn-Remapped-Connection:
      - keep-alive
      x-amzn-Remapped-Content-Length:
      - '67798'
      x-amzn-Remapped-Date:
      - Thu, 07 Sep 2023 20:23:51 GMT
      x-amzn-Remapped-Server:
      - gunicorn
      x-amzn-RequestId:
      - 19e66c04-0d22-49d5-94dd-6b6345764d57
    http_version: HTTP/1.1
    status_code: 200
version: 1
